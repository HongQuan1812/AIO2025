{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV</th>\n",
       "      <th>Radio</th>\n",
       "      <th>Social Media</th>\n",
       "      <th>Influencer</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Low</td>\n",
       "      <td>1.218354</td>\n",
       "      <td>1.270444</td>\n",
       "      <td>Micro</td>\n",
       "      <td>90.054222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Medium</td>\n",
       "      <td>14.949791</td>\n",
       "      <td>0.274451</td>\n",
       "      <td>Macro</td>\n",
       "      <td>222.741668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Low</td>\n",
       "      <td>10.377258</td>\n",
       "      <td>0.061984</td>\n",
       "      <td>Mega</td>\n",
       "      <td>102.774790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High</td>\n",
       "      <td>26.469274</td>\n",
       "      <td>7.070945</td>\n",
       "      <td>Micro</td>\n",
       "      <td>328.239378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>High</td>\n",
       "      <td>36.876302</td>\n",
       "      <td>7.618605</td>\n",
       "      <td>Mega</td>\n",
       "      <td>351.807328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       TV      Radio  Social Media Influencer       Sales\n",
       "0     Low   1.218354      1.270444      Micro   90.054222\n",
       "1  Medium  14.949791      0.274451      Macro  222.741668\n",
       "2     Low  10.377258      0.061984       Mega  102.774790\n",
       "3    High  26.469274      7.070945      Micro  328.239378\n",
       "4    High  36.876302      7.618605       Mega  351.807328"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/thuyduc/GG Advanced Data Analytics/Course 5/Module 4/Hypothesis testing with Python/marketing_sales_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Radio</th>\n",
       "      <th>Social Media</th>\n",
       "      <th>Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.808070</td>\n",
       "      <td>-0.898526</td>\n",
       "      <td>-1.146910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.380950</td>\n",
       "      <td>-1.352277</td>\n",
       "      <td>0.321089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.856177</td>\n",
       "      <td>-1.449071</td>\n",
       "      <td>-1.006175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.816280</td>\n",
       "      <td>1.744041</td>\n",
       "      <td>1.488272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.897891</td>\n",
       "      <td>1.993542</td>\n",
       "      <td>1.749018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Radio  Social Media     Sales\n",
       "0 -1.808070     -0.898526 -1.146910\n",
       "1 -0.380950     -1.352277  0.321089\n",
       "2 -0.856177     -1.449071 -1.006175\n",
       "3  0.816280      1.744041  1.488272\n",
       "4  1.897891      1.993542  1.749018"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df = data[['Radio', 'Social Media', 'Sales']]\n",
    "df = df.dropna()\n",
    "df = scaler.fit_transform(df)\n",
    "df = pd.DataFrame(df, columns=['Radio', 'Social Media', 'Sales'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TV              0\n",
       "Radio           0\n",
       "Social Media    0\n",
       "Influencer      0\n",
       "Sales           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()\n",
    "data = data.dropna()\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[['Radio', 'Social Media']]\n",
    "y = df['Sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatgpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GradientDescent:\n",
    "    \"\"\"Linear regression using gradient descent optimization\n",
    "    Three types available: batch, mini-batch, stochastic\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 0.01, epochs: int = 1000, \n",
    "                 type: str = 'batch', batch_size: int = None) -> None:\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.losses = []\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.type = type.lower()  # Convert to lowercase for consistency\n",
    "        \n",
    "        # Validate type and batch_size\n",
    "        if self.type not in ['batch', 'mini-batch', 'stochastic']:\n",
    "            raise ValueError(\"Type must be one of: 'batch', 'mini-batch', 'stochastic'\")\n",
    "            \n",
    "        if self.type == 'mini-batch':\n",
    "            if batch_size is None:\n",
    "                raise ValueError(\"batch_size must be specified when using mini-batch gradient descent\")\n",
    "            if not isinstance(batch_size, int) or batch_size <= 0:\n",
    "                raise ValueError(\"batch_size must be a positive integer\")\n",
    "            self.batch_size = batch_size\n",
    "        \n",
    "    def initialize_parameters(self, X: np.ndarray) -> None:\n",
    "        \"\"\"Initialize weights and bias\"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.random.randn(n_features) * 0.01\n",
    "        self.b = 0.0\n",
    "        \n",
    "    def calculate_predictions(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Calculate predictions (y_hat) based on type of gradient descent\"\"\"\n",
    "        if self.type == 'batch':\n",
    "            return np.dot(X, self.w) + self.b\n",
    "        elif self.type == 'mini-batch':\n",
    "            return np.dot(X, self.w) + self.b\n",
    "        elif self.type == 'stochastic':\n",
    "            return np.dot(X, self.w) + self.b\n",
    "    \n",
    "    def calculate_loss(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"Calculate mean squared error loss\"\"\"\n",
    "        #m = len(y_true)\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "    \n",
    "    def calculate_gradients(self, X: np.ndarray, y: np.ndarray, y_pred: np.ndarray) -> Tuple[np.ndarray, float]:\n",
    "        \"\"\"Calculate gradients for weights and bias\"\"\"\n",
    "        if self.type == 'batch' or self.type == 'mini-batch':\n",
    "            m = len(y)\n",
    "            dw = -(2/m) * np.dot(X.T, (y - y_pred))\n",
    "            db = -(2/m) * np.sum(y - y_pred)\n",
    "            return dw, db\n",
    "        elif self.type == 'stochastic':\n",
    "            dw = -(2/1) * np.dot(X.T, (y - y_pred))\n",
    "            db = -(2/1) * np.sum(y - y_pred)\n",
    "            return dw, db\n",
    "    \n",
    "    def update_parameters(self, dw: np.ndarray, db: float) -> None:\n",
    "        \"\"\"Update weights and bias using gradients\"\"\"\n",
    "        self.w -= self.learning_rate * dw\n",
    "        self.b -= self.learning_rate * db\n",
    "        \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, verbose: bool = True) -> None:\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        self.initialize_parameters(X)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            if self.type == 'batch':\n",
    "                # Forward pass\n",
    "                y_pred = self.calculate_predictions(X)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = self.calculate_loss(y, y_pred)\n",
    "                self.losses.append(loss)\n",
    "                \n",
    "                # Calculate gradients\n",
    "                dw, db = self.calculate_gradients(X, y, y_pred)\n",
    "                \n",
    "                # Update parameters\n",
    "                self.update_parameters(dw, db)\n",
    "                \n",
    "            # Print progress\n",
    "            if verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f'Epoch {epoch+1}/{self.epochs} - Loss: {loss:.6f}')\n",
    "            elif self.type == 'mini-batch':\n",
    "                temp_loss = []\n",
    "                for i in range(0, X.shape[0], self.batch_size):\n",
    "                    # Forward pass\n",
    "                    y_pred = self.calculate_predictions(X[i:i+self.batch_size])\n",
    "                    \n",
    "                    # Calculate loss\n",
    "                    loss = self.calculate_loss(y[i:i+self.batch_size], y_pred)\n",
    "                    #self.losses.append(loss)\n",
    "                    temp_loss.append(loss)\n",
    "                    \n",
    "                    # Calculate gradients\n",
    "                    dw, db = self.calculate_gradients(X[i:i+self.batch_size], y[i:i+self.batch_size], y_pred)\n",
    "                    \n",
    "                    # Update parameters\n",
    "                    self.update_parameters(dw, db)\n",
    "                self.losses.append(temp_loss[-1])\n",
    "                    \n",
    "                # Print progress\n",
    "                if verbose and (epoch + 1) % 100 == 0:\n",
    "                    print(f'Epoch {epoch+1}/{self.epochs} - Loss: {loss:.6f}')\n",
    "            elif self.type == 'stochastic':\n",
    "                for i in range(X.shape[0]):\n",
    "                    # Forward pass\n",
    "                    y_pred = self.calculate_predictions(X.iloc[i])\n",
    "\n",
    "                    # Calculate loss\n",
    "                    loss = self.calculate_loss(y[i], y_pred)\n",
    "                    #self.losses.append(loss)\n",
    "\n",
    "                    # Calculate gradients\n",
    "                    dw, db = self.calculate_gradients(X.iloc[i], y[i], y_pred)\n",
    "\n",
    "                    # Update parameters\n",
    "                    self.update_parameters(dw, db)\n",
    "\n",
    "                    if i == X.shape[0] - 1:\n",
    "                        self.losses.append(loss)\n",
    "\n",
    "                if verbose and (epoch + 1) % 100 == 0:\n",
    "                    print(f'Epoch {epoch+1}/{self.epochs} - Loss: {loss:.6f}')\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions for new data\"\"\"\n",
    "        return self.calculate_predictions(X)\n",
    "    \n",
    "    def plot_loss(self) -> None:\n",
    "        \"\"\"Plot the training loss history\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.losses)\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Mean Squared Error')\n",
    "        plt.title('Training Loss Over Time')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "model = GradientDescent(learning_rate=0.01, epochs=5, type = 'mini-batch', batch_size=50)\n",
    "model.fit(x, y)\n",
    "model.plot_loss()\n",
    "#predictions = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code chay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([1, 2])\n",
    "x = np.array([-1.808070, -0.898526])\n",
    "b = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.505122"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def linear_function(w, x, b):\n",
    "    \"\"\"Calculate y_hat using a linear function.\"\"\"\n",
    "    t = np.dot(w, x)\n",
    "    y_hat = t + b\n",
    "    return y_hat\n",
    "\n",
    "y_hat = linear_function(w, x, b)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8753.350849710336"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_function(y, y_hat):\n",
    "    \"\"\"Calculate loss function.\"\"\"\n",
    "    loss = (y - y_hat)**2\n",
    "    return loss\n",
    "\n",
    "\n",
    "y = 90.054222\n",
    "loss = loss_function(y, y_hat)\n",
    "loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.97, 1.97]), 0.060000000000000005)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_descent(y, y_hat, x, w, b, alpha, dLdw, dLdb):\n",
    "    \"\"\"Find w, b using gradient descent\"\"\"\n",
    "    \n",
    "    w = w - alpha*dLdw\n",
    "    b = b - alpha*dLdb\n",
    "    #print(dLdw)\n",
    "    return w, b\n",
    "\n",
    "alpha = 0.01\n",
    "dLdw = 3\n",
    "dLdb = 4\n",
    "w, b = gradient_descent(y, y_hat, x, w, b, alpha, dLdw, dLdb)\n",
    "w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Iteration 1: w = [2. 3.], b = 3.0000000000043294, loss = 4.68639570084001\n",
      "Iteration 2: w = [2. 3.], b = 3.000000000008609, loss = 4.578824199672238\n",
      "Iteration 3: w = [2. 3.], b = 3.000000000012716, loss = 4.216424336023136\n",
      "Iteration 4: w = [2. 3.], b = 2.9999999999959632, loss = 70.16426747733088\n",
      "Iteration 5: w = [2. 3.], b = 2.9999999999739084, loss = 121.6033230268059\n",
      "Iteration 6: w = [2. 3.], b = 2.9999999999604707, loss = 45.14374877848892\n",
      "Iteration 7: w = [2. 3.], b = 2.9999999999402127, loss = 102.59514662351667\n",
      "Iteration 8: w = [2. 3.], b = 2.999999999936592, loss = 3.2775217893962716\n",
      "Iteration 9: w = [2. 3.], b = 2.9999999999301132, loss = 10.494218479739247\n",
      "Iteration 10: w = [2. 3.], b = 2.9999999999357234, loss = 7.8686421508278315\n",
      "Iteration 11: w = [2. 3.], b = 2.9999999999324505, loss = 2.678279667829801\n",
      "Iteration 12: w = [2. 3.], b = 2.999999999931531, loss = 0.21130429378666749\n",
      "Iteration 13: w = [2. 3.], b = 2.9999999999118923, loss = 96.42184443189497\n",
      "Iteration 14: w = [2. 3.], b = 2.999999999897429, loss = 52.29615752388387\n",
      "Iteration 15: w = [2. 3.], b = 2.999999999894286, loss = 2.469764763966433\n",
      "Iteration 16: w = [2. 3.], b = 2.9999999998619495, loss = 261.41094392301807\n",
      "Iteration 17: w = [2. 3.], b = 2.9999999998573146, loss = 5.370240441446812\n",
      "Iteration 18: w = [2. 3.], b = 2.999999999861646, loss = 4.689360252842232\n",
      "Iteration 19: w = [2. 3.], b = 2.999999999847248, loss = 51.82342579420069\n",
      "Iteration 20: w = [2. 3.], b = 2.9999999998342024, loss = 42.546967239439766\n",
      "Iteration 21: w = [2. 3.], b = 2.999999999832158, loss = 1.04507988187505\n",
      "Iteration 22: w = [2. 3.], b = 2.999999999827097, loss = 6.4033488712767355\n",
      "Iteration 23: w = [2. 3.], b = 2.999999999826039, loss = 0.2797817696477061\n",
      "Iteration 24: w = [2. 3.], b = 2.9999999998146976, loss = 32.156821602330794\n",
      "Iteration 25: w = [2. 3.], b = 2.9999999998159517, loss = 0.3932808623906892\n",
      "Iteration 26: w = [2. 3.], b = 2.9999999998021383, loss = 47.702964991232115\n",
      "Iteration 27: w = [2. 3.], b = 2.999999999789867, loss = 37.64488186361359\n",
      "Iteration 28: w = [2. 3.], b = 2.9999999997810107, loss = 19.609889767218274\n",
      "Iteration 29: w = [2. 3.], b = 2.9999999997742357, loss = 11.475741704114846\n",
      "Iteration 30: w = [2. 3.], b = 2.9999999997669518, loss = 13.263269577700957\n",
      "Iteration 31: w = [2. 3.], b = 2.999999999761155, loss = 8.400171378215024\n",
      "Iteration 32: w = [2. 3.], b = 2.999999999751773, loss = 22.005793816418766\n",
      "Iteration 33: w = [2. 3.], b = 2.9999999997531943, loss = 0.5050763766408531\n",
      "Iteration 34: w = [2. 3.], b = 2.9999999997455213, loss = 14.71907991855496\n",
      "Iteration 35: w = [2. 3.], b = 2.9999999997355764, loss = 24.726365316135237\n",
      "Iteration 36: w = [2. 3.], b = 2.99999999973147, loss = 4.21607746640289\n",
      "Iteration 37: w = [2. 3.], b = 2.9999999997364917, loss = 6.304221360124811\n",
      "Iteration 38: w = [2. 3.], b = 2.9999999997319855, loss = 5.075939138588147\n",
      "Iteration 39: w = [2. 3.], b = 2.999999999734227, loss = 1.2556772012883723\n",
      "Iteration 40: w = [2. 3.], b = 2.9999999997285554, loss = 8.04146657087379\n",
      "Iteration 41: w = [2. 3.], b = 2.9999999997183413, loss = 26.081056813448075\n",
      "Iteration 42: w = [2. 3.], b = 2.999999999714535, loss = 3.6216190036995495\n",
      "Iteration 43: w = [2. 3.], b = 2.999999999706453, loss = 16.328926895067212\n",
      "Iteration 44: w = [2. 3.], b = 2.9999999997095563, loss = 2.407795159212405\n",
      "Iteration 45: w = [2. 3.], b = 2.999999999697445, loss = 36.669143918332374\n",
      "Iteration 46: w = [2. 3.], b = 2.9999999996844435, loss = 42.26179003259843\n",
      "Iteration 47: w = [2. 3.], b = 2.999999999670753, loss = 46.855878349757056\n",
      "Iteration 48: w = [2. 3.], b = 2.9999999996734164, loss = 1.7732225343532764\n",
      "Iteration 49: w = [2. 3.], b = 2.999999999655974, loss = 76.05822173108535\n",
      "Iteration 50: w = [2. 3.], b = 2.999999999640571, loss = 59.31180403509044\n",
      "Iteration 51: w = [2. 3.], b = 2.9999999996311772, loss = 22.060649514483796\n",
      "Iteration 52: w = [2. 3.], b = 2.999999999616647, loss = 52.77982591495866\n",
      "Iteration 53: w = [2. 3.], b = 2.9999999996064597, loss = 25.946932082803173\n",
      "Iteration 54: w = [2. 3.], b = 2.9999999996060964, loss = 0.032964077691732464\n",
      "Iteration 55: w = [2. 3.], b = 2.999999999601537, loss = 5.1974142598245\n",
      "Iteration 56: w = [2. 3.], b = 2.999999999602997, loss = 0.5328616285032136\n",
      "Iteration 57: w = [2. 3.], b = 2.999999999604017, loss = 0.2600331824512902\n",
      "Iteration 58: w = [2. 3.], b = 2.999999999590782, loss = 43.79130941079286\n",
      "Iteration 59: w = [2. 3.], b = 2.9999999995854583, loss = 7.085238397847508\n",
      "Iteration 60: w = [2. 3.], b = 2.999999999574642, loss = 29.24744055421046\n",
      "Iteration 61: w = [2. 3.], b = 2.999999999561411, loss = 43.765843367512744\n",
      "Iteration 62: w = [2. 3.], b = 2.9999999995608992, loss = 0.06544188231914262\n",
      "Iteration 63: w = [2. 3.], b = 2.999999999560391, loss = 0.06447845863070338\n",
      "Iteration 64: w = [2. 3.], b = 2.999999999545292, loss = 56.99579040689267\n",
      "Iteration 65: w = [2. 3.], b = 2.999999999534363, loss = 29.86212292757689\n",
      "Iteration 66: w = [2. 3.], b = 2.999999999538022, loss = 3.3464074289602097\n",
      "Iteration 67: w = [2. 3.], b = 2.9999999995355773, loss = 1.4939343856394587\n",
      "Iteration 68: w = [2. 3.], b = 2.9999999995380597, loss = 1.5403939558066113\n",
      "Iteration 69: w = [2. 3.], b = 2.999999999541337, loss = 2.684950596492394\n",
      "Iteration 70: w = [2. 3.], b = 2.999999999533969, loss = 13.57370638901231\n",
      "Iteration 71: w = [2. 3.], b = 2.999999999531473, loss = 1.5570005344557991\n",
      "Iteration 72: w = [2. 3.], b = 2.9999999995350577, loss = 3.2128218435558877\n",
      "Iteration 73: w = [2. 3.], b = 2.9999999995272155, loss = 15.375024026390177\n",
      "Iteration 74: w = [2. 3.], b = 2.9999999995193245, loss = 15.566792192264963\n",
      "Iteration 75: w = [2. 3.], b = 2.9999999995207975, loss = 0.5425861192817278\n",
      "Iteration 76: w = [2. 3.], b = 2.999999999512753, loss = 16.18003621929977\n",
      "Iteration 77: w = [2. 3.], b = 2.999999999504578, loss = 16.705937382633785\n",
      "Iteration 78: w = [2. 3.], b = 2.9999999994951323, loss = 22.30551748692398\n",
      "Iteration 79: w = [2. 3.], b = 2.99999999948347, loss = 34.000773541750064\n",
      "Iteration 80: w = [2. 3.], b = 2.9999999994886566, loss = 6.724941871585755\n",
      "Iteration 81: w = [2. 3.], b = 2.9999999994910618, loss = 1.4464040764179287\n",
      "Iteration 82: w = [2. 3.], b = 2.999999999495871, loss = 5.781928150004316\n",
      "Iteration 83: w = [2. 3.], b = 2.9999999994939026, loss = 0.9682716724548557\n",
      "Iteration 84: w = [2. 3.], b = 2.9999999994812274, loss = 40.165323008304654\n",
      "Iteration 85: w = [2. 3.], b = 2.99999999948568, loss = 4.95650073063166\n",
      "Iteration 86: w = [2. 3.], b = 2.9999999994767053, loss = 20.13506343642432\n",
      "Iteration 87: w = [2. 3.], b = 2.9999999994724296, loss = 4.5706846218290185\n",
      "Iteration 88: w = [2. 3.], b = 2.9999999994710143, loss = 0.5007456069208034\n",
      "Iteration 89: w = [2. 3.], b = 2.999999999471119, loss = 0.002752283684181237\n",
      "Iteration 90: w = [2. 3.], b = 2.999999999476145, loss = 6.3142838575822715\n",
      "Iteration 91: w = [2. 3.], b = 2.9999999994551243, loss = 110.46574691203317\n",
      "Iteration 92: w = [2. 3.], b = 2.999999999443797, loss = 32.077778655364675\n",
      "Iteration 93: w = [2. 3.], b = 2.999999999434384, loss = 22.150214764454372\n",
      "Iteration 94: w = [2. 3.], b = 2.9999999994265507, loss = 15.339389180037402\n",
      "Iteration 95: w = [2. 3.], b = 2.9999999994234265, loss = 2.4402397893185825\n",
      "Iteration 96: w = [2. 3.], b = 2.999999999420319, loss = 2.414351510523496\n",
      "Iteration 97: w = [2. 3.], b = 2.999999999406964, loss = 44.58701886917187\n",
      "Iteration 98: w = [2. 3.], b = 2.9999999994067315, loss = 0.013550207906897828\n",
      "Iteration 99: w = [2. 3.], b = 2.9999999994016138, loss = 6.547856437191569\n",
      "Iteration 100: w = [2. 3.], b = 2.9999999994030344, loss = 0.5044356944349132\n",
      "Iteration 101: w = [2. 3.], b = 2.99999999940231, loss = 0.1311119183293954\n",
      "Iteration 102: w = [2. 3.], b = 2.9999999994036863, loss = 0.4735621911171181\n",
      "Iteration 103: w = [2. 3.], b = 2.9999999994025384, loss = 0.3294705650989019\n",
      "Iteration 104: w = [2. 3.], b = 2.9999999994013784, loss = 0.3363976372685751\n",
      "Iteration 105: w = [2. 3.], b = 2.999999999383214, loss = 82.48656264319902\n",
      "Iteration 106: w = [2. 3.], b = 2.999999999386408, loss = 2.550878762026895\n",
      "Iteration 107: w = [2. 3.], b = 2.9999999993752926, loss = 30.889175287828404\n",
      "Iteration 108: w = [2. 3.], b = 2.9999999993703756, loss = 6.044471169568623\n",
      "Iteration 109: w = [2. 3.], b = 2.999999999367954, loss = 1.4661237179087634\n",
      "Iteration 110: w = [2. 3.], b = 2.9999999993725743, loss = 5.336354576542584\n",
      "Iteration 111: w = [2. 3.], b = 2.999999999373733, loss = 0.33560422563862896\n",
      "Iteration 112: w = [2. 3.], b = 2.9999999993641704, loss = 22.86150817119698\n",
      "Iteration 113: w = [2. 3.], b = 2.9999999993640962, loss = 0.0013828845187397372\n",
      "Iteration 114: w = [2. 3.], b = 2.9999999993511115, loss = 42.15017672745278\n",
      "Iteration 115: w = [2. 3.], b = 2.9999999993415636, loss = 22.79088389755768\n",
      "Iteration 116: w = [2. 3.], b = 2.999999999329172, loss = 38.386209774740124\n",
      "Iteration 117: w = [2. 3.], b = 2.9999999993322146, loss = 2.3141508389233305\n",
      "Iteration 118: w = [2. 3.], b = 2.99999999932397, loss = 16.99287314622179\n",
      "Iteration 119: w = [2. 3.], b = 2.9999999993013917, loss = 127.44483831527832\n",
      "Iteration 120: w = [2. 3.], b = 2.9999999993041073, loss = 1.8435114757129978\n",
      "Iteration 121: w = [2. 3.], b = 2.9999999992972324, loss = 11.816059800623366\n",
      "Iteration 122: w = [2. 3.], b = 2.9999999993033635, loss = 9.397161836866752\n",
      "Iteration 123: w = [2. 3.], b = 2.999999999284773, loss = 86.40232014505568\n",
      "Iteration 124: w = [2. 3.], b = 2.999999999282629, loss = 1.1491412093350593\n",
      "Iteration 125: w = [2. 3.], b = 2.999999999275592, loss = 12.380466474120391\n",
      "Iteration 126: w = [2. 3.], b = 2.9999999992790993, loss = 3.075281556950762\n",
      "Iteration 127: w = [2. 3.], b = 2.999999999271575, loss = 14.153890650872897\n",
      "Iteration 128: w = [2. 3.], b = 2.9999999992595163, loss = 36.352726052634175\n",
      "Iteration 129: w = [2. 3.], b = 2.999999999251745, loss = 15.09685048134333\n",
      "Iteration 130: w = [2. 3.], b = 2.9999999992499657, loss = 0.7915851437462545\n",
      "Iteration 131: w = [2. 3.], b = 2.9999999992547375, loss = 5.691845943781071\n",
      "Iteration 132: w = [2. 3.], b = 2.999999999259475, loss = 5.610983766075405\n",
      "Iteration 133: w = [2. 3.], b = 2.9999999992516857, loss = 15.168855985434808\n",
      "Iteration 134: w = [2. 3.], b = 2.999999999248128, loss = 3.1640844887770725\n",
      "Iteration 135: w = [2. 3.], b = 2.9999999992488355, loss = 0.12512302692075486\n",
      "Iteration 136: w = [2. 3.], b = 2.99999999925092, loss = 1.0864189127493478\n",
      "Iteration 137: w = [2. 3.], b = 2.9999999992425304, loss = 17.59715527032064\n",
      "Iteration 138: w = [2. 3.], b = 2.999999999223409, loss = 91.40637551676028\n",
      "Iteration 139: w = [2. 3.], b = 2.9999999992122865, loss = 30.92961595530181\n",
      "Iteration 140: w = [2. 3.], b = 2.999999999212615, loss = 0.027016501865834153\n",
      "Iteration 141: w = [2. 3.], b = 2.999999999203722, loss = 19.771912394388778\n",
      "Iteration 142: w = [2. 3.], b = 2.9999999992077444, loss = 4.044858874212872\n",
      "Iteration 143: w = [2. 3.], b = 2.99999999920138, loss = 10.126040770126844\n",
      "Iteration 144: w = [2. 3.], b = 2.9999999992011785, loss = 0.010152442533866922\n",
      "Iteration 145: w = [2. 3.], b = 2.9999999991848947, loss = 66.29181844456832\n",
      "Iteration 146: w = [2. 3.], b = 2.9999999991662145, loss = 87.23551752297624\n",
      "Iteration 147: w = [2. 3.], b = 2.9999999991723665, loss = 9.461169799697915\n",
      "Iteration 148: w = [2. 3.], b = 2.9999999991758877, loss = 3.0995429499177396\n",
      "Iteration 149: w = [2. 3.], b = 2.9999999991708286, loss = 6.398670005891606\n",
      "Iteration 150: w = [2. 3.], b = 2.9999999991654582, loss = 7.210668975625087\n",
      "Iteration 151: w = [2. 3.], b = 2.999999999168714, loss = 2.6498660181764575\n",
      "Iteration 152: w = [2. 3.], b = 2.9999999991518385, loss = 71.1931868661223\n",
      "Iteration 153: w = [2. 3.], b = 2.9999999991549497, loss = 2.419822136965651\n",
      "Iteration 154: w = [2. 3.], b = 2.999999999154833, loss = 0.0034110750832407263\n",
      "Iteration 155: w = [2. 3.], b = 2.9999999991374393, loss = 75.63619865460034\n",
      "Iteration 156: w = [2. 3.], b = 2.9999999991263824, loss = 30.565155100971364\n",
      "Iteration 157: w = [2. 3.], b = 2.9999999991146336, loss = 34.50778924600258\n",
      "Iteration 158: w = [2. 3.], b = 2.999999999110772, loss = 3.727595699713315\n",
      "Iteration 159: w = [2. 3.], b = 2.9999999990985966, loss = 37.06067844223031\n",
      "Iteration 160: w = [2. 3.], b = 2.999999999084733, loss = 48.048434864145094\n",
      "Iteration 161: w = [2. 3.], b = 2.9999999990778923, loss = 11.699309979854133\n",
      "Iteration 162: w = [2. 3.], b = 2.999999999070981, loss = 11.941012491143523\n",
      "Iteration 163: w = [2. 3.], b = 2.9999999990639474, loss = 12.368070015505753\n",
      "Iteration 164: w = [2. 3.], b = 2.9999999990509156, loss = 42.45604199147073\n",
      "Iteration 165: w = [2. 3.], b = 2.99999999904067, loss = 26.242223768109522\n",
      "Iteration 166: w = [2. 3.], b = 2.9999999990370365, loss = 3.300475029340181\n",
      "Iteration 167: w = [2. 3.], b = 2.999999999036152, loss = 0.1957095397220789\n",
      "Iteration 168: w = [2. 3.], b = 2.9999999990297628, loss = 10.205734662615098\n",
      "Iteration 169: w = [2. 3.], b = 2.999999999030596, loss = 0.17348951391103462\n",
      "Iteration 170: w = [2. 3.], b = 2.9999999990332897, loss = 1.81427907635446\n",
      "Iteration 171: w = [2. 3.], b = 2.9999999990223905, loss = 29.69787126699083\n",
      "Iteration 172: w = [2. 3.], b = 2.99999999902363, loss = 0.38407544296693935\n",
      "Iteration 173: w = [2. 3.], b = 2.9999999990062762, loss = 75.28616772340375\n",
      "Iteration 174: w = [2. 3.], b = 2.9999999990069335, loss = 0.10803601883804319\n",
      "Iteration 175: w = [2. 3.], b = 2.999999999006114, loss = 0.16790240266256268\n",
      "Iteration 176: w = [2. 3.], b = 2.9999999989933177, loss = 40.935906167021635\n",
      "Iteration 177: w = [2. 3.], b = 2.9999999989872994, loss = 9.055236218602747\n",
      "Iteration 178: w = [2. 3.], b = 2.9999999989865738, loss = 0.1316564985136283\n",
      "Iteration 179: w = [2. 3.], b = 2.9999999989676196, loss = 89.81680154519124\n",
      "Iteration 180: w = [2. 3.], b = 2.999999998963478, loss = 4.288156180277282\n",
      "Iteration 181: w = [2. 3.], b = 2.9999999989690345, loss = 7.718523677446117\n",
      "Iteration 182: w = [2. 3.], b = 2.9999999989734425, loss = 4.8576151210203555\n",
      "Iteration 183: w = [2. 3.], b = 2.9999999989546575, loss = 88.2203446961606\n",
      "Iteration 184: w = [2. 3.], b = 2.99999999895086, loss = 3.6046900556072266\n",
      "Iteration 185: w = [2. 3.], b = 2.9999999989374877, loss = 44.70671715561211\n",
      "Iteration 186: w = [2. 3.], b = 2.999999998924075, loss = 44.97570658195273\n",
      "Iteration 187: w = [2. 3.], b = 2.999999998924574, loss = 0.062236943402455544\n",
      "Iteration 188: w = [2. 3.], b = 2.9999999989028927, loss = 117.51825354074563\n",
      "Iteration 189: w = [2. 3.], b = 2.9999999988737938, loss = 211.68531650703832\n",
      "Iteration 190: w = [2. 3.], b = 2.999999998854311, loss = 94.8923623022518\n",
      "Iteration 191: w = [2. 3.], b = 2.9999999988595576, loss = 6.881471714424274\n",
      "Iteration 192: w = [2. 3.], b = 2.999999998853329, loss = 9.698999136116358\n",
      "Iteration 193: w = [2. 3.], b = 2.9999999988489487, loss = 4.7961840326981\n",
      "Iteration 194: w = [2. 3.], b = 2.999999998844846, loss = 4.208363169398075\n",
      "Iteration 195: w = [2. 3.], b = 2.9999999988437853, loss = 0.28108407405536456\n",
      "Iteration 196: w = [2. 3.], b = 2.9999999988480717, loss = 4.593336254570811\n",
      "Iteration 197: w = [2. 3.], b = 2.999999998846892, loss = 0.34773436136290964\n",
      "Iteration 198: w = [2. 3.], b = 2.9999999988185544, loss = 200.7565770135829\n",
      "Iteration 199: w = [2. 3.], b = 2.9999999988160186, loss = 1.607755362036075\n",
      "Iteration 200: w = [2. 3.], b = 2.999999998815632, loss = 0.037395653547916734\n",
      "Iteration 201: w = [2. 3.], b = 2.9999999988019703, loss = 46.658020462276035\n",
      "Iteration 202: w = [2. 3.], b = 2.99999999879377, loss = 16.810046566583967\n",
      "Iteration 203: w = [2. 3.], b = 2.9999999987855994, loss = 16.690001133028215\n",
      "Iteration 204: w = [2. 3.], b = 2.999999998773561, loss = 36.2318645282154\n",
      "Iteration 205: w = [2. 3.], b = 2.9999999987766905, loss = 2.4484913107894495\n",
      "Iteration 206: w = [2. 3.], b = 2.9999999987770325, loss = 0.029225272091459017\n",
      "Iteration 207: w = [2. 3.], b = 2.9999999987673074, loss = 23.643366000521503\n",
      "Iteration 208: w = [2. 3.], b = 2.9999999987627533, loss = 5.184624894883653\n",
      "Iteration 209: w = [2. 3.], b = 2.9999999987474077, loss = 58.87114149983784\n",
      "Iteration 210: w = [2. 3.], b = 2.999999998743548, loss = 3.7239637735988014\n",
      "Iteration 211: w = [2. 3.], b = 2.999999998739041, loss = 5.07791919639947\n",
      "Iteration 212: w = [2. 3.], b = 2.9999999987281227, loss = 29.8032482845305\n",
      "Iteration 213: w = [2. 3.], b = 2.9999999987311856, loss = 2.345440467564119\n",
      "Iteration 214: w = [2. 3.], b = 2.9999999987257913, loss = 7.2752013452360735\n",
      "Iteration 215: w = [2. 3.], b = 2.9999999987135446, loss = 37.49569192450146\n",
      "Iteration 216: w = [2. 3.], b = 2.999999998696358, loss = 73.84430198294987\n",
      "Iteration 217: w = [2. 3.], b = 2.9999999986839665, loss = 38.38681480827521\n",
      "Iteration 218: w = [2. 3.], b = 2.999999998686611, loss = 1.7481246943811282\n",
      "Iteration 219: w = [2. 3.], b = 2.9999999986862003, loss = 0.0422018277469904\n",
      "Iteration 220: w = [2. 3.], b = 2.999999998680322, loss = 8.638564548274598\n",
      "Iteration 221: w = [2. 3.], b = 2.9999999986688137, loss = 33.11002621273482\n",
      "Iteration 222: w = [2. 3.], b = 2.999999998660843, loss = 15.881711997909594\n",
      "Iteration 223: w = [2. 3.], b = 2.99999999864383, loss = 72.36128509337873\n",
      "Iteration 224: w = [2. 3.], b = 2.999999998641177, loss = 1.7598813356597593\n",
      "Iteration 225: w = [2. 3.], b = 2.9999999986182613, loss = 131.28256376303332\n",
      "Iteration 226: w = [2. 3.], b = 2.9999999986131183, loss = 6.6123341592397855\n",
      "Iteration 227: w = [2. 3.], b = 2.999999998602669, loss = 27.29873250636958\n",
      "Iteration 228: w = [2. 3.], b = 2.999999998601231, loss = 0.5168773632648489\n",
      "Iteration 229: w = [2. 3.], b = 2.9999999985927603, loss = 17.937666098222515\n",
      "Iteration 230: w = [2. 3.], b = 2.9999999985907757, loss = 0.9845378553864084\n",
      "Iteration 231: w = [2. 3.], b = 2.999999998589482, loss = 0.4184603518984766\n",
      "Iteration 232: w = [2. 3.], b = 2.9999999985760906, loss = 44.83203547112277\n",
      "Iteration 233: w = [2. 3.], b = 2.9999999985631733, loss = 41.7125158973004\n",
      "Iteration 234: w = [2. 3.], b = 2.9999999985488324, loss = 51.4153692072703\n",
      "Iteration 235: w = [2. 3.], b = 2.9999999985462606, loss = 1.6535239092340597\n",
      "Iteration 236: w = [2. 3.], b = 2.9999999985319, loss = 51.55484255934957\n",
      "Iteration 237: w = [2. 3.], b = 2.9999999985324495, loss = 0.07538720359405528\n",
      "Iteration 238: w = [2. 3.], b = 2.999999998540026, loss = 14.350585038025512\n",
      "Iteration 239: w = [2. 3.], b = 2.9999999985414996, loss = 0.5429368466282364\n",
      "Iteration 240: w = [2. 3.], b = 2.9999999985397223, loss = 0.78973105406452\n",
      "Iteration 241: w = [2. 3.], b = 2.9999999985449324, loss = 6.786534658932169\n",
      "Iteration 242: w = [2. 3.], b = 2.9999999985469956, loss = 1.0641832785715084\n",
      "Iteration 243: w = [2. 3.], b = 2.9999999985499377, loss = 2.163659332932742\n",
      "Iteration 244: w = [2. 3.], b = 2.9999999985341987, loss = 61.92781603782546\n",
      "Iteration 245: w = [2. 3.], b = 2.9999999985248174, loss = 22.002792770663337\n",
      "Iteration 246: w = [2. 3.], b = 2.999999998504857, loss = 99.60638699639024\n",
      "Iteration 247: w = [2. 3.], b = 2.999999998506158, loss = 0.42339567526246896\n",
      "Iteration 248: w = [2. 3.], b = 2.9999999985013974, loss = 5.665985505951147\n",
      "Iteration 249: w = [2. 3.], b = 2.99999999849366, loss = 14.96660899250414\n",
      "Iteration 250: w = [2. 3.], b = 2.9999999984929175, loss = 0.13778427129084594\n",
      "Iteration 251: w = [2. 3.], b = 2.999999998488119, loss = 5.756012429374923\n",
      "Iteration 252: w = [2. 3.], b = 2.999999998490924, loss = 1.9669156112600379\n",
      "Iteration 253: w = [2. 3.], b = 2.999999998491249, loss = 0.026434397350539974\n",
      "Iteration 254: w = [2. 3.], b = 2.999999998483458, loss = 15.174510124017319\n",
      "Iteration 255: w = [2. 3.], b = 2.999999998484892, loss = 0.5141233562253517\n",
      "Iteration 256: w = [2. 3.], b = 2.999999998474827, loss = 25.32542621251044\n",
      "Iteration 257: w = [2. 3.], b = 2.999999998473515, loss = 0.43039463420816465\n",
      "Iteration 258: w = [2. 3.], b = 2.999999998476128, loss = 1.7070603566172091\n",
      "Iteration 259: w = [2. 3.], b = 2.999999998475419, loss = 0.12552251687615745\n",
      "Iteration 260: w = [2. 3.], b = 2.9999999984784065, loss = 2.231280545459727\n",
      "Iteration 261: w = [2. 3.], b = 2.9999999984721946, loss = 9.64699786109876\n",
      "Iteration 262: w = [2. 3.], b = 2.9999999984609236, loss = 31.759426026196437\n",
      "Iteration 263: w = [2. 3.], b = 2.999999998442669, loss = 83.30697395921419\n",
      "Iteration 264: w = [2. 3.], b = 2.9999999984354684, loss = 12.962266130655234\n",
      "Iteration 265: w = [2. 3.], b = 2.999999998433664, loss = 0.813782172106853\n",
      "Iteration 266: w = [2. 3.], b = 2.999999998438912, loss = 6.884720878353285\n",
      "Iteration 267: w = [2. 3.], b = 2.9999999984414996, loss = 1.6740552806300282\n",
      "Iteration 268: w = [2. 3.], b = 2.999999998438027, loss = 3.015401900983393\n",
      "Iteration 269: w = [2. 3.], b = 2.9999999984427212, loss = 5.509705878360747\n",
      "Iteration 270: w = [2. 3.], b = 2.9999999984281622, loss = 52.992028828193604\n",
      "Iteration 271: w = [2. 3.], b = 2.9999999984207384, loss = 13.778711311632415\n",
      "Iteration 272: w = [2. 3.], b = 2.9999999984059422, loss = 54.73004522031922\n",
      "Iteration 273: w = [2. 3.], b = 2.9999999984010253, loss = 6.04364850226295\n",
      "Iteration 274: w = [2. 3.], b = 2.999999998394111, loss = 11.952183566768882\n",
      "Iteration 275: w = [2. 3.], b = 2.9999999983932306, loss = 0.19367277080772546\n",
      "Iteration 276: w = [2. 3.], b = 2.9999999983849652, loss = 17.078885114763946\n",
      "Iteration 277: w = [2. 3.], b = 2.999999998386941, loss = 0.9760232279441811\n",
      "Iteration 278: w = [2. 3.], b = 2.999999998386446, loss = 0.06124301648286857\n",
      "Iteration 279: w = [2. 3.], b = 2.99999999837986, loss = 10.843823044800933\n",
      "Iteration 280: w = [2. 3.], b = 2.999999998356283, loss = 138.9712142254661\n",
      "Iteration 281: w = [2. 3.], b = 2.9999999983554257, loss = 0.183585601694281\n",
      "Iteration 282: w = [2. 3.], b = 2.999999998338958, loss = 67.79527111839461\n",
      "Iteration 283: w = [2. 3.], b = 2.999999998339508, loss = 0.07561922420092518\n",
      "Iteration 284: w = [2. 3.], b = 2.9999999983386076, loss = 0.20252858696695558\n",
      "Iteration 285: w = [2. 3.], b = 2.9999999983305283, loss = 16.31803504698538\n",
      "Iteration 286: w = [2. 3.], b = 2.9999999983319836, loss = 0.5294554289773948\n",
      "Iteration 287: w = [2. 3.], b = 2.9999999983226524, loss = 21.7682849336871\n",
      "Iteration 288: w = [2. 3.], b = 2.999999998305377, loss = 74.61204938857385\n",
      "Iteration 289: w = [2. 3.], b = 2.9999999983004284, loss = 6.121607099896196\n",
      "Iteration 290: w = [2. 3.], b = 2.999999998300753, loss = 0.026374701820119903\n",
      "Iteration 291: w = [2. 3.], b = 2.999999998298925, loss = 0.8352196979039612\n",
      "Iteration 292: w = [2. 3.], b = 2.999999998296563, loss = 1.3946838290216912\n",
      "Iteration 293: w = [2. 3.], b = 2.9999999982838377, loss = 40.482672544328686\n",
      "Iteration 294: w = [2. 3.], b = 2.9999999982799435, loss = 3.7909398509936207\n",
      "Iteration 295: w = [2. 3.], b = 2.999999998273235, loss = 11.250344644424567\n",
      "Iteration 296: w = [2. 3.], b = 2.999999998270741, loss = 1.5548569834913466\n",
      "Iteration 297: w = [2. 3.], b = 2.999999998272546, loss = 0.8143229851099344\n",
      "Iteration 298: w = [2. 3.], b = 2.9999999982436334, loss = 208.9818126236953\n",
      "Iteration 299: w = [2. 3.], b = 2.999999998224808, loss = 88.59942436677439\n",
      "Iteration 300: w = [2. 3.], b = 2.9999999982232413, loss = 0.6137987769788064\n",
      "Iteration 301: w = [2. 3.], b = 2.99999999821967, loss = 3.1889214681796028\n",
      "Iteration 302: w = [2. 3.], b = 2.9999999982234797, loss = 3.6288060887302107\n",
      "Iteration 303: w = [2. 3.], b = 2.999999998216403, loss = 12.519417704415368\n",
      "Iteration 304: w = [2. 3.], b = 2.9999999981989633, loss = 76.03585382342793\n",
      "Iteration 305: w = [2. 3.], b = 2.9999999981922807, loss = 11.16496706613628\n",
      "Iteration 306: w = [2. 3.], b = 2.999999998183786, loss = 18.03996038111404\n",
      "Iteration 307: w = [2. 3.], b = 2.9999999981811203, loss = 1.7766135479287215\n",
      "Iteration 308: w = [2. 3.], b = 2.9999999981738914, loss = 13.06479407739455\n",
      "Iteration 309: w = [2. 3.], b = 2.9999999981821883, loss = 17.20957525389518\n",
      "Iteration 310: w = [2. 3.], b = 2.9999999981803622, loss = 0.8334989466696165\n",
      "Iteration 311: w = [2. 3.], b = 2.999999998166782, loss = 46.105641149320824\n",
      "Iteration 312: w = [2. 3.], b = 2.9999999981595353, loss = 13.129050488143838\n",
      "Iteration 313: w = [2. 3.], b = 2.9999999981502494, loss = 21.557600790814547\n",
      "Iteration 314: w = [2. 3.], b = 2.999999998139096, loss = 31.100088555247414\n",
      "Iteration 315: w = [2. 3.], b = 2.9999999981303045, loss = 19.32344373684126\n",
      "Iteration 316: w = [2. 3.], b = 2.9999999981097725, loss = 105.38910269996816\n",
      "Iteration 317: w = [2. 3.], b = 2.9999999980994336, loss = 26.723150343451042\n",
      "Iteration 318: w = [2. 3.], b = 2.999999998098351, loss = 0.29309970718003536\n",
      "Iteration 319: w = [2. 3.], b = 2.9999999980952503, loss = 2.4033613413160264\n",
      "Iteration 320: w = [2. 3.], b = 2.999999998066309, loss = 209.3990056969242\n",
      "Iteration 321: w = [2. 3.], b = 2.9999999980673486, loss = 0.2701052278434773\n",
      "Iteration 322: w = [2. 3.], b = 2.9999999980599554, loss = 13.66450898740468\n",
      "Iteration 323: w = [2. 3.], b = 2.9999999980575756, loss = 1.4158311861891053\n",
      "Iteration 324: w = [2. 3.], b = 2.9999999980489247, loss = 18.70918762916032\n",
      "Iteration 325: w = [2. 3.], b = 2.999999998040568, loss = 17.4595146040158\n",
      "Iteration 326: w = [2. 3.], b = 2.9999999980319694, loss = 18.483217773872276\n",
      "Iteration 327: w = [2. 3.], b = 2.999999998027663, loss = 4.636375347422698\n",
      "Iteration 328: w = [2. 3.], b = 2.9999999980149483, loss = 40.41629054386782\n",
      "Iteration 329: w = [2. 3.], b = 2.999999998017431, loss = 1.5405622367100364\n",
      "Iteration 330: w = [2. 3.], b = 2.9999999980148284, loss = 1.6933239106043325\n",
      "Iteration 331: w = [2. 3.], b = 2.999999998017867, loss = 2.308136727881842\n",
      "Iteration 332: w = [2. 3.], b = 2.9999999980048506, loss = 42.35669673991472\n",
      "Iteration 333: w = [2. 3.], b = 2.9999999980038847, loss = 0.23329213899668746\n",
      "Iteration 334: w = [2. 3.], b = 2.999999998001127, loss = 1.9011284953708094\n",
      "Iteration 335: w = [2. 3.], b = 2.9999999980072034, loss = 9.231125879016979\n",
      "Iteration 336: w = [2. 3.], b = 2.9999999980091023, loss = 0.901380299829871\n",
      "Iteration 337: w = [2. 3.], b = 2.9999999980006793, loss = 17.73763063015241\n",
      "Iteration 338: w = [2. 3.], b = 2.999999997980569, loss = 101.10641576607998\n",
      "Iteration 339: w = [2. 3.], b = 2.9999999979678402, loss = 40.50703956530255\n",
      "Iteration 340: w = [2. 3.], b = 2.9999999979494403, loss = 84.63790969746478\n",
      "Iteration 341: w = [2. 3.], b = 2.999999997949449, loss = 1.9042270283565923e-05\n",
      "Iteration 342: w = [2. 3.], b = 2.99999999792975, loss = 97.01758941979911\n",
      "Iteration 343: w = [2. 3.], b = 2.999999997916323, loss = 45.068863672167005\n",
      "Iteration 344: w = [2. 3.], b = 2.999999997901729, loss = 53.24846834307486\n",
      "Iteration 345: w = [2. 3.], b = 2.9999999978924916, loss = 21.332861507813266\n",
      "Iteration 346: w = [2. 3.], b = 2.9999999978965053, loss = 4.02729535760108\n",
      "Iteration 347: w = [2. 3.], b = 2.999999997896638, loss = 0.004409875059537419\n",
      "Iteration 348: w = [2. 3.], b = 2.999999997902534, loss = 8.68982237597414\n",
      "Iteration 349: w = [2. 3.], b = 2.999999997891916, loss = 28.185191407924844\n",
      "Iteration 350: w = [2. 3.], b = 2.999999997881588, loss = 26.66840246917893\n",
      "Iteration 351: w = [2. 3.], b = 2.9999999978675698, loss = 49.1263193342912\n",
      "Iteration 352: w = [2. 3.], b = 2.999999997863698, loss = 3.7471346153467016\n",
      "Iteration 353: w = [2. 3.], b = 2.999999997851127, loss = 39.50883189695921\n",
      "Iteration 354: w = [2. 3.], b = 2.9999999978427416, loss = 17.57795315583511\n",
      "Iteration 355: w = [2. 3.], b = 2.999999997828721, loss = 49.1441918293517\n",
      "Iteration 356: w = [2. 3.], b = 2.9999999978296157, loss = 0.20011388153339915\n",
      "Iteration 357: w = [2. 3.], b = 2.9999999978147693, loss = 55.103876500020625\n",
      "Iteration 358: w = [2. 3.], b = 2.9999999978176533, loss = 2.0794968585225964\n",
      "Iteration 359: w = [2. 3.], b = 2.999999997809236, loss = 17.712748540565524\n",
      "Iteration 360: w = [2. 3.], b = 2.999999997801441, loss = 15.191030652774884\n",
      "Iteration 361: w = [2. 3.], b = 2.9999999977893808, loss = 36.36203183584872\n",
      "Iteration 362: w = [2. 3.], b = 2.9999999977961065, loss = 11.308545149106287\n",
      "Iteration 363: w = [2. 3.], b = 2.9999999977888825, loss = 13.047184879074493\n",
      "Iteration 364: w = [2. 3.], b = 2.999999997779757, loss = 20.81812999254762\n",
      "Iteration 365: w = [2. 3.], b = 2.99999999778225, loss = 1.5541192617693016\n",
      "Iteration 366: w = [2. 3.], b = 2.9999999977795984, loss = 1.7579704979233768\n",
      "Iteration 367: w = [2. 3.], b = 2.9999999977760896, loss = 3.0774723328273677\n",
      "Iteration 368: w = [2. 3.], b = 2.999999997775947, loss = 0.00508789874728512\n",
      "Iteration 369: w = [2. 3.], b = 2.99999999777827, loss = 1.3489675411618587\n",
      "Iteration 370: w = [2. 3.], b = 2.9999999977568788, loss = 114.39611614125168\n",
      "Iteration 371: w = [2. 3.], b = 2.999999997756217, loss = 0.10946325689380756\n",
      "Iteration 372: w = [2. 3.], b = 2.9999999977473175, loss = 19.800018985862202\n",
      "Iteration 373: w = [2. 3.], b = 2.999999997733298, loss = 49.136845652305084\n",
      "Iteration 374: w = [2. 3.], b = 2.999999997732623, loss = 0.11386681612595752\n",
      "Iteration 375: w = [2. 3.], b = 2.9999999977196916, loss = 41.80605765598663\n",
      "Iteration 376: w = [2. 3.], b = 2.9999999977179814, loss = 0.7311174264356489\n",
      "Iteration 377: w = [2. 3.], b = 2.9999999977173055, loss = 0.11423941546709462\n",
      "Iteration 378: w = [2. 3.], b = 2.9999999977138643, loss = 2.960415185789341\n",
      "Iteration 379: w = [2. 3.], b = 2.9999999977040153, loss = 24.25146383121529\n",
      "Iteration 380: w = [2. 3.], b = 2.9999999977013654, loss = 1.7556723036586346\n",
      "Iteration 381: w = [2. 3.], b = 2.999999997692858, loss = 18.09484768744395\n",
      "Iteration 382: w = [2. 3.], b = 2.999999997694181, loss = 0.43754011221890315\n",
      "Iteration 383: w = [2. 3.], b = 2.999999997680057, loss = 49.87162948514238\n",
      "Iteration 384: w = [2. 3.], b = 2.9999999976789846, loss = 0.2876142488168607\n",
      "Iteration 385: w = [2. 3.], b = 2.9999999976716367, loss = 13.498706607001527\n",
      "Iteration 386: w = [2. 3.], b = 2.9999999976456277, loss = 169.11576590434484\n",
      "Iteration 387: w = [2. 3.], b = 2.9999999976364706, loss = 20.96381631791721\n",
      "Iteration 388: w = [2. 3.], b = 2.9999999976198324, loss = 69.2070690858172\n",
      "Iteration 389: w = [2. 3.], b = 2.999999997623476, loss = 3.3191222961149505\n",
      "Iteration 390: w = [2. 3.], b = 2.999999997619678, loss = 3.606691903499109\n",
      "Iteration 391: w = [2. 3.], b = 2.9999999976216225, loss = 0.9454238908847784\n",
      "Iteration 392: w = [2. 3.], b = 2.9999999976216243, loss = 7.808451403057608e-07\n",
      "Iteration 393: w = [2. 3.], b = 2.999999997612628, loss = 20.23304133791615\n",
      "Iteration 394: w = [2. 3.], b = 2.9999999975966225, loss = 64.04312071557172\n",
      "Iteration 395: w = [2. 3.], b = 2.9999999975828504, loss = 47.41848932858201\n",
      "Iteration 396: w = [2. 3.], b = 2.9999999975686538, loss = 50.385445749402194\n",
      "Iteration 397: w = [2. 3.], b = 2.999999997560367, loss = 17.166684662128315\n",
      "Iteration 398: w = [2. 3.], b = 2.999999997544115, loss = 66.032092307016\n",
      "Iteration 399: w = [2. 3.], b = 2.999999997537554, loss = 10.761719017189211\n",
      "Iteration 400: w = [2. 3.], b = 2.9999999975373606, loss = 0.009367261724045811\n",
      "Iteration 401: w = [2. 3.], b = 2.999999997525773, loss = 33.567986705781294\n",
      "Iteration 402: w = [2. 3.], b = 2.999999997520071, loss = 8.128506473413044\n",
      "Iteration 403: w = [2. 3.], b = 2.999999997513107, loss = 12.123064216594438\n",
      "Iteration 404: w = [2. 3.], b = 2.999999997507536, loss = 7.759141996279064\n",
      "Iteration 405: w = [2. 3.], b = 2.9999999974893714, loss = 82.48843768869767\n",
      "Iteration 406: w = [2. 3.], b = 2.9999999974905447, loss = 0.34426728570053755\n",
      "Iteration 407: w = [2. 3.], b = 2.9999999974935467, loss = 2.2533141334160236\n",
      "Iteration 408: w = [2. 3.], b = 2.999999997476965, loss = 68.74062904572402\n",
      "Iteration 409: w = [2. 3.], b = 2.999999997462966, loss = 48.99177048116524\n",
      "Iteration 410: w = [2. 3.], b = 2.999999997457716, loss = 6.89099692546982\n",
      "Iteration 411: w = [2. 3.], b = 2.9999999974522864, loss = 7.369864161164834\n",
      "Iteration 412: w = [2. 3.], b = 2.999999997434844, loss = 76.05839253402574\n",
      "Iteration 413: w = [2. 3.], b = 2.9999999974409057, loss = 9.186766704126065\n",
      "Iteration 414: w = [2. 3.], b = 2.999999997426321, loss = 53.18049301667078\n",
      "Iteration 415: w = [2. 3.], b = 2.9999999974199794, loss = 10.053754466956468\n",
      "Iteration 416: w = [2. 3.], b = 2.9999999974223717, loss = 1.4306915650681837\n",
      "Iteration 417: w = [2. 3.], b = 2.999999997425844, loss = 3.0142682940148555\n",
      "Iteration 418: w = [2. 3.], b = 2.999999997409672, loss = 65.38149107913316\n",
      "Iteration 419: w = [2. 3.], b = 2.9999999973950398, loss = 53.525893850429966\n",
      "Iteration 420: w = [2. 3.], b = 2.9999999974009435, loss = 8.713336492522737\n",
      "Iteration 421: w = [2. 3.], b = 2.9999999974050033, loss = 4.12080412126603\n",
      "Iteration 422: w = [2. 3.], b = 2.999999997402695, loss = 1.3320513730738015\n",
      "Iteration 423: w = [2. 3.], b = 2.999999997403392, loss = 0.12147999445260366\n",
      "Iteration 424: w = [2. 3.], b = 2.9999999973964937, loss = 11.897869816533543\n",
      "Iteration 425: w = [2. 3.], b = 2.9999999973928904, loss = 3.245699565241182\n",
      "Iteration 426: w = [2. 3.], b = 2.999999997377195, loss = 61.58766677391641\n",
      "Iteration 427: w = [2. 3.], b = 2.9999999973635267, loss = 46.70604361357086\n",
      "Iteration 428: w = [2. 3.], b = 2.9999999973573765, loss = 9.456275772003455\n",
      "Iteration 429: w = [2. 3.], b = 2.9999999973590286, loss = 0.6824331988464202\n",
      "Iteration 430: w = [2. 3.], b = 2.999999997355204, loss = 3.656377141369626\n",
      "Iteration 431: w = [2. 3.], b = 2.9999999973489633, loss = 9.736914945340073\n",
      "Iteration 432: w = [2. 3.], b = 2.9999999973441964, loss = 5.680441062001806\n",
      "Iteration 433: w = [2. 3.], b = 2.9999999973248377, loss = 93.68872416327753\n",
      "Iteration 434: w = [2. 3.], b = 2.9999999973284277, loss = 3.222084959884758\n",
      "Iteration 435: w = [2. 3.], b = 2.9999999973333633, loss = 6.0903116399283475\n",
      "Iteration 436: w = [2. 3.], b = 2.999999997330081, loss = 2.6932869446811316\n",
      "Iteration 437: w = [2. 3.], b = 2.999999997323015, loss = 12.482369963471456\n",
      "Iteration 438: w = [2. 3.], b = 2.999999997328355, loss = 7.128108140285661\n",
      "Iteration 439: w = [2. 3.], b = 2.999999997318487, loss = 24.342507422250694\n",
      "Iteration 440: w = [2. 3.], b = 2.999999997323811, loss = 7.085148576071412\n",
      "Iteration 441: w = [2. 3.], b = 2.9999999973252787, loss = 0.5384573286364676\n",
      "Iteration 442: w = [2. 3.], b = 2.999999997316606, loss = 18.80328635922307\n",
      "Iteration 443: w = [2. 3.], b = 2.9999999973126106, loss = 3.9913358874986953\n",
      "Iteration 444: w = [2. 3.], b = 2.9999999973091747, loss = 2.9511171325083625\n",
      "Iteration 445: w = [2. 3.], b = 2.999999997305223, loss = 3.903268061312715\n",
      "Iteration 446: w = [2. 3.], b = 2.9999999973024525, loss = 1.9193499549836675\n",
      "Iteration 447: w = [2. 3.], b = 2.9999999972975404, loss = 6.031853889338181\n",
      "Iteration 448: w = [2. 3.], b = 2.999999997299153, loss = 0.6501268897135543\n",
      "Iteration 449: w = [2. 3.], b = 2.9999999972990135, loss = 0.004853475737348399\n",
      "Iteration 450: w = [2. 3.], b = 2.9999999972911304, loss = 15.535509857182255\n",
      "Iteration 451: w = [2. 3.], b = 2.999999997281603, loss = 22.69347906318796\n",
      "Iteration 452: w = [2. 3.], b = 2.9999999972786426, loss = 2.1910778472520787\n",
      "Iteration 453: w = [2. 3.], b = 2.9999999972647444, loss = 48.2911511029743\n",
      "Iteration 454: w = [2. 3.], b = 2.9999999972687355, loss = 3.981671151568902\n",
      "Iteration 455: w = [2. 3.], b = 2.9999999972746787, loss = 8.83052141143281\n",
      "Iteration 456: w = [2. 3.], b = 2.999999997272975, loss = 0.725350260766766\n",
      "Iteration 457: w = [2. 3.], b = 2.999999997256244, loss = 69.98239740449596\n",
      "Iteration 458: w = [2. 3.], b = 2.9999999972529565, loss = 2.702321899212648\n",
      "Iteration 459: w = [2. 3.], b = 2.999999997233249, loss = 97.09663181705812\n",
      "Iteration 460: w = [2. 3.], b = 2.9999999972225, loss = 28.886676934014382\n",
      "Iteration 461: w = [2. 3.], b = 2.9999999972086426, loss = 48.00811577668604\n",
      "Iteration 462: w = [2. 3.], b = 2.9999999971892573, loss = 93.94725807745223\n",
      "Iteration 463: w = [2. 3.], b = 2.999999997181477, loss = 15.13464673532846\n",
      "Iteration 464: w = [2. 3.], b = 2.9999999971679934, loss = 45.45026845117453\n",
      "Iteration 465: w = [2. 3.], b = 2.999999997153931, loss = 49.43977506925716\n",
      "Iteration 466: w = [2. 3.], b = 2.999999997155291, loss = 0.4624342663798121\n",
      "Iteration 467: w = [2. 3.], b = 2.999999997153926, loss = 0.465812710632977\n",
      "Iteration 468: w = [2. 3.], b = 2.999999997149193, loss = 5.600117695200477\n",
      "Iteration 469: w = [2. 3.], b = 2.9999999971545823, loss = 7.261777975224367\n",
      "Iteration 470: w = [2. 3.], b = 2.999999997156377, loss = 0.8052977606937186\n",
      "Iteration 471: w = [2. 3.], b = 2.999999997147546, loss = 19.494714077577807\n",
      "Iteration 472: w = [2. 3.], b = 2.999999997124092, loss = 137.52558406155663\n",
      "Iteration 473: w = [2. 3.], b = 2.999999997120502, loss = 3.221658808363992\n",
      "Iteration 474: w = [2. 3.], b = 2.999999997107754, loss = 40.62784532977053\n",
      "Iteration 475: w = [2. 3.], b = 2.9999999970893185, loss = 84.96699627335136\n",
      "Iteration 476: w = [2. 3.], b = 2.9999999970896476, loss = 0.027063855780204503\n",
      "Iteration 477: w = [2. 3.], b = 2.9999999970815483, loss = 16.400476491191043\n",
      "Iteration 478: w = [2. 3.], b = 2.999999997070433, loss = 30.885182352899655\n",
      "Iteration 479: w = [2. 3.], b = 2.9999999970707596, loss = 0.026650601494985847\n",
      "Iteration 480: w = [2. 3.], b = 2.999999997077145, loss = 10.194322247044644\n",
      "Iteration 481: w = [2. 3.], b = 2.9999999970754243, loss = 0.7403022063572499\n",
      "Iteration 482: w = [2. 3.], b = 2.9999999970670053, loss = 17.720710124774563\n",
      "Iteration 483: w = [2. 3.], b = 2.999999997061823, loss = 6.713222563710956\n",
      "Iteration 484: w = [2. 3.], b = 2.999999997064242, loss = 1.4629582568942832\n",
      "Iteration 485: w = [2. 3.], b = 2.9999999970368325, loss = 187.82294840389883\n",
      "Iteration 486: w = [2. 3.], b = 2.999999997018224, loss = 86.57177442710825\n",
      "Iteration 487: w = [2. 3.], b = 2.999999997023646, loss = 7.350316853156686\n",
      "Iteration 488: w = [2. 3.], b = 2.9999999970077234, loss = 63.382635549868695\n",
      "Iteration 489: w = [2. 3.], b = 2.999999996990372, loss = 75.26920249374353\n",
      "Iteration 490: w = [2. 3.], b = 2.999999996973893, loss = 67.8889795421486\n",
      "Iteration 491: w = [2. 3.], b = 2.999999996968689, loss = 6.7710267826004635\n",
      "Iteration 492: w = [2. 3.], b = 2.9999999969709488, loss = 1.2770454579575963\n",
      "Iteration 493: w = [2. 3.], b = 2.9999999969698496, loss = 0.3019067538418083\n",
      "Iteration 494: w = [2. 3.], b = 2.999999996968724, loss = 0.3168212652743315\n",
      "Iteration 495: w = [2. 3.], b = 2.999999996956567, loss = 36.94647179989015\n",
      "Iteration 496: w = [2. 3.], b = 2.9999999969585436, loss = 0.976561417323233\n",
      "Iteration 497: w = [2. 3.], b = 2.9999999969630697, loss = 5.121787680575093\n",
      "Iteration 498: w = [2. 3.], b = 2.9999999969601787, loss = 2.089634700396392\n",
      "Iteration 499: w = [2. 3.], b = 2.999999996961877, loss = 0.7208492140761839\n",
      "Iteration 500: w = [2. 3.], b = 2.999999996948455, loss = 45.0362087541099\n",
      "Iteration 501: w = [2. 3.], b = 2.9999999969297724, loss = 87.26027290664457\n",
      "Iteration 502: w = [2. 3.], b = 2.9999999969225706, loss = 12.967098374169234\n",
      "Iteration 503: w = [2. 3.], b = 2.9999999969193762, loss = 2.5508996265526522\n",
      "Iteration 504: w = [2. 3.], b = 2.999999996906548, loss = 41.14170775408423\n",
      "Iteration 505: w = [2. 3.], b = 2.9999999968974715, loss = 20.593958483216177\n",
      "Iteration 506: w = [2. 3.], b = 2.999999996872092, loss = 161.0341985209091\n",
      "Iteration 507: w = [2. 3.], b = 2.9999999968710136, loss = 0.2905890088317888\n",
      "Iteration 508: w = [2. 3.], b = 2.999999996853263, loss = 78.77157317899423\n",
      "Iteration 509: w = [2. 3.], b = 2.9999999968496223, loss = 3.313586697458334\n",
      "Iteration 510: w = [2. 3.], b = 2.999999996838643, loss = 30.135523213263998\n",
      "Iteration 511: w = [2. 3.], b = 2.999999996841492, loss = 2.028793712561038\n",
      "Iteration 512: w = [2. 3.], b = 2.9999999968388025, loss = 1.8080723276267534\n",
      "Iteration 513: w = [2. 3.], b = 2.9999999968340503, loss = 5.645756333106535\n",
      "Iteration 514: w = [2. 3.], b = 2.999999996831602, loss = 1.4984347582785151\n",
      "Iteration 515: w = [2. 3.], b = 2.9999999968288797, loss = 1.8524105886167808\n",
      "Iteration 516: w = [2. 3.], b = 2.999999996827494, loss = 0.47999229762855217\n",
      "Iteration 517: w = [2. 3.], b = 2.9999999968150894, loss = 38.46815155688126\n",
      "Iteration 518: w = [2. 3.], b = 2.9999999968169146, loss = 0.8326715564475701\n",
      "Iteration 519: w = [2. 3.], b = 2.9999999968116273, loss = 6.988889703480272\n",
      "Iteration 520: w = [2. 3.], b = 2.9999999968021784, loss = 22.31931403471881\n",
      "Iteration 521: w = [2. 3.], b = 2.9999999968046573, loss = 1.5360943892633268\n",
      "Iteration 522: w = [2. 3.], b = 2.9999999967918116, loss = 41.25203046495067\n",
      "Iteration 523: w = [2. 3.], b = 2.9999999967749833, loss = 70.79647817934351\n",
      "Iteration 524: w = [2. 3.], b = 2.9999999967708675, loss = 4.235286761903104\n",
      "Iteration 525: w = [2. 3.], b = 2.999999996770716, loss = 0.005739537139815898\n",
      "Iteration 526: w = [2. 3.], b = 2.9999999967738873, loss = 2.5142811352504055\n",
      "Iteration 527: w = [2. 3.], b = 2.99999999675481, loss = 90.98612032152415\n",
      "Iteration 528: w = [2. 3.], b = 2.9999999967561854, loss = 0.47296046587363905\n",
      "Iteration 529: w = [2. 3.], b = 2.999999996761315, loss = 6.57808489243286\n",
      "Iteration 530: w = [2. 3.], b = 2.999999996753337, loss = 15.912377122731717\n",
      "Iteration 531: w = [2. 3.], b = 2.9999999967524587, loss = 0.19290633483468125\n",
      "Iteration 532: w = [2. 3.], b = 2.999999996733026, loss = 94.40447813851054\n",
      "Iteration 533: w = [2. 3.], b = 2.999999996733778, loss = 0.1413533142545731\n",
      "Iteration 534: w = [2. 3.], b = 2.9999999967319457, loss = 0.8392254306662921\n",
      "Iteration 535: w = [2. 3.], b = 2.9999999967171322, loss = 54.85911621914988\n",
      "Iteration 536: w = [2. 3.], b = 2.999999996710219, loss = 11.948471023396497\n",
      "Iteration 537: w = [2. 3.], b = 2.9999999967121145, loss = 0.8979413743700508\n",
      "Iteration 538: w = [2. 3.], b = 2.999999996713904, loss = 0.8006554298317005\n",
      "Iteration 539: w = [2. 3.], b = 2.999999996718982, loss = 6.445887660641478\n",
      "Iteration 540: w = [2. 3.], b = 2.9999999967233784, loss = 4.8319695796887\n",
      "Iteration 541: w = [2. 3.], b = 2.9999999967253235, loss = 0.9456516922581636\n",
      "Iteration 542: w = [2. 3.], b = 2.999999996728563, loss = 2.6237457474422214\n",
      "Iteration 543: w = [2. 3.], b = 2.999999996730941, loss = 1.4140418586582373\n",
      "Iteration 544: w = [2. 3.], b = 2.999999996731209, loss = 0.01790644609731416\n",
      "Iteration 545: w = [2. 3.], b = 2.999999996726399, loss = 5.7841160319194085\n",
      "Iteration 546: w = [2. 3.], b = 2.9999999967251307, loss = 0.4021774404141014\n",
      "Iteration 547: w = [2. 3.], b = 2.999999996729804, loss = 5.459718594470171\n",
      "Iteration 548: w = [2. 3.], b = 2.9999999967198163, loss = 24.93847161306923\n",
      "Iteration 549: w = [2. 3.], b = 2.9999999967058986, loss = 48.42631611981534\n",
      "Iteration 550: w = [2. 3.], b = 2.999999996687097, loss = 88.37311249605236\n",
      "Iteration 551: w = [2. 3.], b = 2.999999996683124, loss = 3.947087390077153\n",
      "Iteration 552: w = [2. 3.], b = 2.999999996674712, loss = 17.69058721016082\n",
      "Iteration 553: w = [2. 3.], b = 2.999999996678465, loss = 3.521150553770159\n",
      "Iteration 554: w = [2. 3.], b = 2.999999996672551, loss = 8.743928897965445\n",
      "Iteration 555: w = [2. 3.], b = 2.999999996649373, loss = 134.3039088491098\n",
      "Iteration 556: w = [2. 3.], b = 2.9999999966385364, loss = 29.35885882526916\n",
      "Iteration 557: w = [2. 3.], b = 2.999999996622426, loss = 64.88970143854363\n",
      "Iteration 558: w = [2. 3.], b = 2.999999996624454, loss = 1.028517408467437\n",
      "Iteration 559: w = [2. 3.], b = 2.999999996609493, loss = 55.956462792760945\n",
      "Iteration 560: w = [2. 3.], b = 2.9999999966050757, loss = 4.878528209604111\n",
      "Iteration 561: w = [2. 3.], b = 2.9999999965951147, loss = 24.804857502132975\n",
      "Iteration 562: w = [2. 3.], b = 2.9999999965965727, loss = 0.5314622174739987\n",
      "Iteration 563: w = [2. 3.], b = 2.999999996595515, loss = 0.2798496179853639\n",
      "Iteration 564: w = [2. 3.], b = 2.9999999965972988, loss = 0.795478240468395\n",
      "Iteration 565: w = [2. 3.], b = 2.999999996600245, loss = 2.1698570452868884\n",
      "Iteration 566: w = [2. 3.], b = 2.9999999965895805, loss = 28.431874597890904\n",
      "Iteration 567: w = [2. 3.], b = 2.9999999965834316, loss = 9.451894909896417\n",
      "Iteration 568: w = [2. 3.], b = 2.9999999965873867, loss = 3.9107184512639006\n",
      "Iteration 569: w = [2. 3.], b = 2.9999999965868707, loss = 0.06653107873193466\n",
      "Iteration 570: w = [2. 3.], b = 2.9999999965799984, loss = 11.80631714114746\n",
      "Epoch: 2\n",
      "Iteration 1: w = [2. 3.], b = 2.999999996584328, loss = 4.686395672823408\n",
      "Iteration 2: w = [2. 3.], b = 2.9999999965886075, loss = 4.578824184446892\n",
      "Iteration 3: w = [2. 3.], b = 2.9999999965927144, loss = 4.216424313144803\n",
      "Iteration 4: w = [2. 3.], b = 2.9999999965759616, loss = 70.16426725087877\n",
      "Iteration 5: w = [2. 3.], b = 2.999999996553907, loss = 121.6033226249192\n",
      "Iteration 6: w = [2. 3.], b = 2.999999996540469, loss = 45.14374864263524\n",
      "Iteration 7: w = [2. 3.], b = 2.999999996520211, loss = 102.59514628004675\n",
      "Iteration 8: w = [2. 3.], b = 2.9999999965165904, loss = 3.2775217872223306\n",
      "Iteration 9: w = [2. 3.], b = 2.9999999965101116, loss = 10.494218444064353\n",
      "Iteration 10: w = [2. 3.], b = 2.999999996515722, loss = 7.868642107347866\n",
      "Iteration 11: w = [2. 3.], b = 2.999999996512449, loss = 2.678279668849558\n",
      "Iteration 12: w = [2. 3.], b = 2.9999999965115296, loss = 0.21130429568532347\n",
      "Iteration 13: w = [2. 3.], b = 2.9999999964918906, loss = 96.42184412441974\n",
      "Iteration 14: w = [2. 3.], b = 2.9999999964774275, loss = 52.29615738402381\n",
      "Iteration 15: w = [2. 3.], b = 2.9999999964742843, loss = 2.4697647624811463\n",
      "Iteration 16: w = [2. 3.], b = 2.999999996441948, loss = 261.4109431295051\n",
      "Iteration 17: w = [2. 3.], b = 2.999999996437313, loss = 5.3702404257301195\n",
      "Iteration 18: w = [2. 3.], b = 2.999999996441644, loss = 4.6893602290437535\n",
      "Iteration 19: w = [2. 3.], b = 2.9999999964272464, loss = 51.82342562838876\n",
      "Iteration 20: w = [2. 3.], b = 2.999999996414201, loss = 42.54696711804489\n",
      "Iteration 21: w = [2. 3.], b = 2.999999996412156, loss = 1.0450798810283204\n",
      "Iteration 22: w = [2. 3.], b = 2.9999999964070954, loss = 6.403348842380741\n",
      "Iteration 23: w = [2. 3.], b = 2.9999999964060375, loss = 0.2797817714909397\n",
      "Iteration 24: w = [2. 3.], b = 2.999999996394696, loss = 32.156821541411944\n",
      "Iteration 25: w = [2. 3.], b = 2.99999999639595, loss = 0.39328085920974853\n",
      "Iteration 26: w = [2. 3.], b = 2.9999999963821367, loss = 47.70296482224984\n",
      "Iteration 27: w = [2. 3.], b = 2.9999999963698656, loss = 37.644881794645094\n",
      "Iteration 28: w = [2. 3.], b = 2.999999996361009, loss = 19.609889719758407\n",
      "Iteration 29: w = [2. 3.], b = 2.999999996354234, loss = 11.475741672737572\n",
      "Iteration 30: w = [2. 3.], b = 2.99999999634695, loss = 13.263269523253133\n",
      "Iteration 31: w = [2. 3.], b = 2.9999999963411534, loss = 8.40017135238168\n",
      "Iteration 32: w = [2. 3.], b = 2.999999996331771, loss = 22.005793768470536\n",
      "Iteration 33: w = [2. 3.], b = 2.9999999963331927, loss = 0.5050763782902786\n",
      "Iteration 34: w = [2. 3.], b = 2.9999999963255197, loss = 14.719079869126649\n",
      "Iteration 35: w = [2. 3.], b = 2.999999996315575, loss = 24.72636528026213\n",
      "Iteration 36: w = [2. 3.], b = 2.9999999963114683, loss = 4.2160774689985105\n",
      "Iteration 37: w = [2. 3.], b = 2.99999999631649, loss = 6.304221326023499\n",
      "Iteration 38: w = [2. 3.], b = 2.999999996311984, loss = 5.075939115145432\n",
      "Iteration 39: w = [2. 3.], b = 2.999999996314225, loss = 1.2556771925146306\n",
      "Iteration 40: w = [2. 3.], b = 2.9999999963085537, loss = 8.041466547909984\n",
      "Iteration 41: w = [2. 3.], b = 2.9999999962983397, loss = 26.081056731343075\n",
      "Iteration 42: w = [2. 3.], b = 2.9999999962945334, loss = 3.6216190008363527\n",
      "Iteration 43: w = [2. 3.], b = 2.9999999962864514, loss = 16.328926858493332\n",
      "Iteration 44: w = [2. 3.], b = 2.9999999962895547, loss = 2.4077951481478834\n",
      "Iteration 45: w = [2. 3.], b = 2.9999999962774435, loss = 36.66914378860723\n",
      "Iteration 46: w = [2. 3.], b = 2.999999996264442, loss = 42.26178989793949\n",
      "Iteration 47: w = [2. 3.], b = 2.9999999962507515, loss = 46.855878213510096\n",
      "Iteration 48: w = [2. 3.], b = 2.9999999962534147, loss = 1.77322251869237\n",
      "Iteration 49: w = [2. 3.], b = 2.9999999962359722, loss = 76.05822150448675\n",
      "Iteration 50: w = [2. 3.], b = 2.9999999962205695, loss = 59.31180383315513\n",
      "Iteration 51: w = [2. 3.], b = 2.9999999962111756, loss = 22.06064946968649\n",
      "Iteration 52: w = [2. 3.], b = 2.9999999961966455, loss = 52.7798257532837\n",
      "Iteration 53: w = [2. 3.], b = 2.999999996186458, loss = 25.946932008711283\n",
      "Iteration 54: w = [2. 3.], b = 2.999999996186095, loss = 0.032964078374528874\n",
      "Iteration 55: w = [2. 3.], b = 2.9999999961815353, loss = 5.197414246453021\n",
      "Iteration 56: w = [2. 3.], b = 2.9999999961829955, loss = 0.5328616212095852\n",
      "Iteration 57: w = [2. 3.], b = 2.9999999961840156, loss = 0.2600331787729665\n",
      "Iteration 58: w = [2. 3.], b = 2.9999999961707804, loss = 43.791309283410556\n",
      "Iteration 59: w = [2. 3.], b = 2.9999999961654567, loss = 7.085238378649172\n",
      "Iteration 60: w = [2. 3.], b = 2.9999999961546404, loss = 29.247440492409016\n",
      "Iteration 61: w = [2. 3.], b = 2.999999996141409, loss = 43.765843250456506\n",
      "Iteration 62: w = [2. 3.], b = 2.9999999961408976, loss = 0.0654418834991223\n",
      "Iteration 63: w = [2. 3.], b = 2.9999999961403896, loss = 0.06447846002703854\n",
      "Iteration 64: w = [2. 3.], b = 2.9999999961252906, loss = 56.99579026946041\n",
      "Iteration 65: w = [2. 3.], b = 2.9999999961143615, loss = 29.862122854929964\n",
      "Iteration 66: w = [2. 3.], b = 2.9999999961180204, loss = 3.3464074052391743\n",
      "Iteration 67: w = [2. 3.], b = 2.9999999961155757, loss = 1.493934389743431\n",
      "Iteration 68: w = [2. 3.], b = 2.999999996118058, loss = 1.5403939417684842\n",
      "Iteration 69: w = [2. 3.], b = 2.9999999961213355, loss = 2.6849505841484693\n",
      "Iteration 70: w = [2. 3.], b = 2.999999996113967, loss = 13.573706348440204\n",
      "Iteration 71: w = [2. 3.], b = 2.9999999961114714, loss = 1.557000527186648\n",
      "Iteration 72: w = [2. 3.], b = 2.999999996115056, loss = 3.2128218353470634\n",
      "Iteration 73: w = [2. 3.], b = 2.999999996107214, loss = 15.37502396226501\n",
      "Iteration 74: w = [2. 3.], b = 2.999999996099323, loss = 15.566792147096407\n",
      "Iteration 75: w = [2. 3.], b = 2.999999996100796, loss = 0.5425861130358575\n",
      "Iteration 76: w = [2. 3.], b = 2.9999999960927513, loss = 16.18003616301534\n",
      "Iteration 77: w = [2. 3.], b = 2.9999999960845765, loss = 16.70593732555519\n",
      "Iteration 78: w = [2. 3.], b = 2.9999999960751307, loss = 22.30551741595706\n",
      "Iteration 79: w = [2. 3.], b = 2.9999999960634685, loss = 34.000773430515665\n",
      "Iteration 80: w = [2. 3.], b = 2.999999996068655, loss = 6.724941838347959\n",
      "Iteration 81: w = [2. 3.], b = 2.99999999607106, loss = 1.446404065211775\n",
      "Iteration 82: w = [2. 3.], b = 2.999999996075869, loss = 5.781928117314337\n",
      "Iteration 83: w = [2. 3.], b = 2.999999996073901, loss = 0.9682716765776204\n",
      "Iteration 84: w = [2. 3.], b = 2.999999996061226, loss = 40.165322875051196\n",
      "Iteration 85: w = [2. 3.], b = 2.9999999960656782, loss = 4.956500705134678\n",
      "Iteration 86: w = [2. 3.], b = 2.9999999960567036, loss = 20.135063390877214\n",
      "Iteration 87: w = [2. 3.], b = 2.999999996052428, loss = 4.570684616548658\n",
      "Iteration 88: w = [2. 3.], b = 2.9999999960510126, loss = 0.5007456082964677\n",
      "Iteration 89: w = [2. 3.], b = 2.9999999960511174, loss = 0.002752283408031581\n",
      "Iteration 90: w = [2. 3.], b = 2.999999996056143, loss = 6.314283819864401\n",
      "Iteration 91: w = [2. 3.], b = 2.9999999960351227, loss = 110.46574657424306\n",
      "Iteration 92: w = [2. 3.], b = 2.9999999960237953, loss = 32.07777857130516\n",
      "Iteration 93: w = [2. 3.], b = 2.9999999960143824, loss = 22.15021468154792\n",
      "Iteration 94: w = [2. 3.], b = 2.999999996006549, loss = 15.339389146948845\n",
      "Iteration 95: w = [2. 3.], b = 2.999999996003425, loss = 2.440239790018855\n",
      "Iteration 96: w = [2. 3.], b = 2.999999996000317, loss = 2.4143515111583893\n",
      "Iteration 97: w = [2. 3.], b = 2.9999999959869625, loss = 44.58701870407151\n",
      "Iteration 98: w = [2. 3.], b = 2.99999999598673, loss = 0.013550207862115103\n",
      "Iteration 99: w = [2. 3.], b = 2.999999995981612, loss = 6.547856432306907\n",
      "Iteration 100: w = [2. 3.], b = 2.999999995983033, loss = 0.504435688632848\n",
      "Iteration 101: w = [2. 3.], b = 2.9999999959823085, loss = 0.1311119194185517\n",
      "Iteration 102: w = [2. 3.], b = 2.9999999959836847, loss = 0.47356218817399476\n",
      "Iteration 103: w = [2. 3.], b = 2.9999999959825367, loss = 0.3294705659806742\n",
      "Iteration 104: w = [2. 3.], b = 2.999999995981377, loss = 0.33639763377094856\n",
      "Iteration 105: w = [2. 3.], b = 2.999999995963212, loss = 82.4865624201374\n",
      "Iteration 106: w = [2. 3.], b = 2.9999999959664065, loss = 2.5508787478787736\n",
      "Iteration 107: w = [2. 3.], b = 2.999999995955291, loss = 30.889175174785553\n",
      "Iteration 108: w = [2. 3.], b = 2.999999995950374, loss = 6.044471156519977\n",
      "Iteration 109: w = [2. 3.], b = 2.9999999959479524, loss = 1.4661237232605093\n",
      "Iteration 110: w = [2. 3.], b = 2.9999999959525727, loss = 5.336354563461067\n",
      "Iteration 111: w = [2. 3.], b = 2.9999999959537313, loss = 0.33560422466146955\n",
      "Iteration 112: w = [2. 3.], b = 2.9999999959441688, loss = 22.861508105395018\n",
      "Iteration 113: w = [2. 3.], b = 2.9999999959440946, loss = 0.0013828846558544975\n",
      "Iteration 114: w = [2. 3.], b = 2.99999999593111, loss = 42.15017663369102\n",
      "Iteration 115: w = [2. 3.], b = 2.999999995921562, loss = 22.790883851375614\n",
      "Iteration 116: w = [2. 3.], b = 2.9999999959091705, loss = 38.38620965332215\n",
      "Iteration 117: w = [2. 3.], b = 2.999999995912213, loss = 2.3141508261822374\n",
      "Iteration 118: w = [2. 3.], b = 2.9999999959039685, loss = 16.992873083813652\n",
      "Iteration 119: w = [2.         2.99999999], b = 2.99999999588139, loss = 127.44483792345412\n",
      "Iteration 120: w = [2.         2.99999999], b = 2.9999999958841057, loss = 1.8435114640240797\n",
      "Iteration 121: w = [2.         2.99999999], b = 2.9999999958772308, loss = 11.816059789558146\n",
      "Iteration 122: w = [2.         2.99999999], b = 2.999999995883362, loss = 9.397161785969319\n",
      "Iteration 123: w = [2.         2.99999999], b = 2.9999999958647714, loss = 86.40231989127821\n",
      "Iteration 124: w = [2.         2.99999999], b = 2.9999999958626273, loss = 1.1491412111380548\n",
      "Iteration 125: w = [2.         2.99999999], b = 2.9999999958555903, loss = 12.38046645028784\n",
      "Iteration 126: w = [2.         2.99999999], b = 2.9999999958590977, loss = 3.075281537691198\n",
      "Iteration 127: w = [2.         2.99999999], b = 2.9999999958515735, loss = 14.15389060633929\n",
      "Iteration 128: w = [2.         2.99999999], b = 2.9999999958395147, loss = 36.35272591975947\n",
      "Iteration 129: w = [2.         2.99999999], b = 2.9999999958317436, loss = 15.0968504423679\n",
      "Iteration 130: w = [2.         2.99999999], b = 2.999999995829964, loss = 0.7915851405237796\n",
      "Iteration 131: w = [2.         2.99999999], b = 2.999999995834736, loss = 5.691845922603694\n",
      "Iteration 132: w = [2.         2.99999999], b = 2.9999999958394734, loss = 5.610983746653124\n",
      "Iteration 133: w = [2.         2.99999999], b = 2.999999995831684, loss = 15.168855934146542\n",
      "Iteration 134: w = [2.         2.99999999], b = 2.9999999958281265, loss = 3.164084490235659\n",
      "Iteration 135: w = [2.         2.99999999], b = 2.999999995828834, loss = 0.12512302650638865\n",
      "Iteration 136: w = [2.         2.99999999], b = 2.9999999958309185, loss = 1.0864189019856647\n",
      "Iteration 137: w = [2.         2.99999999], b = 2.9999999958225287, loss = 17.597155216498788\n",
      "Iteration 138: w = [2.         2.99999999], b = 2.9999999958034076, loss = 91.40637525585213\n",
      "Iteration 139: w = [2.         2.99999999], b = 2.999999995792285, loss = 30.929615877150773\n",
      "Iteration 140: w = [2.         2.99999999], b = 2.9999999957926136, loss = 0.027016500713659252\n",
      "Iteration 141: w = [2.         2.99999999], b = 2.9999999957837202, loss = 19.77191234661424\n",
      "Iteration 142: w = [2.         2.99999999], b = 2.999999995787743, loss = 4.044858859462361\n",
      "Iteration 143: w = [2.         2.99999999], b = 2.9999999957813785, loss = 10.126040752551608\n",
      "Iteration 144: w = [2.         2.99999999], b = 2.999999995781177, loss = 0.01015244255331529\n",
      "Iteration 145: w = [2.         2.99999999], b = 2.999999995764893, loss = 66.29181823140591\n",
      "Iteration 146: w = [2.         2.99999999], b = 2.999999995746213, loss = 87.23551725229228\n",
      "Iteration 147: w = [2.         2.99999999], b = 2.999999995752365, loss = 9.461169751472918\n",
      "Iteration 148: w = [2.         2.99999999], b = 2.999999995755886, loss = 3.0995429310366704\n",
      "Iteration 149: w = [2.         2.99999999], b = 2.999999995750827, loss = 6.39866998002264\n",
      "Iteration 150: w = [2.         2.99999999], b = 2.9999999957454566, loss = 7.210668960372109\n",
      "Iteration 151: w = [2.         2.99999999], b = 2.9999999957487122, loss = 2.6498660033576753\n",
      "Iteration 152: w = [2.         2.99999999], b = 2.999999995731837, loss = 71.19318666371798\n",
      "Iteration 153: w = [2.         2.99999999], b = 2.999999995734948, loss = 2.419822117633341\n",
      "Iteration 154: w = [2.         2.99999999], b = 2.9999999957348313, loss = 0.003411075411729333\n",
      "Iteration 155: w = [2.         2.99999999], b = 2.9999999957174377, loss = 75.63619843823118\n",
      "Iteration 156: w = [2.         2.99999999], b = 2.9999999957063808, loss = 30.56515500517947\n",
      "Iteration 157: w = [2.         2.99999999], b = 2.999999995694632, loss = 34.507789161739446\n",
      "Iteration 158: w = [2.         2.99999999], b = 2.9999999956907706, loss = 3.7275956930843264\n",
      "Iteration 159: w = [2.         2.99999999], b = 2.999999995678595, loss = 37.06067833977595\n",
      "Iteration 160: w = [2.         2.99999999], b = 2.9999999956647314, loss = 48.04843472000633\n",
      "Iteration 161: w = [2.         2.99999999], b = 2.9999999956578907, loss = 11.699309945882822\n",
      "Iteration 162: w = [2.         2.99999999], b = 2.9999999956509793, loss = 11.941012456830391\n",
      "Iteration 163: w = [2.         2.99999999], b = 2.999999995643946, loss = 12.368069964584086\n",
      "Iteration 164: w = [2.         2.99999999], b = 2.999999995630914, loss = 42.45604184562355\n",
      "Iteration 165: w = [2.         2.99999999], b = 2.9999999956206684, loss = 26.242223700810406\n",
      "Iteration 166: w = [2.         2.99999999], b = 2.999999995617035, loss = 3.30047502193918\n",
      "Iteration 167: w = [2.         2.99999999], b = 2.9999999956161503, loss = 0.1957095391991046\n",
      "Iteration 168: w = [2.         2.99999999], b = 2.999999995609761, loss = 10.205734626610813\n",
      "Iteration 169: w = [2.         2.99999999], b = 2.9999999956105943, loss = 0.17348951373416702\n",
      "Iteration 170: w = [2.         2.99999999], b = 2.999999995613288, loss = 1.814279068681932\n",
      "Iteration 171: w = [2.         2.99999999], b = 2.999999995602389, loss = 29.697871223221913\n",
      "Iteration 172: w = [2.         2.99999999], b = 2.9999999956036283, loss = 0.3840754406903235\n",
      "Iteration 173: w = [2.         2.99999999], b = 2.9999999955862746, loss = 75.28616751747418\n",
      "Iteration 174: w = [2.         2.99999999], b = 2.999999995586932, loss = 0.1080360188851076\n",
      "Iteration 175: w = [2.         2.99999999], b = 2.9999999955861125, loss = 0.1679024026234538\n",
      "Iteration 176: w = [2.         2.99999999], b = 2.999999995573316, loss = 40.93590605410773\n",
      "Iteration 177: w = [2.         2.99999999], b = 2.999999995567298, loss = 9.055236195244643\n",
      "Iteration 178: w = [2.         2.99999999], b = 2.999999995566572, loss = 0.13165649931405207\n",
      "Iteration 179: w = [2.         2.99999999], b = 2.999999995547618, loss = 89.81680129539986\n",
      "Iteration 180: w = [2.         2.99999999], b = 2.9999999955434764, loss = 4.288156177493765\n",
      "Iteration 181: w = [2.         2.99999999], b = 2.999999995549033, loss = 7.718523641652893\n",
      "Iteration 182: w = [2.         2.99999999], b = 2.999999995553441, loss = 4.857615097689329\n",
      "Iteration 183: w = [2.         2.99999999], b = 2.999999995534656, loss = 88.22034443253584\n",
      "Iteration 184: w = [2.         2.99999999], b = 2.9999999955308585, loss = 3.604690057879408\n",
      "Iteration 185: w = [2.         2.99999999], b = 2.999999995517486, loss = 44.706717005995365\n",
      "Iteration 186: w = [2.         2.99999999], b = 2.9999999955040733, loss = 44.975706453696816\n",
      "Iteration 187: w = [2.         2.99999999], b = 2.9999999955045724, loss = 0.06223694154756523\n",
      "Iteration 188: w = [2.         2.99999999], b = 2.999999995482891, loss = 117.51825314302233\n",
      "Iteration 189: w = [2.         2.99999999], b = 2.999999995453792, loss = 211.68531585692753\n",
      "Iteration 190: w = [2.         2.99999999], b = 2.9999999954343095, loss = 94.89236201018194\n",
      "Iteration 191: w = [2.         2.99999999], b = 2.999999995439556, loss = 6.8814716782826935\n",
      "Iteration 192: w = [2.         2.99999999], b = 2.999999995433327, loss = 9.698999120687203\n",
      "Iteration 193: w = [2.         2.99999999], b = 2.999999995428947, loss = 4.796184033834917\n",
      "Iteration 194: w = [2.         2.99999999], b = 2.999999995424844, loss = 4.208363162853725\n",
      "Iteration 195: w = [2.         2.99999999], b = 2.9999999954237837, loss = 0.2810840759776446\n",
      "Iteration 196: w = [2.         2.99999999], b = 2.99999999542807, loss = 4.593336225006832\n",
      "Iteration 197: w = [2.         2.99999999], b = 2.9999999954268906, loss = 0.34773436302354643\n",
      "Iteration 198: w = [2.         2.99999999], b = 2.9999999953985528, loss = 200.75657643616464\n",
      "Iteration 199: w = [2.         2.99999999], b = 2.999999995396017, loss = 1.6077553574913248\n",
      "Iteration 200: w = [2.         2.99999999], b = 2.9999999953956302, loss = 0.03739565441686002\n",
      "Iteration 201: w = [2.         2.99999999], b = 2.9999999953819687, loss = 46.658020330227636\n",
      "Iteration 202: w = [2.         2.99999999], b = 2.9999999953737686, loss = 16.81004652366911\n",
      "Iteration 203: w = [2.         2.99999999], b = 2.999999995365598, loss = 16.690001085422494\n",
      "Iteration 204: w = [2.         2.99999999], b = 2.9999999953535594, loss = 36.23186446631263\n",
      "Iteration 205: w = [2.         2.99999999], b = 2.999999995356689, loss = 2.4484913001042066\n",
      "Iteration 206: w = [2.         2.99999999], b = 2.999999995357031, loss = 0.029225270797051343\n",
      "Iteration 207: w = [2.         2.99999999], b = 2.9999999953473058, loss = 23.643365931805523\n",
      "Iteration 208: w = [2.         2.99999999], b = 2.9999999953427516, loss = 5.184624871837815\n",
      "Iteration 209: w = [2.         2.99999999], b = 2.999999995327406, loss = 58.8711413019415\n",
      "Iteration 210: w = [2.         2.99999999], b = 2.9999999953235466, loss = 3.7239637780964343\n",
      "Iteration 211: w = [2.         2.99999999], b = 2.9999999953190395, loss = 5.077919182799252\n",
      "Iteration 212: w = [2.         2.99999999], b = 2.999999995308121, loss = 29.80324818888066\n",
      "Iteration 213: w = [2.         2.99999999], b = 2.999999995311184, loss = 2.3454404646366247\n",
      "Iteration 214: w = [2.         2.99999999], b = 2.9999999953057896, loss = 7.27520131891913\n",
      "Iteration 215: w = [2.         2.99999999], b = 2.999999995293543, loss = 37.49569179229636\n",
      "Iteration 216: w = [2.         2.99999999], b = 2.9999999952763563, loss = 73.8443017645473\n",
      "Iteration 217: w = [2.         2.99999999], b = 2.999999995263965, loss = 38.386814713474074\n",
      "Iteration 218: w = [2.         2.99999999], b = 2.9999999952666094, loss = 1.7481246836838384\n",
      "Iteration 219: w = [2.         2.99999999], b = 2.9999999952661986, loss = 0.04220182866087342\n",
      "Iteration 220: w = [2.         2.99999999], b = 2.9999999952603202, loss = 8.63856452739698\n",
      "Iteration 221: w = [2.         2.99999999], b = 2.999999995248812, loss = 33.11002614888274\n",
      "Iteration 222: w = [2.         2.99999999], b = 2.9999999952408416, loss = 15.881711949698163\n",
      "Iteration 223: w = [2.         2.99999999], b = 2.9999999952238285, loss = 72.36128485252857\n",
      "Iteration 224: w = [2.         2.99999999], b = 2.9999999952211756, loss = 1.759881333313522\n",
      "Iteration 225: w = [2.         2.99999999], b = 2.9999999951982597, loss = 131.28256333624492\n",
      "Iteration 226: w = [2.         2.99999999], b = 2.9999999951931167, loss = 6.6123341580302055\n",
      "Iteration 227: w = [2.         2.99999999], b = 2.9999999951826672, loss = 27.298732448622197\n",
      "Iteration 228: w = [2.         2.99999999], b = 2.9999999951812293, loss = 0.5168773609181839\n",
      "Iteration 229: w = [2.         2.99999999], b = 2.9999999951727587, loss = 17.93766604978046\n",
      "Iteration 230: w = [2.         2.99999999], b = 2.999999995170774, loss = 0.984537850866134\n",
      "Iteration 231: w = [2.         2.99999999], b = 2.9999999951694805, loss = 0.41846035341016086\n",
      "Iteration 232: w = [2.         2.99999999], b = 2.999999995156089, loss = 44.832035346607796\n",
      "Iteration 233: w = [2.         2.99999999], b = 2.9999999951431717, loss = 41.71251574861532\n",
      "Iteration 234: w = [2.         2.99999999], b = 2.9999999951288308, loss = 51.41536903858279\n",
      "Iteration 235: w = [2.         2.99999999], b = 2.999999995126259, loss = 1.6535239134161845\n",
      "Iteration 236: w = [2.         2.99999999], b = 2.9999999951118985, loss = 51.55484241630605\n",
      "Iteration 237: w = [2.         2.99999999], b = 2.999999995112448, loss = 0.07538720186395607\n",
      "Iteration 238: w = [2.         2.99999999], b = 2.9999999951200245, loss = 14.350584976028843\n",
      "Iteration 239: w = [2.         2.99999999], b = 2.999999995121498, loss = 0.5429368401138552\n",
      "Iteration 240: w = [2.         2.99999999], b = 2.9999999951197207, loss = 0.7897310557817571\n",
      "Iteration 241: w = [2.         2.99999999], b = 2.9999999951249308, loss = 6.786534627263823\n",
      "Iteration 242: w = [2.         2.99999999], b = 2.999999995126994, loss = 1.0641832695477498\n",
      "Iteration 243: w = [2.         2.99999999], b = 2.999999995129936, loss = 2.1636593161425886\n",
      "Iteration 244: w = [2.         2.99999999], b = 2.999999995114197, loss = 61.927815868793765\n",
      "Iteration 245: w = [2.         2.99999999], b = 2.9999999951048157, loss = 22.002792719416693\n",
      "Iteration 246: w = [2.         2.99999999], b = 2.9999999950848553, loss = 99.60638667572182\n",
      "Iteration 247: w = [2.         2.99999999], b = 2.9999999950861564, loss = 0.423395673438118\n",
      "Iteration 248: w = [2.         2.99999999], b = 2.999999995081396, loss = 5.665985508871728\n",
      "Iteration 249: w = [2.         2.99999999], b = 2.9999999950736584, loss = 14.966608968990254\n",
      "Iteration 250: w = [2.         2.99999999], b = 2.999999995072916, loss = 0.1377842737007646\n",
      "Iteration 251: w = [2.         2.99999999], b = 2.9999999950681175, loss = 5.756012406358568\n",
      "Iteration 252: w = [2.         2.99999999], b = 2.9999999950709224, loss = 1.9669155995119554\n",
      "Iteration 253: w = [2.         2.99999999], b = 2.9999999950712475, loss = 0.026434396026225437\n",
      "Iteration 254: w = [2.         2.99999999], b = 2.9999999950634564, loss = 15.174510077147952\n",
      "Iteration 255: w = [2.         2.99999999], b = 2.9999999950648903, loss = 0.5141233512427518\n",
      "Iteration 256: w = [2.         2.99999999], b = 2.9999999950548255, loss = 25.32542616232727\n",
      "Iteration 257: w = [2.         2.99999999], b = 2.9999999950535132, loss = 0.4303946371864149\n",
      "Iteration 258: w = [2.         2.99999999], b = 2.9999999950561262, loss = 1.7070603515738096\n",
      "Iteration 259: w = [2.         2.99999999], b = 2.9999999950554175, loss = 0.12552251936586514\n",
      "Iteration 260: w = [2.         2.99999999], b = 2.999999995058405, loss = 2.2312805371786135\n",
      "Iteration 261: w = [2.         2.99999999], b = 2.999999995052193, loss = 9.64699784421346\n",
      "Iteration 262: w = [2.         2.99999999], b = 2.999999995040922, loss = 31.759425957361913\n",
      "Iteration 263: w = [2.         2.99999999], b = 2.9999999950226672, loss = 83.30697368039084\n",
      "Iteration 264: w = [2.         2.99999999], b = 2.9999999950154668, loss = 12.96226609485659\n",
      "Iteration 265: w = [2.         2.99999999], b = 2.9999999950136624, loss = 0.8137821727364313\n",
      "Iteration 266: w = [2.         2.99999999], b = 2.9999999950189102, loss = 6.884720837240124\n",
      "Iteration 267: w = [2.         2.99999999], b = 2.999999995021498, loss = 1.674055270442443\n",
      "Iteration 268: w = [2.         2.99999999], b = 2.999999995018025, loss = 3.0154018984016\n",
      "Iteration 269: w = [2.         2.99999999], b = 2.9999999950227196, loss = 5.509705852724917\n",
      "Iteration 270: w = [2.         2.99999999], b = 2.9999999950081606, loss = 52.99202866387938\n",
      "Iteration 271: w = [2.         2.99999999], b = 2.9999999950007368, loss = 13.778711283027723\n",
      "Iteration 272: w = [2.         2.99999999], b = 2.9999999949859406, loss = 54.73004504054954\n",
      "Iteration 273: w = [2.         2.99999999], b = 2.9999999949810237, loss = 6.043648485546627\n",
      "Iteration 274: w = [2.         2.99999999], b = 2.999999994974109, loss = 11.952183516673601\n",
      "Iteration 275: w = [2.         2.99999999], b = 2.999999994973229, loss = 0.193672772834359\n",
      "Iteration 276: w = [2.         2.99999999], b = 2.9999999949649636, loss = 17.07888505700597\n",
      "Iteration 277: w = [2.         2.99999999], b = 2.9999999949669394, loss = 0.9760232188666997\n",
      "Iteration 278: w = [2.         2.99999999], b = 2.999999994966444, loss = 0.06124301680029422\n",
      "Iteration 279: w = [2.         2.99999999], b = 2.9999999949598584, loss = 10.843823036626775\n",
      "Iteration 280: w = [2.         2.99999999], b = 2.9999999949362812, loss = 138.97121380127228\n",
      "Iteration 281: w = [2.         2.99999999], b = 2.999999994935424, loss = 0.18358560163823753\n",
      "Iteration 282: w = [2.         2.99999999], b = 2.9999999949189564, loss = 67.79527091641178\n",
      "Iteration 283: w = [2.         2.99999999], b = 2.999999994919506, loss = 0.07561922214019612\n",
      "Iteration 284: w = [2.         2.99999999], b = 2.999999994918606, loss = 0.20252858796801393\n",
      "Iteration 285: w = [2.         2.99999999], b = 2.9999999949105267, loss = 16.318034996513273\n",
      "Iteration 286: w = [2.         2.99999999], b = 2.999999994911982, loss = 0.529455422309616\n",
      "Iteration 287: w = [2.         2.99999999], b = 2.999999994902651, loss = 21.7682848825307\n",
      "Iteration 288: w = [2.         2.99999999], b = 2.9999999948853753, loss = 74.61204913954836\n",
      "Iteration 289: w = [2.         2.99999999], b = 2.999999994880427, loss = 6.121607085052002\n",
      "Iteration 290: w = [2.         2.99999999], b = 2.9999999948807514, loss = 0.026374700552548522\n",
      "Iteration 291: w = [2.         2.99999999], b = 2.9999999948789235, loss = 0.8352196982067751\n",
      "Iteration 292: w = [2.         2.99999999], b = 2.9999999948765614, loss = 1.3946838299118935\n",
      "Iteration 293: w = [2.         2.99999999], b = 2.999999994863836, loss = 40.4826724075116\n",
      "Iteration 294: w = [2.         2.99999999], b = 2.999999994859942, loss = 3.7909398338295524\n",
      "Iteration 295: w = [2.         2.99999999], b = 2.9999999948532334, loss = 11.250344621004789\n",
      "Iteration 296: w = [2.         2.99999999], b = 2.9999999948507394, loss = 1.554856985085545\n",
      "Iteration 297: w = [2.         2.99999999], b = 2.999999994852544, loss = 0.8143229787075639\n",
      "Iteration 298: w = [1.99999999 2.99999999], b = 2.9999999948236318, loss = 208.98181198543028\n",
      "Iteration 299: w = [1.99999999 2.99999999], b = 2.9999999948048064, loss = 88.59942409792569\n",
      "Iteration 300: w = [1.99999999 2.99999999], b = 2.9999999948032396, loss = 0.613798779680102\n",
      "Iteration 301: w = [1.99999999 2.99999999], b = 2.9999999947996683, loss = 3.1889214614943895\n",
      "Iteration 302: w = [1.99999999 2.99999999], b = 2.999999994803478, loss = 3.6288060744597233\n",
      "Iteration 303: w = [1.99999999 2.99999999], b = 2.9999999947964016, loss = 12.519417682447976\n",
      "Iteration 304: w = [1.99999999 2.99999999], b = 2.9999999947789617, loss = 76.03585356524387\n",
      "Iteration 305: w = [1.99999999 2.99999999], b = 2.999999994772279, loss = 11.164967029726254\n",
      "Iteration 306: w = [1.99999999 2.99999999], b = 2.9999999947637845, loss = 18.03996034476389\n",
      "Iteration 307: w = [1.99999999 2.99999999], b = 2.9999999947611187, loss = 1.7766135447246088\n",
      "Iteration 308: w = [1.99999999 2.99999999], b = 2.99999999475389, loss = 13.064794035806335\n",
      "Iteration 309: w = [1.99999999 2.99999999], b = 2.9999999947621867, loss = 17.209575183592957\n",
      "Iteration 310: w = [1.99999999 2.99999999], b = 2.9999999947603606, loss = 0.833498946478034\n",
      "Iteration 311: w = [1.99999999 2.99999999], b = 2.9999999947467804, loss = 46.105640985423115\n",
      "Iteration 312: w = [1.99999999 2.99999999], b = 2.9999999947395337, loss = 13.129050444943825\n",
      "Iteration 313: w = [1.99999999 2.99999999], b = 2.999999994730248, loss = 21.55760072696028\n",
      "Iteration 314: w = [1.99999999 2.99999999], b = 2.9999999947190945, loss = 31.100088496244343\n",
      "Iteration 315: w = [1.99999999 2.99999999], b = 2.999999994710303, loss = 19.32344367466888\n",
      "Iteration 316: w = [1.99999999 2.99999999], b = 2.999999994689771, loss = 105.38910235736269\n",
      "Iteration 317: w = [1.99999999 2.99999999], b = 2.999999994679432, loss = 26.723150285616413\n",
      "Iteration 318: w = [1.99999999 2.99999999], b = 2.9999999946783493, loss = 0.29309970654953865\n",
      "Iteration 319: w = [1.99999999 2.99999999], b = 2.9999999946752487, loss = 2.4033613297684577\n",
      "Iteration 320: w = [1.99999999 2.99999999], b = 2.9999999946463074, loss = 209.3990050102566\n",
      "Iteration 321: w = [1.99999999 2.99999999], b = 2.999999994647347, loss = 0.27010522486546834\n",
      "Iteration 322: w = [1.99999999 2.99999999], b = 2.999999994639954, loss = 13.664508941044826\n",
      "Iteration 323: w = [1.99999999 2.99999999], b = 2.999999994637574, loss = 1.415831181929915\n",
      "Iteration 324: w = [1.99999999 2.99999999], b = 2.999999994628923, loss = 18.709187602435\n",
      "Iteration 325: w = [1.99999999 2.99999999], b = 2.999999994620566, loss = 17.459514546453267\n",
      "Iteration 326: w = [1.99999999 2.99999999], b = 2.9999999946119678, loss = 18.483217716418554\n",
      "Iteration 327: w = [1.99999999 2.99999999], b = 2.9999999946076614, loss = 4.636375335486177\n",
      "Iteration 328: w = [1.99999999 2.99999999], b = 2.9999999945949467, loss = 40.41629046136336\n",
      "Iteration 329: w = [1.99999999 2.99999999], b = 2.999999994597429, loss = 1.5405622286090064\n",
      "Iteration 330: w = [1.99999999 2.99999999], b = 2.999999994594827, loss = 1.6933239008165035\n",
      "Iteration 331: w = [1.99999999 2.99999999], b = 2.9999999945978653, loss = 2.308136712876514\n",
      "Iteration 332: w = [1.99999999 2.99999999], b = 2.999999994584849, loss = 42.356696602027654\n",
      "Iteration 333: w = [1.99999999 2.99999999], b = 2.999999994583883, loss = 0.23329214186497885\n",
      "Iteration 334: w = [1.99999999 2.99999999], b = 2.9999999945811253, loss = 1.9011284990187767\n",
      "Iteration 335: w = [1.99999999 2.99999999], b = 2.999999994587202, loss = 9.231125848734367\n",
      "Iteration 336: w = [1.99999999 2.99999999], b = 2.9999999945891007, loss = 0.9013802894985797\n",
      "Iteration 337: w = [1.99999999 2.99999999], b = 2.9999999945806777, loss = 17.73763057752206\n",
      "Iteration 338: w = [1.99999999 2.99999999], b = 2.9999999945605675, loss = 101.10641544139072\n",
      "Iteration 339: w = [1.99999999 2.99999999], b = 2.9999999945478386, loss = 40.507039443387534\n",
      "Iteration 340: w = [1.99999999 2.99999999], b = 2.9999999945294387, loss = 84.63790942157776\n",
      "Iteration 341: w = [1.99999999 2.99999999], b = 2.9999999945294475, loss = 1.9042251709561053e-05\n",
      "Iteration 342: w = [1.99999999 2.99999999], b = 2.999999994509748, loss = 97.01758911921951\n",
      "Iteration 343: w = [1.99999999 2.99999999], b = 2.9999999944963216, loss = 45.06886354168145\n",
      "Iteration 344: w = [1.99999999 2.99999999], b = 2.9999999944817275, loss = 53.24846823034697\n",
      "Iteration 345: w = [1.99999999 2.99999999], b = 2.99999999447249, loss = 21.332861436466057\n",
      "Iteration 346: w = [1.99999999 2.99999999], b = 2.9999999944765037, loss = 4.0272953290309905\n",
      "Iteration 347: w = [1.99999999 2.99999999], b = 2.9999999944766365, loss = 0.004409874511574852\n",
      "Iteration 348: w = [1.99999999 2.99999999], b = 2.999999994482532, loss = 8.68982233052287\n",
      "Iteration 349: w = [1.99999999 2.99999999], b = 2.9999999944719145, loss = 28.185191346313115\n",
      "Iteration 350: w = [1.99999999 2.99999999], b = 2.9999999944615863, loss = 26.668402402395635\n",
      "Iteration 351: w = [1.99999999 2.99999999], b = 2.999999994447568, loss = 49.12631918196747\n",
      "Iteration 352: w = [1.99999999 2.99999999], b = 2.9999999944436966, loss = 3.7471346103236765\n",
      "Iteration 353: w = [1.99999999 2.99999999], b = 2.9999999944311253, loss = 39.50883176729234\n",
      "Iteration 354: w = [1.99999999 2.99999999], b = 2.99999999442274, loss = 17.57795313748027\n",
      "Iteration 355: w = [1.99999999 2.99999999], b = 2.9999999944087192, loss = 49.144191657586745\n",
      "Iteration 356: w = [1.99999999 2.99999999], b = 2.999999994409614, loss = 0.20011387899527272\n",
      "Iteration 357: w = [1.99999999 2.99999999], b = 2.9999999943947677, loss = 55.10387631223416\n",
      "Iteration 358: w = [1.99999999 2.99999999], b = 2.9999999943976516, loss = 2.079496844701468\n",
      "Iteration 359: w = [1.99999999 2.99999999], b = 2.9999999943892344, loss = 17.712748513225794\n",
      "Iteration 360: w = [1.99999999 2.99999999], b = 2.9999999943814393, loss = 15.191030623133168\n",
      "Iteration 361: w = [1.99999999 2.99999999], b = 2.999999994369379, loss = 36.362031700436916\n",
      "Iteration 362: w = [1.99999999 2.99999999], b = 2.999999994376105, loss = 11.308545099454836\n",
      "Iteration 363: w = [1.99999999 2.99999999], b = 2.999999994368881, loss = 13.04718486496755\n",
      "Iteration 364: w = [1.99999999 2.99999999], b = 2.9999999943597553, loss = 20.818129957979206\n",
      "Iteration 365: w = [1.99999999 2.99999999], b = 2.9999999943622484, loss = 1.5541192488293756\n",
      "Iteration 366: w = [1.99999999 2.99999999], b = 2.9999999943595967, loss = 1.7579704981436335\n",
      "Iteration 367: w = [1.99999999 2.99999999], b = 2.999999994356088, loss = 3.0774723351246283\n",
      "Iteration 368: w = [1.99999999 2.99999999], b = 2.9999999943559454, loss = 0.005087899189397292\n",
      "Iteration 369: w = [1.99999999 2.99999999], b = 2.9999999943582685, loss = 1.3489675338593048\n",
      "Iteration 370: w = [1.99999999 2.99999999], b = 2.999999994336877, loss = 114.39611582668299\n",
      "Iteration 371: w = [1.99999999 2.99999999], b = 2.9999999943362154, loss = 0.10946325775528078\n",
      "Iteration 372: w = [1.99999999 2.99999999], b = 2.999999994327316, loss = 19.80001892235692\n",
      "Iteration 373: w = [1.99999999 2.99999999], b = 2.9999999943132964, loss = 49.13684550550928\n",
      "Iteration 374: w = [1.99999999 2.99999999], b = 2.9999999943126214, loss = 0.11386681595595456\n",
      "Iteration 375: w = [1.99999999 2.99999999], b = 2.99999999429969, loss = 41.806057560038454\n",
      "Iteration 376: w = [1.99999999 2.99999999], b = 2.99999999429798, loss = 0.7311174240779368\n",
      "Iteration 377: w = [1.99999999 2.99999999], b = 2.999999994297304, loss = 0.11423941776023096\n",
      "Iteration 378: w = [1.99999999 2.99999999], b = 2.9999999942938627, loss = 2.96041518366872\n",
      "Iteration 379: w = [1.99999999 2.99999999], b = 2.9999999942840136, loss = 24.25146376047058\n",
      "Iteration 380: w = [1.99999999 2.99999999], b = 2.9999999942813638, loss = 1.7556722994972118\n",
      "Iteration 381: w = [1.99999999 2.99999999], b = 2.9999999942728564, loss = 18.09484762646429\n",
      "Iteration 382: w = [1.99999999 2.99999999], b = 2.9999999942741793, loss = 0.4375401088550897\n",
      "Iteration 383: w = [1.99999999 2.99999999], b = 2.9999999942600555, loss = 49.87162934515101\n",
      "Iteration 384: w = [1.99999999 2.99999999], b = 2.999999994258983, loss = 0.2876142505716474\n",
      "Iteration 385: w = [1.99999999 2.99999999], b = 2.999999994251635, loss = 13.49870656345304\n",
      "Iteration 386: w = [1.99999999 2.99999999], b = 2.999999994225626, loss = 169.11576536440597\n",
      "Iteration 387: w = [1.99999999 2.99999999], b = 2.999999994216469, loss = 20.963816245006413\n",
      "Iteration 388: w = [1.99999999 2.99999999], b = 2.9999999941998308, loss = 69.2070688773778\n",
      "Iteration 389: w = [1.99999999 2.99999999], b = 2.9999999942034745, loss = 3.319122275081121\n",
      "Iteration 390: w = [1.99999999 2.99999999], b = 2.9999999941996762, loss = 3.6066919011032526\n",
      "Iteration 391: w = [1.99999999 2.99999999], b = 2.999999994201621, loss = 0.9454238806454544\n",
      "Iteration 392: w = [1.99999999 2.99999999], b = 2.9999999942016227, loss = 7.808393066013933e-07\n",
      "Iteration 393: w = [1.99999999 2.99999999], b = 2.9999999941926263, loss = 20.233041298326786\n",
      "Iteration 394: w = [1.99999999 2.99999999], b = 2.999999994176621, loss = 64.04312049371369\n",
      "Iteration 395: w = [1.99999999 2.99999999], b = 2.999999994162849, loss = 47.418489168347\n",
      "Iteration 396: w = [1.99999999 2.99999999], b = 2.999999994148652, loss = 50.38544560165845\n",
      "Iteration 397: w = [1.99999999 2.99999999], b = 2.9999999941403654, loss = 17.166684614751695\n",
      "Iteration 398: w = [1.99999999 2.99999999], b = 2.9999999941241136, loss = 66.03209209315689\n",
      "Iteration 399: w = [1.99999999 2.99999999], b = 2.9999999941175526, loss = 10.761718984260266\n",
      "Iteration 400: w = [1.99999999 2.99999999], b = 2.999999994117359, loss = 0.009367262440531498\n",
      "Iteration 401: w = [1.99999999 2.99999999], b = 2.9999999941057713, loss = 33.567986603753944\n",
      "Iteration 402: w = [1.99999999 2.99999999], b = 2.9999999941000692, loss = 8.128506459082365\n",
      "Iteration 403: w = [1.99999999 2.99999999], b = 2.9999999940931055, loss = 12.123064185641079\n",
      "Iteration 404: w = [1.99999999 2.99999999], b = 2.9999999940875344, loss = 7.759141976258664\n",
      "Iteration 405: w = [1.99999999 2.99999999], b = 2.99999999406937, loss = 82.48843740820442\n",
      "Iteration 406: w = [1.99999999 2.99999999], b = 2.999999994070543, loss = 0.34426728226385106\n",
      "Iteration 407: w = [1.99999999 2.99999999], b = 2.999999994073545, loss = 2.2533141224993387\n",
      "Iteration 408: w = [1.99999999 2.99999999], b = 2.9999999940569633, loss = 68.74062882625955\n",
      "Iteration 409: w = [1.99999999 2.99999999], b = 2.9999999940429642, loss = 48.99177032669032\n",
      "Iteration 410: w = [1.99999999 2.99999999], b = 2.9999999940377142, loss = 6.890996922374237\n",
      "Iteration 411: w = [1.99999999 2.99999999], b = 2.999999994032285, loss = 7.3698641341649225\n",
      "Iteration 412: w = [1.99999999 2.99999999], b = 2.9999999940148423, loss = 76.05839232012113\n",
      "Iteration 413: w = [1.99999999 2.99999999], b = 2.999999994020904, loss = 9.186766657535236\n",
      "Iteration 414: w = [1.99999999 2.99999999], b = 2.9999999940063193, loss = 53.180492853982194\n",
      "Iteration 415: w = [1.99999999 2.99999999], b = 2.9999999939999777, loss = 10.053754425834741\n",
      "Iteration 416: w = [1.99999999 2.99999999], b = 2.99999999400237, loss = 1.4306915555222224\n",
      "Iteration 417: w = [1.99999999 2.99999999], b = 2.9999999940058424, loss = 3.014268277991502\n",
      "Iteration 418: w = [1.99999999 2.99999999], b = 2.9999999939896704, loss = 65.38149090448273\n",
      "Iteration 419: w = [1.99999999 2.99999999], b = 2.999999993975038, loss = 53.525893697486225\n",
      "Iteration 420: w = [1.99999999 2.99999999], b = 2.999999993980942, loss = 8.713336443763513\n",
      "Iteration 421: w = [1.99999999 2.99999999], b = 2.9999999939850017, loss = 4.12080409912249\n",
      "Iteration 422: w = [1.99999999 2.99999999], b = 2.9999999939826933, loss = 1.3320513699982564\n",
      "Iteration 423: w = [1.99999999 2.99999999], b = 2.9999999939833906, loss = 0.1214799918086735\n",
      "Iteration 424: w = [1.99999999 2.99999999], b = 2.999999993976492, loss = 11.89786979073729\n",
      "Iteration 425: w = [1.99999999 2.99999999], b = 2.9999999939728887, loss = 3.2456995649696436\n",
      "Iteration 426: w = [1.99999999 2.99999999], b = 2.9999999939571933, loss = 61.58766661580447\n",
      "Iteration 427: w = [1.99999999 2.99999999], b = 2.999999993943525, loss = 46.70604346084257\n",
      "Iteration 428: w = [1.99999999 2.99999999], b = 2.999999993937375, loss = 9.45627575757927\n",
      "Iteration 429: w = [1.99999999 2.99999999], b = 2.999999993939027, loss = 0.6824331921704959\n",
      "Iteration 430: w = [1.99999999 2.99999999], b = 2.9999999939352024, loss = 3.656377123068454\n",
      "Iteration 431: w = [1.99999999 2.99999999], b = 2.9999999939289617, loss = 9.736914915092854\n",
      "Iteration 432: w = [1.99999999 2.99999999], b = 2.999999993924195, loss = 5.680441048198157\n",
      "Iteration 433: w = [1.99999999 2.99999999], b = 2.999999993904836, loss = 93.68872388576357\n",
      "Iteration 434: w = [1.99999999 2.99999999], b = 2.999999993908426, loss = 3.222084942550863\n",
      "Iteration 435: w = [1.99999999 2.99999999], b = 2.9999999939133617, loss = 6.0903116066042795\n",
      "Iteration 436: w = [1.99999999 2.99999999], b = 2.9999999939100794, loss = 2.693286935470136\n",
      "Iteration 437: w = [1.99999999 2.99999999], b = 2.9999999939030135, loss = 12.4823699215587\n",
      "Iteration 438: w = [1.99999999 2.99999999], b = 2.9999999939083533, loss = 7.128108106187351\n",
      "Iteration 439: w = [1.99999999 2.99999999], b = 2.9999999938984856, loss = 24.342507353550413\n",
      "Iteration 440: w = [1.99999999 2.99999999], b = 2.9999999939038093, loss = 7.085148535188278\n",
      "Iteration 441: w = [1.99999999 2.99999999], b = 2.999999993905277, loss = 0.5384573263252789\n",
      "Iteration 442: w = [1.99999999 2.99999999], b = 2.9999999938966044, loss = 18.80328628866145\n",
      "Iteration 443: w = [1.99999999 2.99999999], b = 2.999999993892609, loss = 3.991335874278288\n",
      "Iteration 444: w = [1.99999999 2.99999999], b = 2.999999993889173, loss = 2.9511171266590157\n",
      "Iteration 445: w = [1.99999999 2.99999999], b = 2.9999999938852215, loss = 3.903268054550928\n",
      "Iteration 446: w = [1.99999999 2.99999999], b = 2.999999993882451, loss = 1.9193499487549064\n",
      "Iteration 447: w = [1.99999999 2.99999999], b = 2.999999993877539, loss = 6.031853884625064\n",
      "Iteration 448: w = [1.99999999 2.99999999], b = 2.9999999938791513, loss = 0.6501268869874844\n",
      "Iteration 449: w = [1.99999999 2.99999999], b = 2.999999993879012, loss = 0.0048534759857301645\n",
      "Iteration 450: w = [1.99999999 2.99999999], b = 2.999999993871129, loss = 15.53550980517514\n",
      "Iteration 451: w = [1.99999999 2.99999999], b = 2.9999999938616013, loss = 22.69347897690691\n",
      "Iteration 452: w = [1.99999999 2.99999999], b = 2.999999993858641, loss = 2.1910778489564104\n",
      "Iteration 453: w = [1.99999999 2.99999999], b = 2.999999993844743, loss = 48.29115093541947\n",
      "Iteration 454: w = [1.99999999 2.99999999], b = 2.999999993848734, loss = 3.981671125740189\n",
      "Iteration 455: w = [1.99999999 2.99999999], b = 2.999999993854677, loss = 8.830521367331748\n",
      "Iteration 456: w = [1.99999999 2.99999999], b = 2.9999999938529736, loss = 0.7253502634242379\n",
      "Iteration 457: w = [1.99999999 2.99999999], b = 2.9999999938362425, loss = 69.98239717800031\n",
      "Iteration 458: w = [1.99999999 2.99999999], b = 2.999999993832955, loss = 2.702321889288894\n",
      "Iteration 459: w = [1.99999999 2.99999999], b = 2.9999999938132476, loss = 97.0966315262466\n",
      "Iteration 460: w = [1.99999999 2.99999999], b = 2.9999999938024984, loss = 28.88667685064085\n",
      "Iteration 461: w = [1.99999999 2.99999999], b = 2.999999993788641, loss = 48.008115625626225\n",
      "Iteration 462: w = [1.99999999 2.99999999], b = 2.9999999937692556, loss = 93.9472577895036\n",
      "Iteration 463: w = [1.99999999 2.99999999], b = 2.999999993761475, loss = 15.13464669596882\n",
      "Iteration 464: w = [1.99999999 2.99999999], b = 2.9999999937479918, loss = 45.450268303131075\n",
      "Iteration 465: w = [1.99999999 2.99999999], b = 2.9999999937339292, loss = 49.4397749368836\n",
      "Iteration 466: w = [1.99999999 2.99999999], b = 2.9999999937352895, loss = 0.4624342643827883\n",
      "Iteration 467: w = [1.99999999 2.99999999], b = 2.9999999937339243, loss = 0.46581271173087074\n",
      "Iteration 468: w = [1.99999999 2.99999999], b = 2.9999999937291912, loss = 5.600117692278379\n",
      "Iteration 469: w = [1.99999999 2.99999999], b = 2.9999999937345807, loss = 7.261777935758558\n",
      "Iteration 470: w = [1.99999999 2.99999999], b = 2.9999999937363753, loss = 0.8052977530686868\n",
      "Iteration 471: w = [1.99999999 2.99999999], b = 2.9999999937275446, loss = 19.494714028893956\n",
      "Iteration 472: w = [1.99999999 2.99999999], b = 2.9999999937040904, loss = 137.5255836235156\n",
      "Iteration 473: w = [1.99999999 2.99999999], b = 2.9999999937005004, loss = 3.2216588023628066\n",
      "Iteration 474: w = [1.99999999 2.99999999], b = 2.9999999936877524, loss = 40.62784521127592\n",
      "Iteration 475: w = [1.99999999 2.99999999], b = 2.999999993669317, loss = 84.96699600733852\n",
      "Iteration 476: w = [1.99999999 2.99999999], b = 2.999999993669646, loss = 0.027063854431842106\n",
      "Iteration 477: w = [1.99999999 2.99999999], b = 2.9999999936615467, loss = 16.400476438622498\n",
      "Iteration 478: w = [1.99999999 2.99999999], b = 2.9999999936504316, loss = 30.885182235947067\n",
      "Iteration 479: w = [1.99999999 2.99999999], b = 2.999999993650758, loss = 0.02665060120321832\n",
      "Iteration 480: w = [1.99999999 2.99999999], b = 2.9999999936571435, loss = 10.19432219206073\n",
      "Iteration 481: w = [1.99999999 2.99999999], b = 2.9999999936554227, loss = 0.7403022052572013\n",
      "Iteration 482: w = [1.99999999 2.99999999], b = 2.9999999936470036, loss = 17.720710062638133\n",
      "Iteration 483: w = [1.99999999 2.99999999], b = 2.9999999936418216, loss = 6.713222549710524\n",
      "Iteration 484: w = [1.99999999 2.99999999], b = 2.9999999936442405, loss = 1.4629582498589186\n",
      "Iteration 485: w = [1.99999999 2.99999999], b = 2.999999993616831, loss = 187.82294781300092\n",
      "Iteration 486: w = [1.99999999 2.99999999], b = 2.9999999935982222, loss = 86.57177415137868\n",
      "Iteration 487: w = [1.99999999 2.99999999], b = 2.9999999936036446, loss = 7.350316816576559\n",
      "Iteration 488: w = [1.99999999 2.99999999], b = 2.9999999935877217, loss = 63.38263534758267\n",
      "Iteration 489: w = [1.99999999 2.99999999], b = 2.9999999935703703, loss = 75.26920229115665\n",
      "Iteration 490: w = [1.99999999 2.99999999], b = 2.9999999935538915, loss = 67.88897931439033\n",
      "Iteration 491: w = [1.99999999 2.99999999], b = 2.999999993548687, loss = 6.771026765259226\n",
      "Iteration 492: w = [1.99999999 2.99999999], b = 2.999999993550947, loss = 1.2770454487984344\n",
      "Iteration 493: w = [1.99999999 2.99999999], b = 2.999999993549848, loss = 0.3019067567472341\n",
      "Iteration 494: w = [1.99999999 2.99999999], b = 2.9999999935487223, loss = 0.316821266144646\n",
      "Iteration 495: w = [1.99999999 2.99999999], b = 2.9999999935365653, loss = 36.94647169663546\n",
      "Iteration 496: w = [1.99999999 2.99999999], b = 2.999999993538542, loss = 0.9765614156140285\n",
      "Iteration 497: w = [1.99999999 2.99999999], b = 2.999999993543068, loss = 5.121787659900379\n",
      "Iteration 498: w = [1.99999999 2.99999999], b = 2.999999993540177, loss = 2.0896346974012903\n",
      "Iteration 499: w = [1.99999999 2.99999999], b = 2.9999999935418753, loss = 0.7208492078084724\n",
      "Iteration 500: w = [1.99999999 2.99999999], b = 2.9999999935284536, loss = 45.03620860406253\n",
      "Iteration 501: w = [1.99999999 2.99999999], b = 2.9999999935097708, loss = 87.26027264037819\n",
      "Iteration 502: w = [1.99999999 2.99999999], b = 2.999999993502569, loss = 12.967098336514965\n",
      "Iteration 503: w = [1.99999999 2.99999999], b = 2.9999999934993746, loss = 2.550899620608264\n",
      "Iteration 504: w = [1.99999999 2.99999999], b = 2.9999999934865462, loss = 41.14170764710848\n",
      "Iteration 505: w = [1.99999999 2.99999999], b = 2.99999999347747, loss = 20.593958430308696\n",
      "Iteration 506: w = [1.99999999 2.99999999], b = 2.9999999934520902, loss = 161.0341980919401\n",
      "Iteration 507: w = [1.99999999 2.99999999], b = 2.999999993451012, loss = 0.2905890095981073\n",
      "Iteration 508: w = [1.99999999 2.99999999], b = 2.9999999934332613, loss = 78.77157292968046\n",
      "Iteration 509: w = [1.99999999 2.99999999], b = 2.9999999934296206, loss = 3.3135866875971525\n",
      "Iteration 510: w = [1.99999999 2.99999999], b = 2.9999999934186414, loss = 30.135523111965863\n",
      "Iteration 511: w = [1.99999999 2.99999999], b = 2.9999999934214903, loss = 2.0287936955788\n",
      "Iteration 512: w = [1.99999999 2.99999999], b = 2.999999993418801, loss = 1.8080723203680387\n",
      "Iteration 513: w = [1.99999999 2.99999999], b = 2.9999999934140487, loss = 5.645756321101813\n",
      "Iteration 514: w = [1.99999999 2.99999999], b = 2.9999999934116004, loss = 1.4984347596605545\n",
      "Iteration 515: w = [1.99999999 2.99999999], b = 2.999999993408878, loss = 1.8524105828307906\n",
      "Iteration 516: w = [1.99999999 2.99999999], b = 2.9999999934074926, loss = 0.4799922962762132\n",
      "Iteration 517: w = [1.99999999 2.99999999], b = 2.999999993395088, loss = 38.468151449521315\n",
      "Iteration 518: w = [1.99999999 2.99999999], b = 2.999999993396913, loss = 0.8326715498428721\n",
      "Iteration 519: w = [1.99999999 2.99999999], b = 2.9999999933916257, loss = 6.988889691545435\n",
      "Iteration 520: w = [1.99999999 2.99999999], b = 2.999999993382177, loss = 22.319313985463754\n",
      "Iteration 521: w = [1.99999999 2.99999999], b = 2.9999999933846557, loss = 1.5360943815766814\n",
      "Iteration 522: w = [1.99999999 2.99999999], b = 2.99999999337181, loss = 41.25203034542635\n",
      "Iteration 523: w = [1.99999999 2.99999999], b = 2.9999999933549817, loss = 70.79647800850842\n",
      "Iteration 524: w = [1.99999999 2.99999999], b = 2.999999993350866, loss = 4.23528675487186\n",
      "Iteration 525: w = [1.99999999 2.99999999], b = 2.9999999933507144, loss = 0.005739537713859481\n",
      "Iteration 526: w = [1.99999999 2.99999999], b = 2.9999999933538857, loss = 2.514281123273928\n",
      "Iteration 527: w = [1.99999999 2.99999999], b = 2.9999999933348085, loss = 90.98612003701447\n",
      "Iteration 528: w = [1.99999999 2.99999999], b = 2.999999993336184, loss = 0.47296045874134646\n",
      "Iteration 529: w = [1.99999999 2.99999999], b = 2.9999999933413135, loss = 6.578084858546589\n",
      "Iteration 530: w = [1.99999999 2.99999999], b = 2.9999999933333354, loss = 15.912377059426138\n",
      "Iteration 531: w = [1.99999999 2.99999999], b = 2.999999993332457, loss = 0.1929063337758387\n",
      "Iteration 532: w = [1.99999999 2.99999999], b = 2.9999999933130246, loss = 94.40447785139895\n",
      "Iteration 533: w = [1.99999999 2.99999999], b = 2.9999999933137764, loss = 0.14135331189294573\n",
      "Iteration 534: w = [1.99999999 2.99999999], b = 2.999999993311944, loss = 0.8392254361507883\n",
      "Iteration 535: w = [1.99999999 2.99999999], b = 2.9999999932971306, loss = 54.859116042968886\n",
      "Iteration 536: w = [1.99999999 2.99999999], b = 2.9999999932902175, loss = 11.948471006196222\n",
      "Iteration 537: w = [1.99999999 2.99999999], b = 2.999999993292113, loss = 0.8979413666563332\n",
      "Iteration 538: w = [1.99999999 2.99999999], b = 2.9999999932939025, loss = 0.8006554232031445\n",
      "Iteration 539: w = [1.99999999 2.99999999], b = 2.9999999932989803, loss = 6.445887623355579\n",
      "Iteration 540: w = [1.99999999 2.99999999], b = 2.9999999933033767, loss = 4.8319695487110055\n",
      "Iteration 541: w = [1.99999999 2.99999999], b = 2.999999993305322, loss = 0.9456516876944209\n",
      "Iteration 542: w = [1.99999999 2.99999999], b = 2.9999999933085615, loss = 2.623745731233083\n",
      "Iteration 543: w = [1.99999999 2.99999999], b = 2.9999999933109396, loss = 1.4140418494399887\n",
      "Iteration 544: w = [1.99999999 2.99999999], b = 2.9999999933112074, loss = 0.017906445848627238\n",
      "Iteration 545: w = [1.99999999 2.99999999], b = 2.9999999933063974, loss = 5.784116015428652\n",
      "Iteration 546: w = [1.99999999 2.99999999], b = 2.999999993305129, loss = 0.40217743758208174\n",
      "Iteration 547: w = [1.99999999 2.99999999], b = 2.9999999933098023, loss = 5.4597185684147425\n",
      "Iteration 548: w = [1.99999999 2.99999999], b = 2.9999999932998147, loss = 24.938471547260004\n",
      "Iteration 549: w = [1.99999999 2.99999999], b = 2.999999993285897, loss = 48.426315993783774\n",
      "Iteration 550: w = [1.99999999 2.99999999], b = 2.9999999932670955, loss = 88.37311222250888\n",
      "Iteration 551: w = [1.99999999 2.99999999], b = 2.9999999932631223, loss = 3.9470873845004673\n",
      "Iteration 552: w = [1.99999999 2.99999999], b = 2.9999999932547103, loss = 17.690587155110794\n",
      "Iteration 553: w = [1.99999999 2.99999999], b = 2.9999999932584633, loss = 3.5211505327913324\n",
      "Iteration 554: w = [1.99999999 2.99999999], b = 2.9999999932525494, loss = 8.743928889529553\n",
      "Iteration 555: w = [1.99999999 2.99999999], b = 2.9999999932293715, loss = 134.30390841178453\n",
      "Iteration 556: w = [1.99999999 2.99999999], b = 2.999999993218535, loss = 29.358858760814446\n",
      "Iteration 557: w = [1.99999999 2.99999999], b = 2.999999993202424, loss = 64.88970124159384\n",
      "Iteration 558: w = [1.99999999 2.99999999], b = 2.9999999932044523, loss = 1.0285173994217274\n",
      "Iteration 559: w = [1.99999999 2.99999999], b = 2.9999999931894914, loss = 55.956462618470745\n",
      "Iteration 560: w = [1.99999999 2.99999999], b = 2.999999993185074, loss = 4.87852819284577\n",
      "Iteration 561: w = [1.99999999 2.99999999], b = 2.999999993175113, loss = 24.804857423094944\n",
      "Iteration 562: w = [1.99999999 2.99999999], b = 2.999999993176571, loss = 0.5314622104056358\n",
      "Iteration 563: w = [1.99999999 2.99999999], b = 2.9999999931755132, loss = 0.27984962028391036\n",
      "Iteration 564: w = [1.99999999 2.99999999], b = 2.999999993177297, loss = 0.795478231295252\n",
      "Iteration 565: w = [1.99999999 2.99999999], b = 2.9999999931802432, loss = 2.1698570278646474\n",
      "Iteration 566: w = [1.99999999 2.99999999], b = 2.999999993169579, loss = 28.431874502362984\n",
      "Iteration 567: w = [1.99999999 2.99999999], b = 2.99999999316343, loss = 9.451894882699666\n",
      "Iteration 568: w = [1.99999999 2.99999999], b = 2.999999993167385, loss = 3.9107184275934737\n",
      "Iteration 569: w = [1.99999999 2.99999999], b = 2.999999993166869, loss = 0.0665310797433367\n",
      "Iteration 570: w = [1.99999999 2.99999999], b = 2.9999999931599968, loss = 11.806317114555467\n",
      "Epoch: 3\n",
      "Iteration 1: w = [1.99999999 2.99999999], b = 2.999999993164326, loss = 4.68639564480681\n",
      "Iteration 2: w = [1.99999999 2.99999999], b = 2.999999993168606, loss = 4.578824169221544\n",
      "Iteration 3: w = [1.99999999 2.99999999], b = 2.999999993172713, loss = 4.216424290266474\n",
      "Iteration 4: w = [1.99999999 2.99999999], b = 2.99999999315596, loss = 70.16426702442665\n",
      "Iteration 5: w = [1.99999999 2.99999999], b = 2.999999993133905, loss = 121.60332222303246\n",
      "Iteration 6: w = [1.99999999 2.99999999], b = 2.9999999931204675, loss = 45.14374850678156\n",
      "Iteration 7: w = [1.99999999 2.99999999], b = 2.9999999931002095, loss = 102.59514593657678\n",
      "Iteration 8: w = [1.99999999 2.99999999], b = 2.999999993096589, loss = 3.2775217850483878\n",
      "Iteration 9: w = [1.99999999 2.99999999], b = 2.99999999309011, loss = 10.49421840838946\n",
      "Iteration 10: w = [1.99999999 2.99999999], b = 2.99999999309572, loss = 7.86864206386789\n",
      "Iteration 11: w = [1.99999999 2.99999999], b = 2.9999999930924472, loss = 2.6782796698693128\n",
      "Iteration 12: w = [1.99999999 2.99999999], b = 2.999999993091528, loss = 0.21130429758397862\n",
      "Iteration 13: w = [1.99999999 2.99999999], b = 2.999999993071889, loss = 96.42184381694447\n",
      "Iteration 14: w = [1.99999999 2.99999999], b = 2.999999993057426, loss = 52.29615724416375\n",
      "Iteration 15: w = [1.99999999 2.99999999], b = 2.9999999930542827, loss = 2.4697647609958606\n",
      "Iteration 16: w = [1.99999999 2.99999999], b = 2.9999999930219463, loss = 261.41094233599205\n",
      "Iteration 17: w = [1.99999999 2.99999999], b = 2.9999999930173114, loss = 5.370240410013428\n",
      "Iteration 18: w = [1.99999999 2.99999999], b = 2.9999999930216426, loss = 4.689360205245279\n",
      "Iteration 19: w = [1.99999999 2.99999999], b = 2.9999999930072447, loss = 51.82342546257681\n",
      "Iteration 20: w = [1.99999999 2.99999999], b = 2.999999992994199, loss = 42.54696699665002\n",
      "Iteration 21: w = [1.99999999 2.99999999], b = 2.9999999929921546, loss = 1.0450798801815897\n",
      "Iteration 22: w = [1.99999999 2.99999999], b = 2.9999999929870937, loss = 6.403348813484753\n",
      "Iteration 23: w = [1.99999999 2.99999999], b = 2.999999992986036, loss = 0.2797817733341728\n",
      "Iteration 24: w = [1.99999999 2.99999999], b = 2.9999999929746943, loss = 32.15682148049309\n",
      "Iteration 25: w = [1.99999999 2.99999999], b = 2.9999999929759484, loss = 0.3932808560288079\n",
      "Iteration 26: w = [1.99999999 2.99999999], b = 2.999999992962135, loss = 47.70296465326756\n",
      "Iteration 27: w = [1.99999999 2.99999999], b = 2.999999992949864, loss = 37.6448817256766\n",
      "Iteration 28: w = [1.99999999 2.99999999], b = 2.9999999929410075, loss = 19.60988967229854\n",
      "Iteration 29: w = [1.99999999 2.99999999], b = 2.9999999929342325, loss = 11.475741641360296\n",
      "Iteration 30: w = [1.99999999 2.99999999], b = 2.9999999929269485, loss = 13.263269468805314\n",
      "Iteration 31: w = [1.99999999 2.99999999], b = 2.999999992921152, loss = 8.400171326548342\n",
      "Iteration 32: w = [1.99999999 2.99999999], b = 2.9999999929117696, loss = 22.005793720522302\n",
      "Iteration 33: w = [1.99999999 2.99999999], b = 2.999999992913191, loss = 0.5050763799397041\n",
      "Iteration 34: w = [1.99999999 2.99999999], b = 2.999999992905518, loss = 14.719079819698338\n",
      "Iteration 35: w = [1.99999999 2.99999999], b = 2.999999992895573, loss = 24.72636524438902\n",
      "Iteration 36: w = [1.99999999 2.99999999], b = 2.9999999928914667, loss = 4.21607747159413\n",
      "Iteration 37: w = [1.99999999 2.99999999], b = 2.9999999928964884, loss = 6.304221291922187\n",
      "Iteration 38: w = [1.99999999 2.99999999], b = 2.9999999928919823, loss = 5.075939091702721\n",
      "Iteration 39: w = [1.99999999 2.99999999], b = 2.9999999928942236, loss = 1.255677183740889\n",
      "Iteration 40: w = [1.99999999 2.99999999], b = 2.999999992888552, loss = 8.041466524946182\n",
      "Iteration 41: w = [1.99999999 2.99999999], b = 2.999999992878338, loss = 26.081056649238057\n",
      "Iteration 42: w = [1.99999999 2.99999999], b = 2.999999992874532, loss = 3.6216189979731563\n",
      "Iteration 43: w = [1.99999999 2.99999999], b = 2.99999999286645, loss = 16.328926821919453\n",
      "Iteration 44: w = [1.99999999 2.99999999], b = 2.999999992869553, loss = 2.4077951370833586\n",
      "Iteration 45: w = [1.99999999 2.99999999], b = 2.999999992857442, loss = 36.669143658882085\n",
      "Iteration 46: w = [1.99999999 2.99999999], b = 2.9999999928444403, loss = 42.261789763280554\n",
      "Iteration 47: w = [1.99999999 2.99999999], b = 2.99999999283075, loss = 46.85587807726311\n",
      "Iteration 48: w = [1.99999999 2.99999999], b = 2.999999992833413, loss = 1.7732225030314663\n",
      "Iteration 49: w = [1.99999999 2.99999999], b = 2.9999999928159706, loss = 76.05822127788817\n",
      "Iteration 50: w = [1.99999999 2.99999999], b = 2.999999992800568, loss = 59.311803631219824\n",
      "Iteration 51: w = [1.99999999 2.99999999], b = 2.999999992791174, loss = 22.060649424889178\n",
      "Iteration 52: w = [1.99999999 2.99999999], b = 2.999999992776644, loss = 52.77982559160875\n",
      "Iteration 53: w = [1.99999999 2.99999999], b = 2.9999999927664565, loss = 25.94693193461937\n",
      "Iteration 54: w = [1.99999999 2.99999999], b = 2.999999992766093, loss = 0.03296407905732513\n",
      "Iteration 55: w = [1.99999999 2.99999999], b = 2.9999999927615337, loss = 5.1974142330815445\n",
      "Iteration 56: w = [1.99999999 2.99999999], b = 2.999999992762994, loss = 0.5328616139159569\n",
      "Iteration 57: w = [1.99999999 2.99999999], b = 2.999999992764014, loss = 0.2600331750946429\n",
      "Iteration 58: w = [1.99999999 2.99999999], b = 2.9999999927507788, loss = 43.79130915602827\n",
      "Iteration 59: w = [1.99999999 2.99999999], b = 2.999999992745455, loss = 7.085238359450834\n",
      "Iteration 60: w = [1.99999999 2.99999999], b = 2.999999992734639, loss = 29.247440430607572\n",
      "Iteration 61: w = [1.99999999 2.99999999], b = 2.9999999927214076, loss = 43.76584313340025\n",
      "Iteration 62: w = [1.99999999 2.99999999], b = 2.999999992720896, loss = 0.06544188467910178\n",
      "Iteration 63: w = [1.99999999 2.99999999], b = 2.999999992720388, loss = 0.06447846142337393\n",
      "Iteration 64: w = [1.99999999 2.99999999], b = 2.999999992705289, loss = 56.99579013202815\n",
      "Iteration 65: w = [1.99999999 2.99999999], b = 2.99999999269436, loss = 29.862122782283045\n",
      "Iteration 66: w = [1.99999999 2.99999999], b = 2.9999999926980188, loss = 3.346407381518139\n",
      "Iteration 67: w = [1.99999999 2.99999999], b = 2.999999992695574, loss = 1.4939343938474043\n",
      "Iteration 68: w = [1.99999999 2.99999999], b = 2.9999999926980565, loss = 1.540393927730355\n",
      "Iteration 69: w = [1.99999999 2.99999999], b = 2.999999992701334, loss = 2.6849505718045443\n",
      "Iteration 70: w = [1.99999999 2.99999999], b = 2.9999999926939656, loss = 13.573706307868097\n",
      "Iteration 71: w = [1.99999999 2.99999999], b = 2.9999999926914698, loss = 1.5570005199174983\n",
      "Iteration 72: w = [1.99999999 2.99999999], b = 2.9999999926950545, loss = 3.212821827138239\n",
      "Iteration 73: w = [1.99999999 2.99999999], b = 2.9999999926872123, loss = 15.37502389813984\n",
      "Iteration 74: w = [1.99999999 2.99999999], b = 2.9999999926793213, loss = 15.566792101927854\n",
      "Iteration 75: w = [1.99999999 2.99999999], b = 2.9999999926807943, loss = 0.5425861067899883\n",
      "Iteration 76: w = [1.99999999 2.99999999], b = 2.9999999926727496, loss = 16.180036106730917\n",
      "Iteration 77: w = [1.99999999 2.99999999], b = 2.999999992664575, loss = 16.705937268476582\n",
      "Iteration 78: w = [1.99999999 2.99999999], b = 2.999999992655129, loss = 22.30551734499014\n",
      "Iteration 79: w = [1.99999999 2.99999999], b = 2.999999992643467, loss = 34.00077331928127\n",
      "Iteration 80: w = [1.99999999 2.99999999], b = 2.9999999926486534, loss = 6.724941805110159\n",
      "Iteration 81: w = [1.99999999 2.99999999], b = 2.9999999926510585, loss = 1.4464040540056189\n",
      "Iteration 82: w = [1.99999999 2.99999999], b = 2.9999999926558676, loss = 5.781928084624358\n",
      "Iteration 83: w = [1.99999999 2.99999999], b = 2.9999999926538994, loss = 0.9682716807003852\n",
      "Iteration 84: w = [1.99999999 2.99999999], b = 2.999999992641224, loss = 40.16532274179774\n",
      "Iteration 85: w = [1.99999999 2.99999999], b = 2.9999999926456766, loss = 4.9565006796376965\n",
      "Iteration 86: w = [1.99999999 2.99999999], b = 2.999999992636702, loss = 20.135063345330117\n",
      "Iteration 87: w = [1.99999999 2.99999999], b = 2.9999999926324263, loss = 4.5706846112682955\n",
      "Iteration 88: w = [1.99999999 2.99999999], b = 2.999999992631011, loss = 0.5007456096721321\n",
      "Iteration 89: w = [1.99999999 2.99999999], b = 2.999999992631116, loss = 0.0027522831318819393\n",
      "Iteration 90: w = [1.99999999 2.99999999], b = 2.9999999926361416, loss = 6.314283782146529\n",
      "Iteration 91: w = [1.99999999 2.99999999], b = 2.999999992615121, loss = 110.46574623645303\n",
      "Iteration 92: w = [1.99999999 2.99999999], b = 2.9999999926037937, loss = 32.07777848724565\n",
      "Iteration 93: w = [1.99999999 2.99999999], b = 2.9999999925943808, loss = 22.150214598641476\n",
      "Iteration 94: w = [1.99999999 2.99999999], b = 2.9999999925865475, loss = 15.339389113860282\n",
      "Iteration 95: w = [1.99999999 2.99999999], b = 2.9999999925834233, loss = 2.440239790719127\n",
      "Iteration 96: w = [1.99999999 2.99999999], b = 2.9999999925803156, loss = 2.4143515117932814\n",
      "Iteration 97: w = [1.99999999 2.99999999], b = 2.999999992566961, loss = 44.58701853897115\n",
      "Iteration 98: w = [1.99999999 2.99999999], b = 2.9999999925667282, loss = 0.013550207817332275\n",
      "Iteration 99: w = [1.99999999 2.99999999], b = 2.9999999925616105, loss = 6.547856427422247\n",
      "Iteration 100: w = [1.99999999 2.99999999], b = 2.999999992563031, loss = 0.5044356828307818\n",
      "Iteration 101: w = [1.99999999 2.99999999], b = 2.999999992562307, loss = 0.13111192050770798\n",
      "Iteration 102: w = [1.99999999 2.99999999], b = 2.999999992563683, loss = 0.4735621852308702\n",
      "Iteration 103: w = [1.99999999 2.99999999], b = 2.999999992562535, loss = 0.3294705668624465\n",
      "Iteration 104: w = [1.99999999 2.99999999], b = 2.999999992561375, loss = 0.33639763027332203\n",
      "Iteration 105: w = [1.99999999 2.99999999], b = 2.9999999925432106, loss = 82.48656219707577\n",
      "Iteration 106: w = [1.99999999 2.99999999], b = 2.999999992546405, loss = 2.5508787337306527\n",
      "Iteration 107: w = [1.99999999 2.99999999], b = 2.9999999925352894, loss = 30.889175061742684\n",
      "Iteration 108: w = [1.99999999 2.99999999], b = 2.9999999925303724, loss = 6.044471143471332\n",
      "Iteration 109: w = [1.99999999 2.99999999], b = 2.999999992527951, loss = 1.4661237286122542\n",
      "Iteration 110: w = [1.99999999 2.99999999], b = 2.999999992532571, loss = 5.336354550379549\n",
      "Iteration 111: w = [1.99999999 2.99999999], b = 2.9999999925337297, loss = 0.33560422368431014\n",
      "Iteration 112: w = [1.99999999 2.99999999], b = 2.999999992524167, loss = 22.86150803959305\n",
      "Iteration 113: w = [1.99999999 2.99999999], b = 2.999999992524093, loss = 0.0013828847929692977\n",
      "Iteration 114: w = [1.99999999 2.99999999], b = 2.9999999925111083, loss = 42.15017653992928\n",
      "Iteration 115: w = [1.99999999 2.99999999], b = 2.9999999925015604, loss = 22.79088380519355\n",
      "Iteration 116: w = [1.99999999 2.99999999], b = 2.999999992489169, loss = 38.38620953190417\n",
      "Iteration 117: w = [1.99999999 2.99999999], b = 2.9999999924922114, loss = 2.314150813441142\n",
      "Iteration 118: w = [1.99999999 2.99999999], b = 2.999999992483967, loss = 16.99287302140553\n",
      "Iteration 119: w = [1.99999999 2.99999999], b = 2.9999999924613885, loss = 127.44483753162986\n",
      "Iteration 120: w = [1.99999999 2.99999999], b = 2.999999992464104, loss = 1.8435114523351617\n",
      "Iteration 121: w = [1.99999999 2.99999999], b = 2.999999992457229, loss = 11.816059778492928\n",
      "Iteration 122: w = [1.99999999 2.99999999], b = 2.9999999924633602, loss = 9.39716173507188\n",
      "Iteration 123: w = [1.99999999 2.99999999], b = 2.99999999244477, loss = 86.40231963750075\n",
      "Iteration 124: w = [1.99999999 2.99999999], b = 2.9999999924426257, loss = 1.1491412129410494\n",
      "Iteration 125: w = [1.99999999 2.99999999], b = 2.9999999924355887, loss = 12.380466426455293\n",
      "Iteration 126: w = [1.99999999 2.99999999], b = 2.999999992439096, loss = 3.075281518431635\n",
      "Iteration 127: w = [1.99999999 2.99999999], b = 2.999999992431572, loss = 14.153890561805685\n",
      "Iteration 128: w = [1.99999999 2.99999999], b = 2.999999992419513, loss = 36.35272578688477\n",
      "Iteration 129: w = [1.99999999 2.99999999], b = 2.999999992411742, loss = 15.096850403392471\n",
      "Iteration 130: w = [1.99999999 2.99999999], b = 2.9999999924099625, loss = 0.7915851373013048\n",
      "Iteration 131: w = [1.99999999 2.99999999], b = 2.9999999924147343, loss = 5.691845901426321\n",
      "Iteration 132: w = [1.99999999 2.99999999], b = 2.999999992419472, loss = 5.610983727230848\n",
      "Iteration 133: w = [1.99999999 2.99999999], b = 2.9999999924116825, loss = 15.168855882858267\n",
      "Iteration 134: w = [1.99999999 2.99999999], b = 2.999999992408125, loss = 3.1640844916942474\n",
      "Iteration 135: w = [1.99999999 2.99999999], b = 2.9999999924088323, loss = 0.12512302609202214\n",
      "Iteration 136: w = [1.99999999 2.99999999], b = 2.999999992410917, loss = 1.0864188912219814\n",
      "Iteration 137: w = [1.99999999 2.99999999], b = 2.999999992402527, loss = 17.59715516267694\n",
      "Iteration 138: w = [1.99999999 2.99999999], b = 2.999999992383406, loss = 91.40637499494397\n",
      "Iteration 139: w = [1.99999999 2.99999999], b = 2.9999999923722833, loss = 30.929615798999734\n",
      "Iteration 140: w = [1.99999999 2.99999999], b = 2.999999992372612, loss = 0.027016499561484376\n",
      "Iteration 141: w = [1.99999999 2.99999999], b = 2.9999999923637186, loss = 19.771912298839702\n",
      "Iteration 142: w = [1.99999999 2.99999999], b = 2.999999992367741, loss = 4.044858844711851\n",
      "Iteration 143: w = [1.99999999 2.99999999], b = 2.999999992361377, loss = 10.126040734976373\n",
      "Iteration 144: w = [1.99999999 2.99999999], b = 2.9999999923611753, loss = 0.01015244257276366\n",
      "Iteration 145: w = [1.99999999 2.99999999], b = 2.9999999923448915, loss = 66.29181801824352\n",
      "Iteration 146: w = [1.99999999 2.99999999], b = 2.9999999923262113, loss = 87.23551698160838\n",
      "Iteration 147: w = [1.99999999 2.99999999], b = 2.9999999923323633, loss = 9.461169703247922\n",
      "Iteration 148: w = [1.99999999 2.99999999], b = 2.9999999923358844, loss = 3.099542912155598\n",
      "Iteration 149: w = [1.99999999 2.99999999], b = 2.9999999923308254, loss = 6.398669954153674\n",
      "Iteration 150: w = [1.99999999 2.99999999], b = 2.999999992325455, loss = 7.210668945119132\n",
      "Iteration 151: w = [1.99999999 2.99999999], b = 2.9999999923287106, loss = 2.6498659885388958\n",
      "Iteration 152: w = [1.99999999 2.99999999], b = 2.9999999923118352, loss = 71.19318646131367\n",
      "Iteration 153: w = [1.99999999 2.99999999], b = 2.9999999923149465, loss = 2.4198220983010312\n",
      "Iteration 154: w = [1.99999999 2.99999999], b = 2.9999999923148297, loss = 0.003411075740217955\n",
      "Iteration 155: w = [1.99999999 2.99999999], b = 2.999999992297436, loss = 75.63619822186202\n",
      "Iteration 156: w = [1.99999999 2.99999999], b = 2.999999992286379, loss = 30.565154909387587\n",
      "Iteration 157: w = [1.99999999 2.99999999], b = 2.9999999922746303, loss = 34.50778907747631\n",
      "Iteration 158: w = [1.99999999 2.99999999], b = 2.999999992270769, loss = 3.727595686455337\n",
      "Iteration 159: w = [1.99999999 2.99999999], b = 2.9999999922585934, loss = 37.06067823732159\n",
      "Iteration 160: w = [1.99999999 2.99999999], b = 2.99999999224473, loss = 48.048434575867574\n",
      "Iteration 161: w = [1.99999999 2.99999999], b = 2.999999992237889, loss = 11.699309911911516\n",
      "Iteration 162: w = [1.99999999 2.99999999], b = 2.9999999922309777, loss = 11.941012422517261\n",
      "Iteration 163: w = [1.99999999 2.99999999], b = 2.999999992223944, loss = 12.368069913662412\n",
      "Iteration 164: w = [1.99999999 2.99999999], b = 2.9999999922109124, loss = 42.45604169977641\n",
      "Iteration 165: w = [1.99999999 2.99999999], b = 2.999999992200667, loss = 26.242223633511312\n",
      "Iteration 166: w = [1.99999999 2.99999999], b = 2.9999999921970333, loss = 3.300475014538175\n",
      "Iteration 167: w = [1.99999999 2.99999999], b = 2.9999999921961487, loss = 0.1957095386761303\n",
      "Iteration 168: w = [1.99999999 2.99999999], b = 2.9999999921897595, loss = 10.205734590606532\n",
      "Iteration 169: w = [1.99999999 2.99999999], b = 2.9999999921905927, loss = 0.1734895135572994\n",
      "Iteration 170: w = [1.99999999 2.99999999], b = 2.9999999921932865, loss = 1.8142790610094037\n",
      "Iteration 171: w = [1.99999999 2.99999999], b = 2.999999992182387, loss = 29.697871179452996\n",
      "Iteration 172: w = [1.99999999 2.99999999], b = 2.9999999921836267, loss = 0.38407543841370767\n",
      "Iteration 173: w = [1.99999999 2.99999999], b = 2.999999992166273, loss = 75.28616731154462\n",
      "Iteration 174: w = [1.99999999 2.99999999], b = 2.9999999921669303, loss = 0.10803601893217202\n",
      "Iteration 175: w = [1.99999999 2.99999999], b = 2.999999992166111, loss = 0.16790240258434488\n",
      "Iteration 176: w = [1.99999999 2.99999999], b = 2.9999999921533145, loss = 40.93590594119384\n",
      "Iteration 177: w = [1.99999999 2.99999999], b = 2.999999992147296, loss = 9.055236171886536\n",
      "Iteration 178: w = [1.99999999 2.99999999], b = 2.9999999921465705, loss = 0.13165650011447552\n",
      "Iteration 179: w = [1.99999999 2.99999999], b = 2.9999999921276164, loss = 89.81680104560846\n",
      "Iteration 180: w = [1.99999999 2.99999999], b = 2.999999992123475, loss = 4.288156174710251\n",
      "Iteration 181: w = [1.99999999 2.99999999], b = 2.9999999921290312, loss = 7.718523605859669\n",
      "Iteration 182: w = [1.99999999 2.99999999], b = 2.9999999921334393, loss = 4.857615074358303\n",
      "Iteration 183: w = [1.99999999 2.99999999], b = 2.9999999921146543, loss = 88.22034416891115\n",
      "Iteration 184: w = [1.99999999 2.99999999], b = 2.999999992110857, loss = 3.604690060151589\n",
      "Iteration 185: w = [1.99999999 2.99999999], b = 2.9999999920974845, loss = 44.70671685637864\n",
      "Iteration 186: w = [1.99999999 2.99999999], b = 2.9999999920840716, loss = 44.97570632544091\n",
      "Iteration 187: w = [1.99999999 2.99999999], b = 2.999999992084571, loss = 0.06223693969267495\n",
      "Iteration 188: w = [1.99999999 2.99999999], b = 2.9999999920628895, loss = 117.51825274529911\n",
      "Iteration 189: w = [1.99999999 2.99999999], b = 2.9999999920337905, loss = 211.68531520681677\n",
      "Iteration 190: w = [1.99999999 2.99999999], b = 2.999999992014308, loss = 94.89236171811211\n",
      "Iteration 191: w = [1.99999999 2.99999999], b = 2.9999999920195544, loss = 6.881471642141104\n",
      "Iteration 192: w = [1.99999999 2.99999999], b = 2.9999999920133256, loss = 9.698999105258054\n",
      "Iteration 193: w = [1.99999999 2.99999999], b = 2.9999999920089455, loss = 4.796184034971736\n",
      "Iteration 194: w = [1.99999999 2.99999999], b = 2.9999999920048426, loss = 4.208363156309373\n",
      "Iteration 195: w = [1.99999999 2.99999999], b = 2.999999992003782, loss = 0.28108407789992423\n",
      "Iteration 196: w = [1.99999999 2.99999999], b = 2.9999999920080684, loss = 4.593336195442853\n",
      "Iteration 197: w = [1.99999999 2.99999999], b = 2.999999992006889, loss = 0.3477343646841832\n",
      "Iteration 198: w = [1.99999999 2.99999999], b = 2.999999991978551, loss = 200.75657585874637\n",
      "Iteration 199: w = [1.99999999 2.99999999], b = 2.9999999919760154, loss = 1.6077553529465738\n",
      "Iteration 200: w = [1.99999999 2.99999999], b = 2.9999999919756286, loss = 0.03739565528580314\n",
      "Iteration 201: w = [1.99999999 2.99999999], b = 2.999999991961967, loss = 46.65802019817922\n",
      "Iteration 202: w = [1.99999999 2.99999999], b = 2.999999991953767, loss = 16.810046480754266\n",
      "Iteration 203: w = [1.99999999 2.99999999], b = 2.999999991945596, loss = 16.690001037816778\n",
      "Iteration 204: w = [1.99999999 2.99999999], b = 2.999999991933558, loss = 36.23186440440987\n",
      "Iteration 205: w = [1.99999999 2.99999999], b = 2.9999999919366873, loss = 2.448491289418961\n",
      "Iteration 206: w = [1.99999999 2.99999999], b = 2.9999999919370293, loss = 0.029225269502643998\n",
      "Iteration 207: w = [1.99999999 2.99999999], b = 2.999999991927304, loss = 23.64336586308955\n",
      "Iteration 208: w = [1.99999999 2.99999999], b = 2.99999999192275, loss = 5.184624848791981\n",
      "Iteration 209: w = [1.99999999 2.99999999], b = 2.9999999919074045, loss = 58.87114110404515\n",
      "Iteration 210: w = [1.99999999 2.99999999], b = 2.999999991903545, loss = 3.723963782594067\n",
      "Iteration 211: w = [1.99999999 2.99999999], b = 2.999999991899038, loss = 5.077919169199034\n",
      "Iteration 212: w = [1.99999999 2.99999999], b = 2.9999999918881195, loss = 29.803248093230817\n",
      "Iteration 213: w = [1.99999999 2.99999999], b = 2.9999999918911824, loss = 2.3454404617091287\n",
      "Iteration 214: w = [1.99999999 2.99999999], b = 2.999999991885788, loss = 7.275201292602185\n",
      "Iteration 215: w = [1.99999999 2.99999999], b = 2.9999999918735414, loss = 37.49569166009128\n",
      "Iteration 216: w = [1.99999999 2.99999999], b = 2.9999999918563547, loss = 73.84430154614473\n",
      "Iteration 217: w = [1.99999999 2.99999999], b = 2.9999999918439633, loss = 38.38681461867294\n",
      "Iteration 218: w = [1.99999999 2.99999999], b = 2.999999991846608, loss = 1.7481246729865487\n",
      "Iteration 219: w = [1.99999999 2.99999999], b = 2.999999991846197, loss = 0.042201829574756815\n",
      "Iteration 220: w = [1.99999999 2.99999999], b = 2.9999999918403186, loss = 8.638564506519362\n",
      "Iteration 221: w = [1.99999999 2.99999999], b = 2.9999999918288105, loss = 33.11002608503067\n",
      "Iteration 222: w = [1.99999999 2.99999999], b = 2.99999999182084, loss = 15.881711901486732\n",
      "Iteration 223: w = [1.99999999 2.99999999], b = 2.999999991803827, loss = 72.36128461167841\n",
      "Iteration 224: w = [1.99999999 2.99999999], b = 2.999999991801174, loss = 1.7598813309672845\n",
      "Iteration 225: w = [1.99999999 2.99999999], b = 2.999999991778258, loss = 131.28256290945652\n",
      "Iteration 226: w = [1.99999999 2.99999999], b = 2.999999991773115, loss = 6.6123341568206255\n",
      "Iteration 227: w = [1.99999999 2.99999999], b = 2.9999999917626656, loss = 27.298732390874815\n",
      "Iteration 228: w = [1.99999999 2.99999999], b = 2.9999999917612277, loss = 0.5168773585715191\n",
      "Iteration 229: w = [1.99999999 2.99999999], b = 2.999999991752757, loss = 17.937666001338396\n",
      "Iteration 230: w = [1.99999999 2.99999999], b = 2.9999999917507725, loss = 0.9845378463458588\n",
      "Iteration 231: w = [1.99999999 2.99999999], b = 2.999999991749479, loss = 0.41846035492184513\n",
      "Iteration 232: w = [1.99999999 2.99999999], b = 2.9999999917360873, loss = 44.83203522209281\n",
      "Iteration 233: w = [1.99999999 2.99999999], b = 2.99999999172317, loss = 41.712515599930235\n",
      "Iteration 234: w = [1.99999999 2.99999999], b = 2.999999991708829, loss = 51.41536886989527\n",
      "Iteration 235: w = [1.99999999 2.99999999], b = 2.9999999917062574, loss = 1.6535239175983092\n",
      "Iteration 236: w = [1.99999999 2.99999999], b = 2.999999991691897, loss = 51.55484227326255\n",
      "Iteration 237: w = [1.99999999 2.99999999], b = 2.9999999916924462, loss = 0.07538720013385689\n",
      "Iteration 238: w = [1.99999999 2.99999999], b = 2.999999991700023, loss = 14.350584914032176\n",
      "Iteration 239: w = [1.99999999 2.99999999], b = 2.9999999917014963, loss = 0.5429368335994741\n",
      "Iteration 240: w = [1.99999999 2.99999999], b = 2.999999991699719, loss = 0.7897310574989933\n",
      "Iteration 241: w = [1.99999999 2.99999999], b = 2.999999991704929, loss = 6.786534595595482\n",
      "Iteration 242: w = [1.99999999 2.99999999], b = 2.9999999917069924, loss = 1.064183260523993\n",
      "Iteration 243: w = [1.99999999 2.99999999], b = 2.9999999917099345, loss = 2.16365929935243\n",
      "Iteration 244: w = [1.99999999 2.99999999], b = 2.9999999916941955, loss = 61.92781569976209\n",
      "Iteration 245: w = [1.99999999 2.99999999], b = 2.999999991684814, loss = 22.00279266817004\n",
      "Iteration 246: w = [1.99999999 2.99999999], b = 2.9999999916648536, loss = 99.6063863550534\n",
      "Iteration 247: w = [1.99999999 2.99999999], b = 2.999999991666155, loss = 0.4233956716137676\n",
      "Iteration 248: w = [1.99999999 2.99999999], b = 2.999999991661394, loss = 5.6659855117923135\n",
      "Iteration 249: w = [1.99999999 2.99999999], b = 2.999999991653657, loss = 14.966608945476366\n",
      "Iteration 250: w = [1.99999999 2.99999999], b = 2.9999999916529143, loss = 0.1377842761106833\n",
      "Iteration 251: w = [1.99999999 2.99999999], b = 2.999999991648116, loss = 5.756012383342218\n",
      "Iteration 252: w = [1.99999999 2.99999999], b = 2.999999991650921, loss = 1.9669155877638753\n",
      "Iteration 253: w = [1.99999999 2.99999999], b = 2.999999991651246, loss = 0.026434394701911224\n",
      "Iteration 254: w = [1.99999999 2.99999999], b = 2.9999999916434548, loss = 15.174510030278586\n",
      "Iteration 255: w = [1.99999999 2.99999999], b = 2.9999999916448887, loss = 0.514123346260152\n",
      "Iteration 256: w = [1.99999999 2.99999999], b = 2.999999991634824, loss = 25.325426112144097\n",
      "Iteration 257: w = [1.99999999 2.99999999], b = 2.9999999916335116, loss = 0.4303946401646652\n",
      "Iteration 258: w = [1.99999999 2.99999999], b = 2.9999999916361246, loss = 1.7070603465304102\n",
      "Iteration 259: w = [1.99999999 2.99999999], b = 2.999999991635416, loss = 0.12552252185557283\n",
      "Iteration 260: w = [1.99999999 2.99999999], b = 2.9999999916384033, loss = 2.231280528897497\n",
      "Iteration 261: w = [1.99999999 2.99999999], b = 2.9999999916321913, loss = 9.646997827328164\n",
      "Iteration 262: w = [1.99999999 2.99999999], b = 2.9999999916209203, loss = 31.75942588852739\n",
      "Iteration 263: w = [1.99999999 2.99999999], b = 2.9999999916026656, loss = 83.30697340156749\n",
      "Iteration 264: w = [1.99999999 2.99999999], b = 2.999999991595465, loss = 12.962266059057947\n",
      "Iteration 265: w = [1.99999999 2.99999999], b = 2.999999991593661, loss = 0.8137821733660088\n",
      "Iteration 266: w = [1.99999999 2.99999999], b = 2.9999999915989086, loss = 6.884720796126964\n",
      "Iteration 267: w = [1.99999999 2.99999999], b = 2.9999999916014963, loss = 1.6740552602548602\n",
      "Iteration 268: w = [1.99999999 2.99999999], b = 2.9999999915980236, loss = 3.0154018958198066\n",
      "Iteration 269: w = [1.99999999 2.99999999], b = 2.999999991602718, loss = 5.509705827089083\n",
      "Iteration 270: w = [1.99999999 2.99999999], b = 2.999999991588159, loss = 52.99202849956518\n",
      "Iteration 271: w = [1.99999999 2.99999999], b = 2.999999991580735, loss = 13.778711254423031\n",
      "Iteration 272: w = [1.99999999 2.99999999], b = 2.999999991565939, loss = 54.730044860779806\n",
      "Iteration 273: w = [1.99999999 2.99999999], b = 2.999999991561022, loss = 6.043648468830305\n",
      "Iteration 274: w = [1.99999999 2.99999999], b = 2.9999999915541076, loss = 11.95218346657832\n",
      "Iteration 275: w = [1.99999999 2.99999999], b = 2.9999999915532274, loss = 0.19367277486099252\n",
      "Iteration 276: w = [1.99999999 2.99999999], b = 2.999999991544962, loss = 17.078884999247986\n",
      "Iteration 277: w = [1.99999999 2.99999999], b = 2.9999999915469377, loss = 0.9760232097892168\n",
      "Iteration 278: w = [1.99999999 2.99999999], b = 2.9999999915464426, loss = 0.06124301711771964\n",
      "Iteration 279: w = [1.99999999 2.99999999], b = 2.9999999915398567, loss = 10.843823028452615\n",
      "Iteration 280: w = [1.99999999 2.99999999], b = 2.9999999915162796, loss = 138.9712133770784\n",
      "Iteration 281: w = [1.99999999 2.99999999], b = 2.9999999915154225, loss = 0.1835856015821937\n",
      "Iteration 282: w = [1.99999999 2.99999999], b = 2.999999991498955, loss = 67.795270714429\n",
      "Iteration 283: w = [1.99999999 2.99999999], b = 2.9999999914995046, loss = 0.07561922007946759\n",
      "Iteration 284: w = [1.99999999 2.99999999], b = 2.9999999914986044, loss = 0.20252858896907266\n",
      "Iteration 285: w = [1.99999999 2.99999999], b = 2.999999991490525, loss = 16.318034946041177\n",
      "Iteration 286: w = [1.99999999 2.99999999], b = 2.9999999914919804, loss = 0.5294554156418385\n",
      "Iteration 287: w = [1.99999999 2.99999999], b = 2.999999991482649, loss = 21.76828483137429\n",
      "Iteration 288: w = [1.99999999 2.99999999], b = 2.9999999914653737, loss = 74.61204889052294\n",
      "Iteration 289: w = [1.99999999 2.99999999], b = 2.999999991460425, loss = 6.12160707020781\n",
      "Iteration 290: w = [1.99999999 2.99999999], b = 2.99999999146075, loss = 0.026374699284977172\n",
      "Iteration 291: w = [1.99999999 2.99999999], b = 2.999999991458922, loss = 0.835219698509589\n",
      "Iteration 292: w = [1.99999999 2.99999999], b = 2.99999999145656, loss = 1.394683830802096\n",
      "Iteration 293: w = [1.99999999 2.99999999], b = 2.9999999914438344, loss = 40.48267227069452\n",
      "Iteration 294: w = [1.99999999 2.99999999], b = 2.9999999914399402, loss = 3.7909398166654857\n",
      "Iteration 295: w = [1.99999999 2.99999999], b = 2.999999991433232, loss = 11.250344597585006\n",
      "Iteration 296: w = [1.99999999 2.99999999], b = 2.999999991430738, loss = 1.5548569866797441\n",
      "Iteration 297: w = [1.99999999 2.99999999], b = 2.9999999914325426, loss = 0.8143229723051949\n",
      "Iteration 298: w = [1.99999999 2.99999999], b = 2.99999999140363, loss = 208.9818113471653\n",
      "Iteration 299: w = [1.99999999 2.99999999], b = 2.9999999913848048, loss = 88.59942382907703\n",
      "Iteration 300: w = [1.99999999 2.99999999], b = 2.999999991383238, loss = 0.6137987823813983\n",
      "Iteration 301: w = [1.99999999 2.99999999], b = 2.9999999913796667, loss = 3.1889214548091744\n",
      "Iteration 302: w = [1.99999999 2.99999999], b = 2.9999999913834765, loss = 3.628806060189236\n",
      "Iteration 303: w = [1.99999999 2.99999999], b = 2.9999999913764, loss = 12.519417660480586\n",
      "Iteration 304: w = [1.99999999 2.99999999], b = 2.99999999135896, loss = 76.03585330705974\n",
      "Iteration 305: w = [1.99999999 2.99999999], b = 2.9999999913522775, loss = 11.164966993316236\n",
      "Iteration 306: w = [1.99999999 2.99999999], b = 2.999999991343783, loss = 18.039960308413747\n",
      "Iteration 307: w = [1.99999999 2.99999999], b = 2.999999991341117, loss = 1.776613541520496\n",
      "Iteration 308: w = [1.99999999 2.99999999], b = 2.999999991333888, loss = 13.064793994218114\n",
      "Iteration 309: w = [1.99999999 2.99999999], b = 2.999999991342185, loss = 17.20957511329073\n",
      "Iteration 310: w = [1.99999999 2.99999999], b = 2.999999991340359, loss = 0.8334989462864507\n",
      "Iteration 311: w = [1.99999999 2.99999999], b = 2.9999999913267787, loss = 46.105640821525405\n",
      "Iteration 312: w = [1.99999999 2.99999999], b = 2.999999991319532, loss = 13.129050401743818\n",
      "Iteration 313: w = [1.99999999 2.99999999], b = 2.999999991310246, loss = 21.557600663106005\n",
      "Iteration 314: w = [1.99999999 2.99999999], b = 2.999999991299093, loss = 31.100088437241283\n",
      "Iteration 315: w = [1.99999999 2.99999999], b = 2.9999999912903013, loss = 19.3234436124965\n",
      "Iteration 316: w = [1.99999999 2.99999999], b = 2.9999999912697692, loss = 105.38910201475727\n",
      "Iteration 317: w = [1.99999999 2.99999999], b = 2.9999999912594304, loss = 26.72315022778178\n",
      "Iteration 318: w = [1.99999999 2.99999999], b = 2.9999999912583477, loss = 0.2930997059190415\n",
      "Iteration 319: w = [1.99999999 2.99999999], b = 2.999999991255247, loss = 2.4033613182208904\n",
      "Iteration 320: w = [1.99999999 2.99999999], b = 2.999999991226306, loss = 209.39900432358888\n",
      "Iteration 321: w = [1.99999999 2.99999999], b = 2.9999999912273454, loss = 0.27010522188745945\n",
      "Iteration 322: w = [1.99999999 2.99999999], b = 2.999999991219952, loss = 13.664508894684976\n",
      "Iteration 323: w = [1.99999999 2.99999999], b = 2.9999999912175723, loss = 1.4158311776707244\n",
      "Iteration 324: w = [1.99999999 2.99999999], b = 2.9999999912089215, loss = 18.709187575709695\n",
      "Iteration 325: w = [1.99999999 2.99999999], b = 2.9999999912005646, loss = 17.459514488890733\n",
      "Iteration 326: w = [1.99999999 2.99999999], b = 2.999999991191966, loss = 18.483217658964836\n",
      "Iteration 327: w = [1.99999999 2.99999999], b = 2.99999999118766, loss = 4.636375323549653\n",
      "Iteration 328: w = [1.99999999 2.99999999], b = 2.999999991174945, loss = 40.4162903788589\n",
      "Iteration 329: w = [1.99999999 2.99999999], b = 2.9999999911774275, loss = 1.5405622205079739\n",
      "Iteration 330: w = [1.99999999 2.99999999], b = 2.999999991174825, loss = 1.6933238910286759\n",
      "Iteration 331: w = [1.99999999 2.99999999], b = 2.9999999911778636, loss = 2.308136697871186\n",
      "Iteration 332: w = [1.99999999 2.99999999], b = 2.9999999911648474, loss = 42.35669646414057\n",
      "Iteration 333: w = [1.99999999 2.99999999], b = 2.9999999911638815, loss = 0.23329214473327067\n",
      "Iteration 334: w = [1.99999999 2.99999999], b = 2.9999999911611237, loss = 1.9011285026667442\n",
      "Iteration 335: w = [1.99999999 2.99999999], b = 2.9999999911672, loss = 9.231125818451744\n",
      "Iteration 336: w = [1.99999999 2.99999999], b = 2.999999991169099, loss = 0.9013802791672901\n",
      "Iteration 337: w = [1.99999999 2.99999999], b = 2.999999991160676, loss = 17.73763052489172\n",
      "Iteration 338: w = [1.99999999 2.99999999], b = 2.999999991140566, loss = 101.1064151167015\n",
      "Iteration 339: w = [1.99999999 2.99999999], b = 2.999999991127837, loss = 40.50703932147254\n",
      "Iteration 340: w = [1.99999999 2.99999999], b = 2.999999991109437, loss = 84.63790914569074\n",
      "Iteration 341: w = [1.99999999 2.99999999], b = 2.999999991109446, loss = 1.904223313556912e-05\n",
      "Iteration 342: w = [1.99999999 2.99999999], b = 2.9999999910897466, loss = 97.01758881863991\n",
      "Iteration 343: w = [1.99999999 2.99999999], b = 2.99999999107632, loss = 45.06886341119589\n",
      "Iteration 344: w = [1.99999999 2.99999999], b = 2.999999991061726, loss = 53.24846811761909\n",
      "Iteration 345: w = [1.99999999 2.99999999], b = 2.9999999910524884, loss = 21.33286136511883\n",
      "Iteration 346: w = [1.99999999 2.99999999], b = 2.999999991056502, loss = 4.0272953004608905\n",
      "Iteration 347: w = [1.99999999 2.99999999], b = 2.999999991056635, loss = 0.004409873963612438\n",
      "Iteration 348: w = [1.99999999 2.99999999], b = 2.9999999910625306, loss = 8.689822285071605\n",
      "Iteration 349: w = [1.99999999 2.99999999], b = 2.999999991051913, loss = 28.185191284701396\n",
      "Iteration 350: w = [1.99999999 2.99999999], b = 2.9999999910415847, loss = 26.66840233561234\n",
      "Iteration 351: w = [1.99999999 2.99999999], b = 2.9999999910275665, loss = 49.12631902964373\n",
      "Iteration 352: w = [1.99999999 2.99999999], b = 2.999999991023695, loss = 3.747134605300651\n",
      "Iteration 353: w = [1.99999999 2.99999999], b = 2.9999999910111237, loss = 39.5088316376255\n",
      "Iteration 354: w = [1.99999999 2.99999999], b = 2.9999999910027384, loss = 17.577953119125436\n",
      "Iteration 355: w = [1.99999999 2.99999999], b = 2.9999999909887176, loss = 49.144191485821814\n",
      "Iteration 356: w = [1.99999999 2.99999999], b = 2.9999999909896125, loss = 0.20011387645714634\n",
      "Iteration 357: w = [1.99999999 2.99999999], b = 2.999999990974766, loss = 55.10387612444768\n",
      "Iteration 358: w = [1.99999999 2.99999999], b = 2.99999999097765, loss = 2.0794968308803425\n",
      "Iteration 359: w = [1.99999999 2.99999999], b = 2.9999999909692328, loss = 17.71274848588606\n",
      "Iteration 360: w = [1.99999999 2.99999999], b = 2.9999999909614377, loss = 15.191030593491455\n",
      "Iteration 361: w = [1.99999999 2.99999999], b = 2.9999999909493775, loss = 36.362031565025106\n",
      "Iteration 362: w = [1.99999999 2.99999999], b = 2.9999999909561033, loss = 11.308545049803383\n",
      "Iteration 363: w = [1.99999999 2.99999999], b = 2.9999999909488793, loss = 13.047184850860608\n",
      "Iteration 364: w = [1.99999999 2.99999999], b = 2.9999999909397537, loss = 20.818129923410783\n",
      "Iteration 365: w = [1.99999999 2.99999999], b = 2.999999990942247, loss = 1.5541192358894498\n",
      "Iteration 366: w = [1.99999999 2.99999999], b = 2.999999990939595, loss = 1.7579704983638913\n",
      "Iteration 367: w = [1.99999999 2.99999999], b = 2.9999999909360864, loss = 3.077472337421889\n",
      "Iteration 368: w = [1.99999999 2.99999999], b = 2.999999990935944, loss = 0.005087899631509484\n",
      "Iteration 369: w = [1.99999999 2.99999999], b = 2.999999990938267, loss = 1.3489675265567527\n",
      "Iteration 370: w = [1.99999999 2.99999999], b = 2.9999999909168755, loss = 114.39611551211425\n",
      "Iteration 371: w = [1.99999999 2.99999999], b = 2.999999990916214, loss = 0.109463258616754\n",
      "Iteration 372: w = [1.99999999 2.99999999], b = 2.9999999909073143, loss = 19.800018858851633\n",
      "Iteration 373: w = [1.99999999 2.99999999], b = 2.999999990893295, loss = 49.13684535871348\n",
      "Iteration 374: w = [1.99999999 2.99999999], b = 2.99999999089262, loss = 0.1138668157859516\n",
      "Iteration 375: w = [1.99999999 2.99999999], b = 2.9999999908796884, loss = 41.80605746409029\n",
      "Iteration 376: w = [1.99999999 2.99999999], b = 2.999999990877978, loss = 0.7311174217202248\n",
      "Iteration 377: w = [1.99999999 2.99999999], b = 2.9999999908773023, loss = 0.11423942005336732\n",
      "Iteration 378: w = [1.99999999 2.99999999], b = 2.999999990873861, loss = 2.9604151815480986\n",
      "Iteration 379: w = [1.99999999 2.99999999], b = 2.999999990864012, loss = 24.251463689725874\n",
      "Iteration 380: w = [1.99999999 2.99999999], b = 2.999999990861362, loss = 1.7556722953357893\n",
      "Iteration 381: w = [1.99999999 2.99999999], b = 2.9999999908528547, loss = 18.094847565484635\n",
      "Iteration 382: w = [1.99999999 2.99999999], b = 2.9999999908541777, loss = 0.43754010549127625\n",
      "Iteration 383: w = [1.99999999 2.99999999], b = 2.999999990840054, loss = 49.871629205159614\n",
      "Iteration 384: w = [1.99999999 2.99999999], b = 2.9999999908389814, loss = 0.2876142523264332\n",
      "Iteration 385: w = [1.99999999 2.99999999], b = 2.9999999908316335, loss = 13.498706519904552\n",
      "Iteration 386: w = [1.99999999 2.99999999], b = 2.9999999908056245, loss = 169.11576482446708\n",
      "Iteration 387: w = [1.99999999 2.99999999], b = 2.9999999907964674, loss = 20.963816172095633\n",
      "Iteration 388: w = [1.99999999 2.99999999], b = 2.999999990779829, loss = 69.20706866893839\n",
      "Iteration 389: w = [1.99999999 2.99999999], b = 2.999999990783473, loss = 3.3191222540472887\n",
      "Iteration 390: w = [1.99999999 2.99999999], b = 2.9999999907796746, loss = 3.6066918987073957\n",
      "Iteration 391: w = [1.99999999 2.99999999], b = 2.9999999907816193, loss = 0.9454238704061303\n",
      "Iteration 392: w = [1.99999999 2.99999999], b = 2.999999990781621, loss = 7.808334729188176e-07\n",
      "Iteration 393: w = [1.99999999 2.99999999], b = 2.9999999907726247, loss = 20.233041258737426\n",
      "Iteration 394: w = [1.99999999 2.99999999], b = 2.9999999907566193, loss = 64.04312027185566\n",
      "Iteration 395: w = [1.99999999 2.99999999], b = 2.999999990742847, loss = 47.418489008112026\n",
      "Iteration 396: w = [1.99999999 2.99999999], b = 2.9999999907286505, loss = 50.38544545391471\n",
      "Iteration 397: w = [1.99999999 2.99999999], b = 2.999999990720364, loss = 17.166684567375064\n",
      "Iteration 398: w = [1.99999999 2.99999999], b = 2.999999990704112, loss = 66.03209187929778\n",
      "Iteration 399: w = [1.99999999 2.99999999], b = 2.999999990697551, loss = 10.761718951331321\n",
      "Iteration 400: w = [1.99999999 2.99999999], b = 2.9999999906973573, loss = 0.009367263157017212\n",
      "Iteration 401: w = [1.99999999 2.99999999], b = 2.9999999906857697, loss = 33.5679865017266\n",
      "Iteration 402: w = [1.99999999 2.99999999], b = 2.9999999906800676, loss = 8.128506444751684\n",
      "Iteration 403: w = [1.99999999 2.99999999], b = 2.999999990673104, loss = 12.123064154687727\n",
      "Iteration 404: w = [1.99999999 2.99999999], b = 2.9999999906675328, loss = 7.759141956238263\n",
      "Iteration 405: w = [1.99999999 2.99999999], b = 2.999999990649368, loss = 82.4884371277111\n",
      "Iteration 406: w = [1.99999999 2.99999999], b = 2.9999999906505415, loss = 0.3442672788271647\n",
      "Iteration 407: w = [1.99999999 2.99999999], b = 2.9999999906535435, loss = 2.253314111582654\n",
      "Iteration 408: w = [1.99999999 2.99999999], b = 2.9999999906369617, loss = 68.74062860679511\n",
      "Iteration 409: w = [1.99999999 2.99999999], b = 2.9999999906229626, loss = 48.991770172215375\n",
      "Iteration 410: w = [1.99999999 2.99999999], b = 2.9999999906177126, loss = 6.890996919278658\n",
      "Iteration 411: w = [1.99999999 2.99999999], b = 2.999999990612283, loss = 7.369864107165\n",
      "Iteration 412: w = [1.99999999 2.99999999], b = 2.9999999905948407, loss = 76.05839210621654\n",
      "Iteration 413: w = [1.99999999 2.99999999], b = 2.9999999906009025, loss = 9.186766610944417\n",
      "Iteration 414: w = [1.99999999 2.99999999], b = 2.9999999905863177, loss = 53.1804926912936\n",
      "Iteration 415: w = [1.99999999 2.99999999], b = 2.999999990579976, loss = 10.053754384713015\n",
      "Iteration 416: w = [1.99999999 2.99999999], b = 2.9999999905823684, loss = 1.4306915459762608\n",
      "Iteration 417: w = [1.99999999 2.99999999], b = 2.9999999905858408, loss = 3.0142682619681493\n",
      "Iteration 418: w = [1.99999999 2.99999999], b = 2.999999990569669, loss = 65.3814907298323\n",
      "Iteration 419: w = [1.99999999 2.99999999], b = 2.9999999905550365, loss = 53.525893544542484\n",
      "Iteration 420: w = [1.99999999 2.99999999], b = 2.9999999905609402, loss = 8.713336395004289\n",
      "Iteration 421: w = [1.99999999 2.99999999], b = 2.999999990565, loss = 4.120804076978947\n",
      "Iteration 422: w = [1.99999999 2.99999999], b = 2.9999999905626917, loss = 1.3320513669227105\n",
      "Iteration 423: w = [1.99999999 2.99999999], b = 2.999999990563389, loss = 0.12147998916474337\n",
      "Iteration 424: w = [1.99999999 2.99999999], b = 2.9999999905564905, loss = 11.897869764941039\n",
      "Iteration 425: w = [1.99999999 2.99999999], b = 2.999999990552887, loss = 3.2456995646981053\n",
      "Iteration 426: w = [1.99999999 2.99999999], b = 2.9999999905371917, loss = 61.587666457692535\n",
      "Iteration 427: w = [1.99999999 2.99999999], b = 2.9999999905235235, loss = 46.70604330811429\n",
      "Iteration 428: w = [1.99999999 2.99999999], b = 2.9999999905173733, loss = 9.45627574315509\n",
      "Iteration 429: w = [1.99999999 2.99999999], b = 2.9999999905190253, loss = 0.6824331854945731\n",
      "Iteration 430: w = [1.99999999 2.99999999], b = 2.999999990515201, loss = 3.6563771047672824\n",
      "Iteration 431: w = [1.99999999 2.99999999], b = 2.99999999050896, loss = 9.736914884845639\n",
      "Iteration 432: w = [1.99999999 2.99999999], b = 2.999999990504193, loss = 5.6804410343945095\n",
      "Iteration 433: w = [1.99999999 2.99999999], b = 2.9999999904848345, loss = 93.68872360824965\n",
      "Iteration 434: w = [1.99999999 2.99999999], b = 2.9999999904884245, loss = 3.2220849252169677\n",
      "Iteration 435: w = [1.99999999 2.99999999], b = 2.99999999049336, loss = 6.0903115732802116\n",
      "Iteration 436: w = [1.99999999 2.99999999], b = 2.999999990490078, loss = 2.69328692625914\n",
      "Iteration 437: w = [1.99999999 2.99999999], b = 2.999999990483012, loss = 12.48236987964595\n",
      "Iteration 438: w = [1.99999999 2.99999999], b = 2.9999999904883516, loss = 7.128108072089045\n",
      "Iteration 439: w = [1.99999999 2.99999999], b = 2.999999990478484, loss = 24.342507284850132\n",
      "Iteration 440: w = [1.99999999 2.99999999], b = 2.9999999904838077, loss = 7.085148494305144\n",
      "Iteration 441: w = [1.99999999 2.99999999], b = 2.9999999904852754, loss = 0.5384573240140901\n",
      "Iteration 442: w = [1.99999999 2.99999999], b = 2.999999990476603, loss = 18.803286218099828\n",
      "Iteration 443: w = [1.99999999 2.99999999], b = 2.9999999904726073, loss = 3.9913358610578826\n",
      "Iteration 444: w = [1.99999999 2.99999999], b = 2.9999999904691714, loss = 2.9511171208096694\n",
      "Iteration 445: w = [1.99999999 2.99999999], b = 2.99999999046522, loss = 3.9032680477891413\n",
      "Iteration 446: w = [1.99999999 2.99999999], b = 2.9999999904624493, loss = 1.9193499425261453\n",
      "Iteration 447: w = [1.99999999 2.99999999], b = 2.999999990457537, loss = 6.031853879911944\n",
      "Iteration 448: w = [1.99999999 2.99999999], b = 2.9999999904591497, loss = 0.6501268842614145\n",
      "Iteration 449: w = [1.99999999 2.99999999], b = 2.9999999904590102, loss = 0.004853476234111936\n",
      "Iteration 450: w = [1.99999999 2.99999999], b = 2.999999990451127, loss = 15.535509753168013\n",
      "Iteration 451: w = [1.99999999 2.99999999], b = 2.9999999904415997, loss = 22.693478890625837\n",
      "Iteration 452: w = [1.99999999 2.99999999], b = 2.9999999904386394, loss = 2.191077850660742\n",
      "Iteration 453: w = [1.99999999 2.99999999], b = 2.999999990424741, loss = 48.29115076786461\n",
      "Iteration 454: w = [1.99999999 2.99999999], b = 2.999999990428732, loss = 3.9816710999114764\n",
      "Iteration 455: w = [1.99999999 2.99999999], b = 2.9999999904346755, loss = 8.830521323230686\n",
      "Iteration 456: w = [1.99999999 2.99999999], b = 2.999999990432972, loss = 0.7253502660817099\n",
      "Iteration 457: w = [1.99999999 2.99999999], b = 2.999999990416241, loss = 69.98239695150467\n",
      "Iteration 458: w = [1.99999999 2.99999999], b = 2.9999999904129533, loss = 2.7023218793651385\n",
      "Iteration 459: w = [1.99999999 2.99999999], b = 2.999999990393246, loss = 97.09663123543503\n",
      "Iteration 460: w = [1.99999999 2.99999999], b = 2.9999999903824968, loss = 28.88667676726733\n",
      "Iteration 461: w = [1.99999999 2.99999999], b = 2.9999999903686394, loss = 48.008115474566374\n",
      "Iteration 462: w = [1.99999999 2.99999999], b = 2.999999990349254, loss = 93.94725750155497\n",
      "Iteration 463: w = [1.99999999 2.99999999], b = 2.9999999903414736, loss = 15.134646656609181\n",
      "Iteration 464: w = [1.99999999 2.99999999], b = 2.99999999032799, loss = 45.4502681550876\n",
      "Iteration 465: w = [1.99999999 2.99999999], b = 2.9999999903139276, loss = 49.43977480451008\n",
      "Iteration 466: w = [1.99999999 2.99999999], b = 2.999999990315288, loss = 0.4624342623857639\n",
      "Iteration 467: w = [1.99999999 2.99999999], b = 2.9999999903139227, loss = 0.4658127128287651\n",
      "Iteration 468: w = [1.99999999 2.99999999], b = 2.9999999903091896, loss = 5.600117689356278\n",
      "Iteration 469: w = [1.99999999 2.99999999], b = 2.999999990314579, loss = 7.261777896292748\n",
      "Iteration 470: w = [1.99999999 2.99999999], b = 2.9999999903163737, loss = 0.8052977454436551\n",
      "Iteration 471: w = [1.99999999 2.99999999], b = 2.999999990307543, loss = 19.494713980210093\n",
      "Iteration 472: w = [1.99999999 2.99999999], b = 2.999999990284089, loss = 137.52558318547452\n",
      "Iteration 473: w = [1.99999999 2.99999999], b = 2.999999990280499, loss = 3.2216587963616226\n",
      "Iteration 474: w = [1.99999999 2.99999999], b = 2.9999999902677508, loss = 40.62784509278129\n",
      "Iteration 475: w = [1.99999999 2.99999999], b = 2.9999999902493153, loss = 84.96699574132565\n",
      "Iteration 476: w = [1.99999999 2.99999999], b = 2.9999999902496444, loss = 0.027063853083479448\n",
      "Iteration 477: w = [1.99999999 2.99999999], b = 2.999999990241545, loss = 16.400476386053953\n",
      "Iteration 478: w = [1.99999999 2.99999999], b = 2.99999999023043, loss = 30.88518211899448\n",
      "Iteration 479: w = [1.99999999 2.99999999], b = 2.9999999902307564, loss = 0.026650600911450793\n",
      "Iteration 480: w = [1.99999999 2.99999999], b = 2.999999990237142, loss = 10.194322137076815\n",
      "Iteration 481: w = [1.99999999 2.99999999], b = 2.999999990235421, loss = 0.7403022041571525\n",
      "Iteration 482: w = [1.99999999 2.99999999], b = 2.999999990227002, loss = 17.720710000501704\n",
      "Iteration 483: w = [1.99999999 2.99999999], b = 2.99999999022182, loss = 6.713222535710091\n",
      "Iteration 484: w = [1.99999999 2.99999999], b = 2.999999990224239, loss = 1.4629582428235521\n",
      "Iteration 485: w = [1.99999999 2.99999999], b = 2.9999999901968293, loss = 187.822947222103\n",
      "Iteration 486: w = [1.99999999 2.99999999], b = 2.9999999901782206, loss = 86.5717738756491\n",
      "Iteration 487: w = [1.99999999 2.99999999], b = 2.999999990183643, loss = 7.350316779996436\n",
      "Iteration 488: w = [1.99999999 2.99999999], b = 2.99999999016772, loss = 63.38263514529668\n",
      "Iteration 489: w = [1.99999999 2.99999999], b = 2.9999999901503687, loss = 75.26920208856974\n",
      "Iteration 490: w = [1.99999999 2.99999999], b = 2.99999999013389, loss = 67.88897908663205\n",
      "Iteration 491: w = [1.99999999 2.99999999], b = 2.9999999901286856, loss = 6.771026747917989\n",
      "Iteration 492: w = [1.99999999 2.99999999], b = 2.9999999901309455, loss = 1.2770454396392708\n",
      "Iteration 493: w = [1.99999999 2.99999999], b = 2.9999999901298464, loss = 0.30190675965265995\n",
      "Iteration 494: w = [1.99999999 2.99999999], b = 2.9999999901287207, loss = 0.31682126701496\n",
      "Iteration 495: w = [1.99999999 2.99999999], b = 2.9999999901165637, loss = 36.94647159338078\n",
      "Iteration 496: w = [1.99999999 2.99999999], b = 2.9999999901185403, loss = 0.9765614139048241\n",
      "Iteration 497: w = [1.99999999 2.99999999], b = 2.9999999901230665, loss = 5.12178763922567\n",
      "Iteration 498: w = [1.99999999 2.99999999], b = 2.9999999901201755, loss = 2.0896346944061874\n",
      "Iteration 499: w = [1.99999999 2.99999999], b = 2.9999999901218737, loss = 0.7208492015407609\n",
      "Iteration 500: w = [1.99999999 2.99999999], b = 2.999999990108452, loss = 45.03620845401516\n",
      "Iteration 501: w = [1.99999999 2.99999999], b = 2.999999990089769, loss = 87.26027237411182\n",
      "Iteration 502: w = [1.99999999 2.99999999], b = 2.9999999900825673, loss = 12.967098298860696\n",
      "Iteration 503: w = [1.99999999 2.99999999], b = 2.999999990079373, loss = 2.5508996146638756\n",
      "Iteration 504: w = [1.99999999 2.99999999], b = 2.9999999900665446, loss = 41.14170754013272\n",
      "Iteration 505: w = [1.99999999 2.99999999], b = 2.9999999900574683, loss = 20.593958377401222\n",
      "Iteration 506: w = [1.99999999 2.99999999], b = 2.9999999900320886, loss = 161.03419766297117\n",
      "Iteration 507: w = [1.99999999 2.99999999], b = 2.9999999900310104, loss = 0.29058901036442536\n",
      "Iteration 508: w = [1.99999999 2.99999999], b = 2.9999999900132597, loss = 78.77157268036675\n",
      "Iteration 509: w = [1.99999999 2.99999999], b = 2.999999990009619, loss = 3.3135866777359713\n",
      "Iteration 510: w = [1.99999999 2.99999999], b = 2.99999998999864, loss = 30.135523010667733\n",
      "Iteration 511: w = [1.99999999 2.99999999], b = 2.9999999900014886, loss = 2.028793678596562\n",
      "Iteration 512: w = [1.99999999 2.99999999], b = 2.9999999899987992, loss = 1.8080723131093237\n",
      "Iteration 513: w = [1.99999999 2.99999999], b = 2.999999989994047, loss = 5.645756309097093\n",
      "Iteration 514: w = [1.99999999 2.99999999], b = 2.999999989991599, loss = 1.4984347610425937\n",
      "Iteration 515: w = [1.99999999 2.99999999], b = 2.9999999899888765, loss = 1.852410577044801\n",
      "Iteration 516: w = [1.99999999 2.99999999], b = 2.999999989987491, loss = 0.47999229492387396\n",
      "Iteration 517: w = [1.99999999 2.99999999], b = 2.999999989975086, loss = 38.46815134216139\n",
      "Iteration 518: w = [1.99999999 2.99999999], b = 2.9999999899769114, loss = 0.8326715432381742\n",
      "Iteration 519: w = [1.99999999 2.99999999], b = 2.999999989971624, loss = 6.988889679610596\n",
      "Iteration 520: w = [1.99999999 2.99999999], b = 2.999999989962175, loss = 22.319313936208694\n",
      "Iteration 521: w = [1.99999999 2.99999999], b = 2.999999989964654, loss = 1.536094373890034\n",
      "Iteration 522: w = [1.99999999 2.99999999], b = 2.9999999899518084, loss = 41.25203022590204\n",
      "Iteration 523: w = [1.99999999 2.99999999], b = 2.99999998993498, loss = 70.7964778376733\n",
      "Iteration 524: w = [1.99999999 2.99999999], b = 2.9999999899308643, loss = 4.235286747840613\n",
      "Iteration 525: w = [1.99999999 2.99999999], b = 2.999999989930713, loss = 0.005739538287902958\n",
      "Iteration 526: w = [1.99999999 2.99999999], b = 2.999999989933884, loss = 2.5142811112974472\n",
      "Iteration 527: w = [1.99999999 2.99999999], b = 2.999999989914807, loss = 90.9861197525048\n",
      "Iteration 528: w = [1.99999999 2.99999999], b = 2.999999989916182, loss = 0.4729604516090539\n",
      "Iteration 529: w = [1.99999999 2.99999999], b = 2.999999989921312, loss = 6.578084824660314\n",
      "Iteration 530: w = [1.99999999 2.99999999], b = 2.999999989913334, loss = 15.912376996120559\n",
      "Iteration 531: w = [1.99999999 2.99999999], b = 2.9999999899124554, loss = 0.19290633271699614\n",
      "Iteration 532: w = [1.99999999 2.99999999], b = 2.999999989893023, loss = 94.40447756428736\n",
      "Iteration 533: w = [1.99999999 2.99999999], b = 2.999999989893775, loss = 0.1413533095313184\n",
      "Iteration 534: w = [1.99999999 2.99999999], b = 2.9999999898919425, loss = 0.8392254416352845\n",
      "Iteration 535: w = [1.99999999 2.99999999], b = 2.999999989877129, loss = 54.85911586678789\n",
      "Iteration 536: w = [1.99999999 2.99999999], b = 2.999999989870216, loss = 11.948470988995947\n",
      "Iteration 537: w = [1.99999999 2.99999999], b = 2.9999999898721113, loss = 0.8979413589426173\n",
      "Iteration 538: w = [1.99999999 2.99999999], b = 2.999999989873901, loss = 0.80065541657459\n",
      "Iteration 539: w = [1.99999999 2.99999999], b = 2.9999999898789786, loss = 6.445887586069679\n",
      "Iteration 540: w = [1.99999999 2.99999999], b = 2.999999989883375, loss = 4.831969517733316\n",
      "Iteration 541: w = [1.99999999 2.99999999], b = 2.9999999898853202, loss = 0.9456516831306783\n",
      "Iteration 542: w = [1.99999999 2.99999999], b = 2.99999998988856, loss = 2.623745715023944\n",
      "Iteration 543: w = [1.99999999 2.99999999], b = 2.999999989890938, loss = 1.4140418402217398\n",
      "Iteration 544: w = [1.99999999 2.99999999], b = 2.9999999898912058, loss = 0.017906445599940434\n",
      "Iteration 545: w = [1.99999999 2.99999999], b = 2.999999989886396, loss = 5.784115998937896\n",
      "Iteration 546: w = [1.99999999 2.99999999], b = 2.9999999898851275, loss = 0.40217743475006157\n",
      "Iteration 547: w = [1.99999999 2.99999999], b = 2.9999999898898007, loss = 5.45971854235931\n",
      "Iteration 548: w = [1.99999999 2.99999999], b = 2.999999989879813, loss = 24.938471481450783\n",
      "Iteration 549: w = [1.99999999 2.99999999], b = 2.9999999898658953, loss = 48.42631586775221\n",
      "Iteration 550: w = [1.99999999 2.99999999], b = 2.999999989847094, loss = 88.37311194896542\n",
      "Iteration 551: w = [1.99999999 2.99999999], b = 2.9999999898431207, loss = 3.947087378923782\n",
      "Iteration 552: w = [1.99999999 2.99999999], b = 2.9999999898347087, loss = 17.690587100060757\n",
      "Iteration 553: w = [1.99999999 2.99999999], b = 2.9999999898384617, loss = 3.5211505118125053\n",
      "Iteration 554: w = [1.99999999 2.99999999], b = 2.999999989832548, loss = 8.743928881093659\n",
      "Iteration 555: w = [1.99999999 2.99999999], b = 2.99999998980937, loss = 134.30390797445926\n",
      "Iteration 556: w = [1.99999999 2.99999999], b = 2.999999989798533, loss = 29.358858696359736\n",
      "Iteration 557: w = [1.99999999 2.99999999], b = 2.9999999897824225, loss = 64.88970104464408\n",
      "Iteration 558: w = [1.99999999 2.99999999], b = 2.9999999897844507, loss = 1.0285173903760179\n",
      "Iteration 559: w = [1.99999999 2.99999999], b = 2.99999998976949, loss = 55.956462444180545\n",
      "Iteration 560: w = [1.99999999 2.99999999], b = 2.9999999897650724, loss = 4.8785281760874275\n",
      "Iteration 561: w = [1.99999999 2.99999999], b = 2.9999999897551115, loss = 24.80485734405691\n",
      "Iteration 562: w = [1.99999999 2.99999999], b = 2.9999999897565695, loss = 0.5314622033372742\n",
      "Iteration 563: w = [1.99999999 2.99999999], b = 2.9999999897555116, loss = 0.2798496225824568\n",
      "Iteration 564: w = [1.99999999 2.99999999], b = 2.9999999897572955, loss = 0.7954782221221074\n",
      "Iteration 565: w = [1.99999999 2.99999999], b = 2.9999999897602416, loss = 2.1698570104424095\n",
      "Iteration 566: w = [1.99999999 2.99999999], b = 2.9999999897495773, loss = 28.431874406835068\n",
      "Iteration 567: w = [1.99999999 2.99999999], b = 2.9999999897434284, loss = 9.451894855502916\n",
      "Iteration 568: w = [1.99999999 2.99999999], b = 2.9999999897473835, loss = 3.910718403923054\n",
      "Iteration 569: w = [1.99999999 2.99999999], b = 2.9999999897468674, loss = 0.06653108075473876\n",
      "Iteration 570: w = [1.99999999 2.99999999], b = 2.999999989739995, loss = 11.806317087963471\n",
      "Epoch: 4\n",
      "Iteration 1: w = [1.99999999 2.99999999], b = 2.9999999897443246, loss = 4.686395616790207\n",
      "Iteration 2: w = [1.99999999 2.99999999], b = 2.9999999897486043, loss = 4.5788241539962\n",
      "Iteration 3: w = [1.99999999 2.99999999], b = 2.999999989752711, loss = 4.2164242673881445\n",
      "Iteration 4: w = [1.99999999 2.99999999], b = 2.9999999897359584, loss = 70.16426679797453\n",
      "Iteration 5: w = [1.99999999 2.99999999], b = 2.9999999897139036, loss = 121.60332182114577\n",
      "Iteration 6: w = [1.99999999 2.99999999], b = 2.999999989700466, loss = 45.14374837092789\n",
      "Iteration 7: w = [1.99999999 2.99999999], b = 2.999999989680208, loss = 102.59514559310689\n",
      "Iteration 8: w = [1.99999999 2.99999999], b = 2.999999989676587, loss = 3.277521782874445\n",
      "Iteration 9: w = [1.99999999 2.99999999], b = 2.9999999896701084, loss = 10.494218372714565\n",
      "Iteration 10: w = [1.99999999 2.99999999], b = 2.9999999896757186, loss = 7.868642020387924\n",
      "Iteration 11: w = [1.99999999 2.99999999], b = 2.9999999896724456, loss = 2.67827967088907\n",
      "Iteration 12: w = [1.99999999 2.99999999], b = 2.9999999896715264, loss = 0.21130429948263418\n",
      "Iteration 13: w = [1.99999999 2.99999999], b = 2.9999999896518874, loss = 96.42184350946921\n",
      "Iteration 14: w = [1.99999999 2.99999999], b = 2.9999999896374243, loss = 52.29615710430369\n",
      "Iteration 15: w = [1.99999999 2.99999999], b = 2.999999989634281, loss = 2.469764759510574\n",
      "Iteration 16: w = [1.99999999 2.99999999], b = 2.9999999896019447, loss = 261.41094154247907\n",
      "Iteration 17: w = [1.99999999 2.99999999], b = 2.9999999895973097, loss = 5.370240394296735\n",
      "Iteration 18: w = [1.99999999 2.99999999], b = 2.999999989601641, loss = 4.6893601814468004\n",
      "Iteration 19: w = [1.99999999 2.99999999], b = 2.999999989587243, loss = 51.82342529676488\n",
      "Iteration 20: w = [1.99999999 2.99999999], b = 2.9999999895741976, loss = 42.54696687525514\n",
      "Iteration 21: w = [1.99999999 2.99999999], b = 2.999999989572153, loss = 1.0450798793348592\n",
      "Iteration 22: w = [1.99999999 2.99999999], b = 2.999999989567092, loss = 6.403348784588759\n",
      "Iteration 23: w = [1.99999999 2.99999999], b = 2.9999999895660343, loss = 0.27978177517740593\n",
      "Iteration 24: w = [1.99999999 2.99999999], b = 2.9999999895546927, loss = 32.15682141957423\n",
      "Iteration 25: w = [1.99999999 2.99999999], b = 2.999999989555947, loss = 0.3932808528478667\n",
      "Iteration 26: w = [1.99999999 2.99999999], b = 2.9999999895421334, loss = 47.70296448428528\n",
      "Iteration 27: w = [1.99999999 2.99999999], b = 2.9999999895298624, loss = 37.64488165670811\n",
      "Iteration 28: w = [1.99999999 2.99999999], b = 2.999999989521006, loss = 19.60988962483867\n",
      "Iteration 29: w = [1.99999999 2.99999999], b = 2.999999989514231, loss = 11.475741609983022\n",
      "Iteration 30: w = [1.99999999 2.99999999], b = 2.999999989506947, loss = 13.263269414357495\n",
      "Iteration 31: w = [1.99999999 2.99999999], b = 2.99999998950115, loss = 8.400171300715003\n",
      "Iteration 32: w = [1.99999999 2.99999999], b = 2.999999989491768, loss = 22.00579367257407\n",
      "Iteration 33: w = [1.99999999 2.99999999], b = 2.9999999894931895, loss = 0.5050763815891293\n",
      "Iteration 34: w = [1.99999999 2.99999999], b = 2.9999999894855165, loss = 14.719079770270035\n",
      "Iteration 35: w = [1.99999999 2.99999999], b = 2.9999999894755716, loss = 24.726365208515894\n",
      "Iteration 36: w = [1.99999999 2.99999999], b = 2.999999989471465, loss = 4.216077474189751\n",
      "Iteration 37: w = [1.99999999 2.99999999], b = 2.999999989476487, loss = 6.304221257820876\n",
      "Iteration 38: w = [1.99999999 2.99999999], b = 2.9999999894719807, loss = 5.0759390682600065\n",
      "Iteration 39: w = [1.99999999 2.99999999], b = 2.999999989474222, loss = 1.2556771749671476\n",
      "Iteration 40: w = [1.99999999 2.99999999], b = 2.9999999894685505, loss = 8.041466501982379\n",
      "Iteration 41: w = [1.99999999 2.99999999], b = 2.9999999894583365, loss = 26.08105656713306\n",
      "Iteration 42: w = [1.99999999 2.99999999], b = 2.99999998945453, loss = 3.62161899510996\n",
      "Iteration 43: w = [1.99999999 2.99999999], b = 2.999999989446448, loss = 16.328926785345573\n",
      "Iteration 44: w = [1.99999999 2.99999999], b = 2.9999999894495515, loss = 2.4077951260188337\n",
      "Iteration 45: w = [1.99999999 2.99999999], b = 2.9999999894374403, loss = 36.66914352915694\n",
      "Iteration 46: w = [1.99999999 2.99999999], b = 2.9999999894244387, loss = 42.261789628621614\n",
      "Iteration 47: w = [1.99999999 2.99999999], b = 2.9999999894107483, loss = 46.855877941016125\n",
      "Iteration 48: w = [1.99999999 2.99999999], b = 2.9999999894134115, loss = 1.7732224873705602\n",
      "Iteration 49: w = [1.99999999 2.99999999], b = 2.999999989395969, loss = 76.05822105128958\n",
      "Iteration 50: w = [1.99999999 2.99999999], b = 2.9999999893805662, loss = 59.31180342928449\n",
      "Iteration 51: w = [1.99999999 2.99999999], b = 2.9999999893711724, loss = 22.060649380091867\n",
      "Iteration 52: w = [1.99999999 2.99999999], b = 2.9999999893566422, loss = 52.77982542993381\n",
      "Iteration 53: w = [1.99999999 2.99999999], b = 2.999999989346455, loss = 25.94693186052748\n",
      "Iteration 54: w = [1.99999999 2.99999999], b = 2.9999999893460916, loss = 0.0329640797401214\n",
      "Iteration 55: w = [1.99999999 2.99999999], b = 2.999999989341532, loss = 5.1974142197100655\n",
      "Iteration 56: w = [1.99999999 2.99999999], b = 2.9999999893429923, loss = 0.5328616066223286\n",
      "Iteration 57: w = [1.99999999 2.99999999], b = 2.9999999893440124, loss = 0.26003317141631926\n",
      "Iteration 58: w = [1.99999999 2.99999999], b = 2.999999989330777, loss = 43.791309028645976\n",
      "Iteration 59: w = [1.99999999 2.99999999], b = 2.9999999893254534, loss = 7.085238340252498\n",
      "Iteration 60: w = [1.99999999 2.99999999], b = 2.999999989314637, loss = 29.247440368806124\n",
      "Iteration 61: w = [1.99999999 2.99999999], b = 2.999999989301406, loss = 43.765843016344014\n",
      "Iteration 62: w = [1.99999999 2.99999999], b = 2.9999999893008944, loss = 0.06544188585908149\n",
      "Iteration 63: w = [1.99999999 2.99999999], b = 2.9999999893003864, loss = 0.06447846281970888\n",
      "Iteration 64: w = [1.99999999 2.99999999], b = 2.9999999892852873, loss = 56.99578999459592\n",
      "Iteration 65: w = [1.99999999 2.99999999], b = 2.9999999892743583, loss = 29.862122709636118\n",
      "Iteration 66: w = [1.99999999 2.99999999], b = 2.999999989278017, loss = 3.346407357797103\n",
      "Iteration 67: w = [1.99999999 2.99999999], b = 2.9999999892755724, loss = 1.4939343979513777\n",
      "Iteration 68: w = [1.99999999 2.99999999], b = 2.999999989278055, loss = 1.5403939136922258\n",
      "Iteration 69: w = [1.99999999 2.99999999], b = 2.9999999892813323, loss = 2.6849505594606193\n",
      "Iteration 70: w = [1.99999999 2.99999999], b = 2.999999989273964, loss = 13.573706267295991\n",
      "Iteration 71: w = [1.99999999 2.99999999], b = 2.999999989271468, loss = 1.5570005126483475\n",
      "Iteration 72: w = [1.99999999 2.99999999], b = 2.999999989275053, loss = 3.2128218189294184\n",
      "Iteration 73: w = [1.99999999 2.99999999], b = 2.9999999892672107, loss = 15.375023834014673\n",
      "Iteration 74: w = [1.99999999 2.99999999], b = 2.9999999892593197, loss = 15.5667920567593\n",
      "Iteration 75: w = [1.99999999 2.99999999], b = 2.9999999892607927, loss = 0.5425861005441193\n",
      "Iteration 76: w = [1.99999999 2.99999999], b = 2.999999989252748, loss = 16.18003605044649\n",
      "Iteration 77: w = [1.99999999 2.99999999], b = 2.9999999892445732, loss = 16.705937211397988\n",
      "Iteration 78: w = [1.99999999 2.99999999], b = 2.9999999892351275, loss = 22.305517274023213\n",
      "Iteration 79: w = [1.99999999 2.99999999], b = 2.9999999892234652, loss = 34.00077320804686\n",
      "Iteration 80: w = [1.99999999 2.99999999], b = 2.9999999892286517, loss = 6.724941771872359\n",
      "Iteration 81: w = [1.99999999 2.99999999], b = 2.999999989231057, loss = 1.4464040427994629\n",
      "Iteration 82: w = [1.99999999 2.99999999], b = 2.999999989235866, loss = 5.781928051934384\n",
      "Iteration 83: w = [1.99999999 2.99999999], b = 2.9999999892338978, loss = 0.9682716848231507\n",
      "Iteration 84: w = [1.99999999 2.99999999], b = 2.9999999892212226, loss = 40.16532260854428\n",
      "Iteration 85: w = [1.99999999 2.99999999], b = 2.999999989225675, loss = 4.956500654140715\n",
      "Iteration 86: w = [1.99999999 2.99999999], b = 2.9999999892167004, loss = 20.13506329978302\n",
      "Iteration 87: w = [1.99999999 2.99999999], b = 2.9999999892124247, loss = 4.570684605987935\n",
      "Iteration 88: w = [1.99999999 2.99999999], b = 2.9999999892110094, loss = 0.5007456110477966\n",
      "Iteration 89: w = [1.99999999 2.99999999], b = 2.999999989211114, loss = 0.0027522828557322646\n",
      "Iteration 90: w = [1.99999999 2.99999999], b = 2.99999998921614, loss = 6.314283744428659\n",
      "Iteration 91: w = [1.99999999 2.99999999], b = 2.9999999891951195, loss = 110.46574589866299\n",
      "Iteration 92: w = [1.99999999 2.99999999], b = 2.999999989183792, loss = 32.07777840318615\n",
      "Iteration 93: w = [1.99999999 2.99999999], b = 2.999999989174379, loss = 22.150214515735033\n",
      "Iteration 94: w = [1.99999999 2.99999999], b = 2.999999989166546, loss = 15.339389080771717\n",
      "Iteration 95: w = [1.99999999 2.99999999], b = 2.9999999891634217, loss = 2.4402397914193994\n",
      "Iteration 96: w = [1.99999999 2.99999999], b = 2.999999989160314, loss = 2.4143515124281745\n",
      "Iteration 97: w = [1.99999999 2.99999999], b = 2.9999999891469593, loss = 44.58701837387077\n",
      "Iteration 98: w = [1.99999999 2.99999999], b = 2.9999999891467266, loss = 0.013550207772549448\n",
      "Iteration 99: w = [1.99999999 2.99999999], b = 2.999999989141609, loss = 6.5478564225375875\n",
      "Iteration 100: w = [1.99999999 2.99999999], b = 2.9999999891430296, loss = 0.5044356770287167\n",
      "Iteration 101: w = [1.99999999 2.99999999], b = 2.9999999891423053, loss = 0.1311119215968643\n",
      "Iteration 102: w = [1.99999999 2.99999999], b = 2.9999999891436815, loss = 0.4735621822877468\n",
      "Iteration 103: w = [1.99999999 2.99999999], b = 2.9999999891425335, loss = 0.32947056774421885\n",
      "Iteration 104: w = [1.99999999 2.99999999], b = 2.9999999891413736, loss = 0.33639762677569557\n",
      "Iteration 105: w = [1.99999999 2.99999999], b = 2.999999989123209, loss = 82.48656197401415\n",
      "Iteration 106: w = [1.99999999 2.99999999], b = 2.9999999891264033, loss = 2.5508787195825313\n",
      "Iteration 107: w = [1.99999999 2.99999999], b = 2.9999999891152878, loss = 30.889174948699832\n",
      "Iteration 108: w = [1.99999999 2.99999999], b = 2.999999989110371, loss = 6.044471130422686\n",
      "Iteration 109: w = [1.99999999 2.99999999], b = 2.999999989107949, loss = 1.466123733964\n",
      "Iteration 110: w = [1.99999999 2.99999999], b = 2.9999999891125695, loss = 5.336354537298032\n",
      "Iteration 111: w = [1.99999999 2.99999999], b = 2.999999989113728, loss = 0.33560422270715073\n",
      "Iteration 112: w = [1.99999999 2.99999999], b = 2.9999999891041655, loss = 22.86150797379109\n",
      "Iteration 113: w = [1.99999999 2.99999999], b = 2.9999999891040914, loss = 0.0013828849300841048\n",
      "Iteration 114: w = [1.99999999 2.99999999], b = 2.9999999890911067, loss = 42.15017644616753\n",
      "Iteration 115: w = [1.99999999 2.99999999], b = 2.9999999890815587, loss = 22.790883759011482\n",
      "Iteration 116: w = [1.99999999 2.99999999], b = 2.9999999890691673, loss = 38.38620941048619\n",
      "Iteration 117: w = [1.99999999 2.99999999], b = 2.9999999890722098, loss = 2.314150800700049\n",
      "Iteration 118: w = [1.99999999 2.99999999], b = 2.9999999890639653, loss = 16.99287295899739\n",
      "Iteration 119: w = [1.99999999 2.99999999], b = 2.999999989041387, loss = 127.44483713980568\n",
      "Iteration 120: w = [1.99999999 2.99999999], b = 2.9999999890441025, loss = 1.8435114406462412\n",
      "Iteration 121: w = [1.99999999 2.99999999], b = 2.9999999890372275, loss = 11.816059767427706\n",
      "Iteration 122: w = [1.99999999 2.99999999], b = 2.9999999890433586, loss = 9.397161684174442\n",
      "Iteration 123: w = [1.99999999 2.99999999], b = 2.999999989024768, loss = 86.40231938372332\n",
      "Iteration 124: w = [1.99999999 2.99999999], b = 2.999999989022624, loss = 1.149141214744043\n",
      "Iteration 125: w = [1.99999999 2.99999999], b = 2.999999989015587, loss = 12.38046640262274\n",
      "Iteration 126: w = [1.99999999 2.99999999], b = 2.9999999890190945, loss = 3.0752814991720685\n",
      "Iteration 127: w = [1.99999999 2.99999999], b = 2.9999999890115703, loss = 14.153890517272078\n",
      "Iteration 128: w = [1.99999999 2.99999999], b = 2.9999999889995115, loss = 36.35272565401005\n",
      "Iteration 129: w = [1.99999999 2.99999999], b = 2.9999999889917404, loss = 15.096850364417046\n",
      "Iteration 130: w = [1.99999999 2.99999999], b = 2.999999988989961, loss = 0.7915851340788298\n",
      "Iteration 131: w = [1.99999999 2.99999999], b = 2.9999999889947326, loss = 5.691845880248948\n",
      "Iteration 132: w = [1.99999999 2.99999999], b = 2.99999998899947, loss = 5.6109837078085665\n",
      "Iteration 133: w = [1.99999999 2.99999999], b = 2.999999988991681, loss = 15.168855831570001\n",
      "Iteration 134: w = [1.99999999 2.99999999], b = 2.9999999889881233, loss = 3.1640844931528345\n",
      "Iteration 135: w = [1.99999999 2.99999999], b = 2.9999999889888307, loss = 0.12512302567765593\n",
      "Iteration 136: w = [1.99999999 2.99999999], b = 2.9999999889909152, loss = 1.0864188804582984\n",
      "Iteration 137: w = [1.99999999 2.99999999], b = 2.9999999889825255, loss = 17.597155108855087\n",
      "Iteration 138: w = [1.99999999 2.99999999], b = 2.9999999889634044, loss = 91.40637473403585\n",
      "Iteration 139: w = [1.99999999 2.99999999], b = 2.9999999889522817, loss = 30.929615720848698\n",
      "Iteration 140: w = [1.99999999 2.99999999], b = 2.9999999889526103, loss = 0.027016498409309527\n",
      "Iteration 141: w = [1.99999999 2.99999999], b = 2.999999988943717, loss = 19.77191225106517\n",
      "Iteration 142: w = [1.99999999 2.99999999], b = 2.9999999889477396, loss = 4.044858829961337\n",
      "Iteration 143: w = [1.99999999 2.99999999], b = 2.9999999889413753, loss = 10.126040717401136\n",
      "Iteration 144: w = [1.99999999 2.99999999], b = 2.9999999889411737, loss = 0.010152442592212029\n",
      "Iteration 145: w = [1.99999999 2.99999999], b = 2.99999998892489, loss = 66.29181780508111\n",
      "Iteration 146: w = [1.99999999 2.99999999], b = 2.9999999889062097, loss = 87.23551671092443\n",
      "Iteration 147: w = [1.99999999 2.99999999], b = 2.9999999889123616, loss = 9.461169655022925\n",
      "Iteration 148: w = [1.99999999 2.99999999], b = 2.999999988915883, loss = 3.0995428932745264\n",
      "Iteration 149: w = [1.99999999 2.99999999], b = 2.9999999889108238, loss = 6.398669928284706\n",
      "Iteration 150: w = [1.99999999 2.99999999], b = 2.9999999889054534, loss = 7.2106689298661575\n",
      "Iteration 151: w = [1.99999999 2.99999999], b = 2.999999988908709, loss = 2.6498659737201162\n",
      "Iteration 152: w = [1.99999999 2.99999999], b = 2.9999999888918336, loss = 71.19318625890935\n",
      "Iteration 153: w = [1.99999999 2.99999999], b = 2.999999988894945, loss = 2.419822078968719\n",
      "Iteration 154: w = [1.99999999 2.99999999], b = 2.999999988894828, loss = 0.0034110760687065932\n",
      "Iteration 155: w = [1.99999999 2.99999999], b = 2.9999999888774345, loss = 75.63619800549284\n",
      "Iteration 156: w = [1.99999999 2.99999999], b = 2.9999999888663775, loss = 30.565154813595694\n",
      "Iteration 157: w = [1.99999999 2.99999999], b = 2.9999999888546287, loss = 34.50778899321319\n",
      "Iteration 158: w = [1.99999999 2.99999999], b = 2.9999999888507674, loss = 3.7275956798263485\n",
      "Iteration 159: w = [1.99999999 2.99999999], b = 2.9999999888385918, loss = 37.060678134867246\n",
      "Iteration 160: w = [1.99999999 2.99999999], b = 2.999999988824728, loss = 48.04843443172882\n",
      "Iteration 161: w = [1.99999999 2.99999999], b = 2.9999999888178874, loss = 11.699309877940209\n",
      "Iteration 162: w = [1.99999999 2.99999999], b = 2.999999988810976, loss = 11.941012388204136\n",
      "Iteration 163: w = [1.99999999 2.99999999], b = 2.9999999888039426, loss = 12.368069862740743\n",
      "Iteration 164: w = [1.99999999 2.99999999], b = 2.999999988790911, loss = 42.45604155392926\n",
      "Iteration 165: w = [1.99999999 2.99999999], b = 2.999999988780665, loss = 26.242223566212218\n",
      "Iteration 166: w = [1.99999999 2.99999999], b = 2.9999999887770317, loss = 3.3004750071371722\n",
      "Iteration 167: w = [1.99999999 2.99999999], b = 2.999999988776147, loss = 0.195709538153156\n",
      "Iteration 168: w = [1.99999999 2.99999999], b = 2.999999988769758, loss = 10.205734554602246\n",
      "Iteration 169: w = [1.99999999 2.99999999], b = 2.999999988770591, loss = 0.1734895133804318\n",
      "Iteration 170: w = [1.99999999 2.99999999], b = 2.999999988773285, loss = 1.814279053336878\n",
      "Iteration 171: w = [1.99999999 2.99999999], b = 2.9999999887623856, loss = 29.69787113568409\n",
      "Iteration 172: w = [1.99999999 2.99999999], b = 2.999999988763625, loss = 0.3840754361370919\n",
      "Iteration 173: w = [1.99999999 2.99999999], b = 2.9999999887462714, loss = 75.28616710561501\n",
      "Iteration 174: w = [1.99999999 2.99999999], b = 2.9999999887469286, loss = 0.10803601897923643\n",
      "Iteration 175: w = [1.99999999 2.99999999], b = 2.9999999887461093, loss = 0.167902402545236\n",
      "Iteration 176: w = [1.99999999 2.99999999], b = 2.999999988733313, loss = 40.93590582827994\n",
      "Iteration 177: w = [1.99999999 2.99999999], b = 2.9999999887272946, loss = 9.055236148528431\n",
      "Iteration 178: w = [1.99999999 2.99999999], b = 2.999999988726569, loss = 0.131656500914899\n",
      "Iteration 179: w = [1.99999999 2.99999999], b = 2.9999999887076148, loss = 89.81680079581704\n",
      "Iteration 180: w = [1.99999999 2.99999999], b = 2.999999988703473, loss = 4.288156171926733\n",
      "Iteration 181: w = [1.99999999 2.99999999], b = 2.9999999887090296, loss = 7.718523570066446\n",
      "Iteration 182: w = [1.99999999 2.99999999], b = 2.9999999887134376, loss = 4.857615051027277\n",
      "Iteration 183: w = [1.99999999 2.99999999], b = 2.9999999886946527, loss = 88.2203439052864\n",
      "Iteration 184: w = [1.99999999 2.99999999], b = 2.9999999886908553, loss = 3.6046900624237703\n",
      "Iteration 185: w = [1.99999999 2.99999999], b = 2.999999988677483, loss = 44.70671670676189\n",
      "Iteration 186: w = [1.99999999 2.99999999], b = 2.99999998866407, loss = 44.975706197184984\n",
      "Iteration 187: w = [1.99999999 2.99999999], b = 2.999999988664569, loss = 0.06223693783778469\n",
      "Iteration 188: w = [1.99999999 2.99999999], b = 2.999999988642888, loss = 117.51825234757578\n",
      "Iteration 189: w = [1.99999999 2.99999999], b = 2.999999988613789, loss = 211.68531455670603\n",
      "Iteration 190: w = [1.99999999 2.99999999], b = 2.9999999885943063, loss = 94.89236142604229\n",
      "Iteration 191: w = [1.99999999 2.99999999], b = 2.9999999885995527, loss = 6.881471605999519\n",
      "Iteration 192: w = [1.99999999 2.99999999], b = 2.999999988593324, loss = 9.698999089828899\n",
      "Iteration 193: w = [1.99999999 2.99999999], b = 2.999999988588944, loss = 4.796184036108555\n",
      "Iteration 194: w = [1.99999999 2.99999999], b = 2.999999988584841, loss = 4.208363149765022\n",
      "Iteration 195: w = [1.99999999 2.99999999], b = 2.9999999885837805, loss = 0.2810840798222043\n",
      "Iteration 196: w = [1.99999999 2.99999999], b = 2.999999988588067, loss = 4.593336165878874\n",
      "Iteration 197: w = [1.99999999 2.99999999], b = 2.9999999885868873, loss = 0.34773436634481947\n",
      "Iteration 198: w = [1.99999999 2.99999999], b = 2.9999999885585495, loss = 200.7565752813282\n",
      "Iteration 199: w = [1.99999999 2.99999999], b = 2.999999988556014, loss = 1.607755348401823\n",
      "Iteration 200: w = [1.99999999 2.99999999], b = 2.999999988555627, loss = 0.03739565615474644\n",
      "Iteration 201: w = [1.99999999 2.99999999], b = 2.9999999885419655, loss = 46.65802006613084\n",
      "Iteration 202: w = [1.99999999 2.99999999], b = 2.9999999885337654, loss = 16.810046437839414\n",
      "Iteration 203: w = [1.99999999 2.99999999], b = 2.9999999885255946, loss = 16.690000990211068\n",
      "Iteration 204: w = [1.99999999 2.99999999], b = 2.999999988513556, loss = 36.231864342507095\n",
      "Iteration 205: w = [1.99999999 2.99999999], b = 2.9999999885166857, loss = 2.4484912787337185\n",
      "Iteration 206: w = [1.99999999 2.99999999], b = 2.9999999885170276, loss = 0.029225268208236687\n",
      "Iteration 207: w = [1.99999999 2.99999999], b = 2.9999999885073025, loss = 23.64336579437358\n",
      "Iteration 208: w = [1.99999999 2.99999999], b = 2.9999999885027484, loss = 5.184624825746144\n",
      "Iteration 209: w = [1.99999999 2.99999999], b = 2.999999988487403, loss = 58.87114090614881\n",
      "Iteration 210: w = [1.99999999 2.99999999], b = 2.9999999884835433, loss = 3.7239637870916997\n",
      "Iteration 211: w = [1.99999999 2.99999999], b = 2.9999999884790363, loss = 5.077919155598818\n",
      "Iteration 212: w = [1.99999999 2.99999999], b = 2.999999988468118, loss = 29.803247997580975\n",
      "Iteration 213: w = [1.99999999 2.99999999], b = 2.9999999884711808, loss = 2.345440458781633\n",
      "Iteration 214: w = [1.99999999 2.99999999], b = 2.9999999884657864, loss = 7.275201266285243\n",
      "Iteration 215: w = [1.99999999 2.99999999], b = 2.9999999884535398, loss = 37.49569152788619\n",
      "Iteration 216: w = [1.99999999 2.99999999], b = 2.999999988436353, loss = 73.84430132774213\n",
      "Iteration 217: w = [1.99999999 2.99999999], b = 2.9999999884239616, loss = 38.38681452387179\n",
      "Iteration 218: w = [1.99999999 2.99999999], b = 2.999999988426606, loss = 1.7481246622892566\n",
      "Iteration 219: w = [1.99999999 2.99999999], b = 2.9999999884261954, loss = 0.04220183048863986\n",
      "Iteration 220: w = [1.99999999 2.99999999], b = 2.999999988420317, loss = 8.638564485641743\n",
      "Iteration 221: w = [1.99999999 2.99999999], b = 2.999999988408809, loss = 33.110026021178584\n",
      "Iteration 222: w = [1.99999999 2.99999999], b = 2.9999999884008384, loss = 15.881711853275293\n",
      "Iteration 223: w = [1.99999999 2.99999999], b = 2.9999999883838253, loss = 72.36128437082826\n",
      "Iteration 224: w = [1.99999999 2.99999999], b = 2.9999999883811723, loss = 1.7598813286210466\n",
      "Iteration 225: w = [1.99999999 2.99999999], b = 2.9999999883582564, loss = 131.28256248266814\n",
      "Iteration 226: w = [1.99999999 2.99999999], b = 2.9999999883531134, loss = 6.612334155611044\n",
      "Iteration 227: w = [1.99999999 2.99999999], b = 2.999999988342664, loss = 27.298732333127443\n",
      "Iteration 228: w = [1.99999999 2.99999999], b = 2.999999988341226, loss = 0.5168773562248539\n",
      "Iteration 229: w = [1.99999999 2.99999999], b = 2.9999999883327555, loss = 17.93766595289633\n",
      "Iteration 230: w = [1.99999999 2.99999999], b = 2.999999988330771, loss = 0.9845378418255846\n",
      "Iteration 231: w = [1.99999999 2.99999999], b = 2.9999999883294772, loss = 0.4184603564335294\n",
      "Iteration 232: w = [1.99999999 2.99999999], b = 2.9999999883160857, loss = 44.832035097577844\n",
      "Iteration 233: w = [1.99999999 2.99999999], b = 2.9999999883031685, loss = 41.71251545124515\n",
      "Iteration 234: w = [1.99999999 2.99999999], b = 2.9999999882888275, loss = 51.415368701207754\n",
      "Iteration 235: w = [1.99999999 2.99999999], b = 2.999999988286256, loss = 1.653523921780434\n",
      "Iteration 236: w = [1.99999999 2.99999999], b = 2.9999999882718953, loss = 51.55484213021905\n",
      "Iteration 237: w = [1.99999999 2.99999999], b = 2.9999999882724446, loss = 0.07538719840375771\n",
      "Iteration 238: w = [1.99999999 2.99999999], b = 2.9999999882800212, loss = 14.350584852035508\n",
      "Iteration 239: w = [1.99999999 2.99999999], b = 2.9999999882814947, loss = 0.542936827085093\n",
      "Iteration 240: w = [1.99999999 2.99999999], b = 2.9999999882797175, loss = 0.7897310592162304\n",
      "Iteration 241: w = [1.99999999 2.99999999], b = 2.9999999882849275, loss = 6.78653456392714\n",
      "Iteration 242: w = [1.99999999 2.99999999], b = 2.9999999882869908, loss = 1.0641832515002343\n",
      "Iteration 243: w = [1.99999999 2.99999999], b = 2.999999988289933, loss = 2.1636592825622745\n",
      "Iteration 244: w = [1.99999999 2.99999999], b = 2.999999988274194, loss = 61.9278155307304\n",
      "Iteration 245: w = [1.99999999 2.99999999], b = 2.9999999882648125, loss = 22.0027926169234\n",
      "Iteration 246: w = [1.99999999 2.99999999], b = 2.999999988244852, loss = 99.60638603438503\n",
      "Iteration 247: w = [1.99999999 2.99999999], b = 2.999999988246153, loss = 0.4233956697894173\n",
      "Iteration 248: w = [1.99999999 2.99999999], b = 2.9999999882413926, loss = 5.6659855147128955\n",
      "Iteration 249: w = [1.99999999 2.99999999], b = 2.999999988233655, loss = 14.96660892196248\n",
      "Iteration 250: w = [1.99999999 2.99999999], b = 2.9999999882329127, loss = 0.13778427852060135\n",
      "Iteration 251: w = [1.99999999 2.99999999], b = 2.9999999882281143, loss = 5.7560123603258635\n",
      "Iteration 252: w = [1.99999999 2.99999999], b = 2.999999988230919, loss = 1.9669155760157928\n",
      "Iteration 253: w = [1.99999999 2.99999999], b = 2.9999999882312443, loss = 0.026434393377596753\n",
      "Iteration 254: w = [1.99999999 2.99999999], b = 2.999999988223453, loss = 15.17450998340922\n",
      "Iteration 255: w = [1.99999999 2.99999999], b = 2.999999988224887, loss = 0.5141233412775522\n",
      "Iteration 256: w = [1.99999999 2.99999999], b = 2.9999999882148223, loss = 25.325426061960933\n",
      "Iteration 257: w = [1.99999999 2.99999999], b = 2.99999998821351, loss = 0.4303946431429161\n",
      "Iteration 258: w = [1.99999999 2.99999999], b = 2.999999988216123, loss = 1.7070603414870107\n",
      "Iteration 259: w = [1.99999999 2.99999999], b = 2.9999999882154142, loss = 0.12552252434528058\n",
      "Iteration 260: w = [1.99999999 2.99999999], b = 2.9999999882184016, loss = 2.2312805206163833\n",
      "Iteration 261: w = [1.99999999 2.99999999], b = 2.9999999882121897, loss = 9.646997810442867\n",
      "Iteration 262: w = [1.99999999 2.99999999], b = 2.9999999882009187, loss = 31.759425819692865\n",
      "Iteration 263: w = [1.99999999 2.99999999], b = 2.999999988182664, loss = 83.30697312274421\n",
      "Iteration 264: w = [1.99999999 2.99999999], b = 2.9999999881754635, loss = 12.962266023259303\n",
      "Iteration 265: w = [1.99999999 2.99999999], b = 2.999999988173659, loss = 0.8137821739955863\n",
      "Iteration 266: w = [1.99999999 2.99999999], b = 2.999999988178907, loss = 6.884720755013804\n",
      "Iteration 267: w = [1.99999999 2.99999999], b = 2.9999999881814947, loss = 1.6740552500672752\n",
      "Iteration 268: w = [1.99999999 2.99999999], b = 2.999999988178022, loss = 3.0154018932380136\n",
      "Iteration 269: w = [1.99999999 2.99999999], b = 2.9999999881827164, loss = 5.5097058014532525\n",
      "Iteration 270: w = [1.99999999 2.99999999], b = 2.9999999881681574, loss = 52.99202833525096\n",
      "Iteration 271: w = [1.99999999 2.99999999], b = 2.9999999881607335, loss = 13.77871122581834\n",
      "Iteration 272: w = [1.99999999 2.99999999], b = 2.9999999881459374, loss = 54.73004468101012\n",
      "Iteration 273: w = [1.99999999 2.99999999], b = 2.9999999881410204, loss = 6.04364845211398\n",
      "Iteration 274: w = [1.99999999 2.99999999], b = 2.999999988134106, loss = 11.952183416483042\n",
      "Iteration 275: w = [1.99999999 2.99999999], b = 2.9999999881332258, loss = 0.19367277688762605\n",
      "Iteration 276: w = [1.99999999 2.99999999], b = 2.9999999881249604, loss = 17.078884941490006\n",
      "Iteration 277: w = [1.99999999 2.99999999], b = 2.999999988126936, loss = 0.9760232007117338\n",
      "Iteration 278: w = [1.99999999 2.99999999], b = 2.999999988126441, loss = 0.06124301743514507\n",
      "Iteration 279: w = [1.99999999 2.99999999], b = 2.999999988119855, loss = 10.843823020278457\n",
      "Iteration 280: w = [1.99999999 2.99999999], b = 2.999999988096278, loss = 138.97121295288454\n",
      "Iteration 281: w = [1.99999999 2.99999999], b = 2.999999988095421, loss = 0.18358560152615022\n",
      "Iteration 282: w = [1.99999999 2.99999999], b = 2.999999988078953, loss = 67.79527051244615\n",
      "Iteration 283: w = [1.99999999 2.99999999], b = 2.999999988079503, loss = 0.07561921801873858\n",
      "Iteration 284: w = [1.99999999 2.99999999], b = 2.999999988078603, loss = 0.20252858997013143\n",
      "Iteration 285: w = [1.99999999 2.99999999], b = 2.9999999880705235, loss = 16.318034895569085\n",
      "Iteration 286: w = [1.99999999 2.99999999], b = 2.9999999880719788, loss = 0.5294554089740597\n",
      "Iteration 287: w = [1.99999999 2.99999999], b = 2.9999999880626476, loss = 21.768284780217893\n",
      "Iteration 288: w = [1.99999999 2.99999999], b = 2.999999988045372, loss = 74.61204864149745\n",
      "Iteration 289: w = [1.99999999 2.99999999], b = 2.9999999880404236, loss = 6.121607055363616\n",
      "Iteration 290: w = [1.99999999 2.99999999], b = 2.999999988040748, loss = 0.02637469801740585\n",
      "Iteration 291: w = [1.99999999 2.99999999], b = 2.9999999880389203, loss = 0.8352196988124029\n",
      "Iteration 292: w = [1.99999999 2.99999999], b = 2.999999988036558, loss = 1.3946838316922971\n",
      "Iteration 293: w = [1.99999999 2.99999999], b = 2.999999988023833, loss = 40.48267213387745\n",
      "Iteration 294: w = [1.99999999 2.99999999], b = 2.9999999880199386, loss = 3.7909397995014174\n",
      "Iteration 295: w = [1.99999999 2.99999999], b = 2.99999998801323, loss = 11.250344574165226\n",
      "Iteration 296: w = [1.99999999 2.99999999], b = 2.999999988010736, loss = 1.5548569882739436\n",
      "Iteration 297: w = [1.99999999 2.99999999], b = 2.999999988012541, loss = 0.814322965902826\n",
      "Iteration 298: w = [1.99999999 2.99999999], b = 2.9999999879836285, loss = 208.98181070890024\n",
      "Iteration 299: w = [1.99999999 2.99999999], b = 2.999999987964803, loss = 88.59942356022836\n",
      "Iteration 300: w = [1.99999999 2.99999999], b = 2.9999999879632364, loss = 0.6137987850826946\n",
      "Iteration 301: w = [1.99999999 2.99999999], b = 2.999999987959665, loss = 3.1889214481239625\n",
      "Iteration 302: w = [1.99999999 2.99999999], b = 2.999999987963475, loss = 3.628806045918748\n",
      "Iteration 303: w = [1.99999999 2.99999999], b = 2.9999999879563983, loss = 12.519417638513197\n",
      "Iteration 304: w = [1.99999999 2.99999999], b = 2.9999999879389585, loss = 76.03585304887568\n",
      "Iteration 305: w = [1.99999999 2.99999999], b = 2.999999987932276, loss = 11.164966956906213\n",
      "Iteration 306: w = [1.99999999 2.99999999], b = 2.9999999879237813, loss = 18.039960272063595\n",
      "Iteration 307: w = [1.99999999 2.99999999], b = 2.9999999879211154, loss = 1.776613538316384\n",
      "Iteration 308: w = [1.99999999 2.99999999], b = 2.9999999879138866, loss = 13.0647939526299\n",
      "Iteration 309: w = [1.99999999 2.99999999], b = 2.9999999879221835, loss = 17.209575042988504\n",
      "Iteration 310: w = [1.99999999 2.99999999], b = 2.9999999879203574, loss = 0.8334989460948674\n",
      "Iteration 311: w = [1.99999999 2.99999999], b = 2.999999987906777, loss = 46.105640657627724\n",
      "Iteration 312: w = [1.99999999 2.99999999], b = 2.9999999878995305, loss = 13.129050358543806\n",
      "Iteration 313: w = [1.99999999 2.99999999], b = 2.9999999878902446, loss = 21.557600599251728\n",
      "Iteration 314: w = [1.99999999 2.99999999], b = 2.9999999878790913, loss = 31.100088378238212\n",
      "Iteration 315: w = [1.99999999 2.99999999], b = 2.9999999878702996, loss = 19.323443550324107\n",
      "Iteration 316: w = [1.99999999 2.99999999], b = 2.9999999878497676, loss = 105.38910167215184\n",
      "Iteration 317: w = [1.99999999 2.99999999], b = 2.999999987839429, loss = 26.723150169947143\n",
      "Iteration 318: w = [1.99999999 2.99999999], b = 2.999999987838346, loss = 0.2930997052885448\n",
      "Iteration 319: w = [1.99999999 2.99999999], b = 2.9999999878352455, loss = 2.4033613066733217\n",
      "Iteration 320: w = [1.99999999 2.99999999], b = 2.999999987806304, loss = 209.3990036369213\n",
      "Iteration 321: w = [1.99999999 2.99999999], b = 2.999999987807344, loss = 0.2701052189094505\n",
      "Iteration 322: w = [1.99999999 2.99999999], b = 2.9999999877999506, loss = 13.664508848325127\n",
      "Iteration 323: w = [1.99999999 2.99999999], b = 2.9999999877975707, loss = 1.415831173411534\n",
      "Iteration 324: w = [1.99999999 2.99999999], b = 2.99999998778892, loss = 18.709187548984378\n",
      "Iteration 325: w = [1.99999999 2.99999999], b = 2.999999987780563, loss = 17.45951443132819\n",
      "Iteration 326: w = [1.99999999 2.99999999], b = 2.9999999877719645, loss = 18.483217601511125\n",
      "Iteration 327: w = [1.99999999 2.99999999], b = 2.999999987767658, loss = 4.636375311613129\n",
      "Iteration 328: w = [1.99999999 2.99999999], b = 2.9999999877549435, loss = 40.41629029635447\n",
      "Iteration 329: w = [1.99999999 2.99999999], b = 2.999999987757426, loss = 1.5405622124069436\n",
      "Iteration 330: w = [1.99999999 2.99999999], b = 2.9999999877548236, loss = 1.6933238812408482\n",
      "Iteration 331: w = [1.99999999 2.99999999], b = 2.999999987757862, loss = 2.3081366828658583\n",
      "Iteration 332: w = [1.99999999 2.99999999], b = 2.9999999877448458, loss = 42.35669632625349\n",
      "Iteration 333: w = [1.99999999 2.99999999], b = 2.99999998774388, loss = 0.23329214760156253\n",
      "Iteration 334: w = [1.99999999 2.99999999], b = 2.999999987741122, loss = 1.9011285063147114\n",
      "Iteration 335: w = [1.99999999 2.99999999], b = 2.9999999877471986, loss = 9.231125788169134\n",
      "Iteration 336: w = [1.99999999 2.99999999], b = 2.9999999877490975, loss = 0.901380268835999\n",
      "Iteration 337: w = [1.99999999 2.99999998], b = 2.9999999877406744, loss = 17.737630472261376\n",
      "Iteration 338: w = [1.99999999 2.99999998], b = 2.9999999877205643, loss = 101.10641479201225\n",
      "Iteration 339: w = [1.99999999 2.99999998], b = 2.9999999877078354, loss = 40.50703919955754\n",
      "Iteration 340: w = [1.99999999 2.99999998], b = 2.9999999876894354, loss = 84.63790886980371\n",
      "Iteration 341: w = [1.99999999 2.99999998], b = 2.9999999876894443, loss = 1.9042214561586243e-05\n",
      "Iteration 342: w = [1.99999999 2.99999998], b = 2.999999987669745, loss = 97.01758851806024\n",
      "Iteration 343: w = [1.99999999 2.99999998], b = 2.9999999876563184, loss = 45.06886328071033\n",
      "Iteration 344: w = [1.99999999 2.99999998], b = 2.9999999876417243, loss = 53.24846800489122\n",
      "Iteration 345: w = [1.99999999 2.99999998], b = 2.9999999876324868, loss = 21.33286129377162\n",
      "Iteration 346: w = [1.99999999 2.99999998], b = 2.9999999876365004, loss = 4.027295271890798\n",
      "Iteration 347: w = [1.99999999 2.99999998], b = 2.9999999876366332, loss = 0.004409873415650058\n",
      "Iteration 348: w = [1.99999999 2.99999998], b = 2.999999987642529, loss = 8.689822239620339\n",
      "Iteration 349: w = [1.99999999 2.99999998], b = 2.9999999876319112, loss = 28.185191223089667\n",
      "Iteration 350: w = [1.99999999 2.99999998], b = 2.999999987621583, loss = 26.668402268829045\n",
      "Iteration 351: w = [1.99999999 2.99999998], b = 2.999999987607565, loss = 49.126318877319996\n",
      "Iteration 352: w = [1.99999999 2.99999998], b = 2.9999999876036934, loss = 3.747134600277625\n",
      "Iteration 353: w = [1.99999999 2.99999998], b = 2.999999987591122, loss = 39.50883150795865\n",
      "Iteration 354: w = [1.99999999 2.99999998], b = 2.999999987582737, loss = 17.577953100770596\n",
      "Iteration 355: w = [1.99999999 2.99999998], b = 2.999999987568716, loss = 49.14419131405686\n",
      "Iteration 356: w = [1.99999999 2.99999998], b = 2.999999987569611, loss = 0.20011387391901955\n",
      "Iteration 357: w = [1.99999999 2.99999998], b = 2.9999999875547645, loss = 55.10387593666122\n",
      "Iteration 358: w = [1.99999999 2.99999998], b = 2.9999999875576484, loss = 2.079496817059217\n",
      "Iteration 359: w = [1.99999999 2.99999998], b = 2.999999987549231, loss = 17.712748458546322\n",
      "Iteration 360: w = [1.99999999 2.99999998], b = 2.999999987541436, loss = 15.191030563849738\n",
      "Iteration 361: w = [1.99999999 2.99999998], b = 2.999999987529376, loss = 36.3620314296133\n",
      "Iteration 362: w = [1.99999999 2.99999998], b = 2.9999999875361016, loss = 11.30854500015193\n",
      "Iteration 363: w = [1.99999999 2.99999998], b = 2.9999999875288776, loss = 13.047184836753667\n",
      "Iteration 364: w = [1.99999999 2.99999998], b = 2.999999987519752, loss = 20.818129888842368\n",
      "Iteration 365: w = [1.99999999 2.99999998], b = 2.999999987522245, loss = 1.5541192229495238\n",
      "Iteration 366: w = [1.99999999 2.99999998], b = 2.9999999875195935, loss = 1.7579704985841478\n",
      "Iteration 367: w = [1.99999999 2.99999998], b = 2.9999999875160848, loss = 3.077472339719148\n",
      "Iteration 368: w = [1.99999999 2.99999998], b = 2.999999987515942, loss = 0.005087900073621696\n",
      "Iteration 369: w = [1.99999999 2.99999998], b = 2.9999999875182652, loss = 1.3489675192542008\n",
      "Iteration 370: w = [1.99999999 2.99999998], b = 2.999999987496874, loss = 114.39611519754551\n",
      "Iteration 371: w = [1.99999999 2.99999998], b = 2.999999987496212, loss = 0.10946325947822692\n",
      "Iteration 372: w = [1.99999999 2.99999998], b = 2.9999999874873127, loss = 19.800018795346347\n",
      "Iteration 373: w = [1.99999999 2.99999998], b = 2.999999987473293, loss = 49.13684521191769\n",
      "Iteration 374: w = [1.99999999 2.99999998], b = 2.999999987472618, loss = 0.11386681561594865\n",
      "Iteration 375: w = [1.99999999 2.99999998], b = 2.9999999874596868, loss = 41.80605736814211\n",
      "Iteration 376: w = [1.99999999 2.99999998], b = 2.9999999874579766, loss = 0.7311174193625123\n",
      "Iteration 377: w = [1.99999999 2.99999998], b = 2.9999999874573007, loss = 0.11423942234650371\n",
      "Iteration 378: w = [1.99999999 2.99999998], b = 2.9999999874538594, loss = 2.9604151794274776\n",
      "Iteration 379: w = [1.99999999 2.99999998], b = 2.9999999874440104, loss = 24.25146361898117\n",
      "Iteration 380: w = [1.99999999 2.99999998], b = 2.9999999874413605, loss = 1.7556722911743667\n",
      "Iteration 381: w = [1.99999999 2.99999998], b = 2.999999987432853, loss = 18.094847504504976\n",
      "Iteration 382: w = [1.99999999 2.99999998], b = 2.999999987434176, loss = 0.43754010212746286\n",
      "Iteration 383: w = [1.99999999 2.99999998], b = 2.9999999874200522, loss = 49.87162906516822\n",
      "Iteration 384: w = [1.99999999 2.99999998], b = 2.9999999874189798, loss = 0.2876142540812194\n",
      "Iteration 385: w = [1.99999999 2.99999998], b = 2.999999987411632, loss = 13.498706476356064\n",
      "Iteration 386: w = [1.99999999 2.99999998], b = 2.999999987385623, loss = 169.11576428452813\n",
      "Iteration 387: w = [1.99999999 2.99999998], b = 2.999999987376466, loss = 20.963816099184836\n",
      "Iteration 388: w = [1.99999999 2.99999998], b = 2.9999999873598275, loss = 69.20706846049897\n",
      "Iteration 389: w = [1.99999999 2.99999998], b = 2.9999999873634713, loss = 3.319122233013456\n",
      "Iteration 390: w = [1.99999999 2.99999998], b = 2.999999987359673, loss = 3.6066918963115393\n",
      "Iteration 391: w = [1.99999999 2.99999998], b = 2.9999999873616177, loss = 0.9454238601668064\n",
      "Iteration 392: w = [1.99999999 2.99999998], b = 2.9999999873616194, loss = 7.808276392596035e-07\n",
      "Iteration 393: w = [1.99999999 2.99999998], b = 2.999999987352623, loss = 20.233041219148063\n",
      "Iteration 394: w = [1.99999999 2.99999998], b = 2.9999999873366177, loss = 64.04312004999767\n",
      "Iteration 395: w = [1.99999999 2.99999998], b = 2.9999999873228456, loss = 47.41848884787704\n",
      "Iteration 396: w = [1.99999999 2.99999998], b = 2.999999987308649, loss = 50.38544530617097\n",
      "Iteration 397: w = [1.99999999 2.99999998], b = 2.999999987300362, loss = 17.166684519998444\n",
      "Iteration 398: w = [1.99999999 2.99999998], b = 2.9999999872841103, loss = 66.0320916654387\n",
      "Iteration 399: w = [1.99999999 2.99999998], b = 2.9999999872775494, loss = 10.761718918402376\n",
      "Iteration 400: w = [1.99999999 2.99999998], b = 2.9999999872773557, loss = 0.009367263873502955\n",
      "Iteration 401: w = [1.99999999 2.99999998], b = 2.999999987265768, loss = 33.56798639969926\n",
      "Iteration 402: w = [1.99999999 2.99999998], b = 2.999999987260066, loss = 8.128506430421009\n",
      "Iteration 403: w = [1.99999999 2.99999998], b = 2.9999999872531022, loss = 12.123064123734368\n",
      "Iteration 404: w = [1.99999999 2.99999998], b = 2.999999987247531, loss = 7.759141936217863\n",
      "Iteration 405: w = [1.99999999 2.99999998], b = 2.9999999872293666, loss = 82.4884368472179\n",
      "Iteration 406: w = [1.99999999 2.99999998], b = 2.99999998723054, loss = 0.34426727539047824\n",
      "Iteration 407: w = [1.99999999 2.99999998], b = 2.999999987233542, loss = 2.253314100665969\n",
      "Iteration 408: w = [1.99999999 2.99999998], b = 2.99999998721696, loss = 68.74062838733067\n",
      "Iteration 409: w = [1.99999999 2.99999998], b = 2.999999987202961, loss = 48.991770017740436\n",
      "Iteration 410: w = [1.99999999 2.99999998], b = 2.999999987197711, loss = 6.890996916183075\n",
      "Iteration 411: w = [1.99999999 2.99999998], b = 2.9999999871922816, loss = 7.369864080165083\n",
      "Iteration 412: w = [1.99999999 2.99999998], b = 2.999999987174839, loss = 76.05839189231196\n",
      "Iteration 413: w = [1.99999999 2.99999998], b = 2.999999987180901, loss = 9.186766564353588\n",
      "Iteration 414: w = [1.99999999 2.99999998], b = 2.999999987166316, loss = 53.180492528605036\n",
      "Iteration 415: w = [1.99999999 2.99999998], b = 2.9999999871599745, loss = 10.053754343591281\n",
      "Iteration 416: w = [1.99999999 2.99999998], b = 2.999999987162367, loss = 1.4306915364302994\n",
      "Iteration 417: w = [1.99999999 2.99999998], b = 2.999999987165839, loss = 3.014268245944796\n",
      "Iteration 418: w = [1.99999999 2.99999998], b = 2.999999987149667, loss = 65.38149055518187\n",
      "Iteration 419: w = [1.99999999 2.99999998], b = 2.999999987135035, loss = 53.52589339159875\n",
      "Iteration 420: w = [1.99999999 2.99999998], b = 2.9999999871409386, loss = 8.71333634624506\n",
      "Iteration 421: w = [1.99999999 2.99999998], b = 2.9999999871449985, loss = 4.120804054835405\n",
      "Iteration 422: w = [1.99999999 2.99999998], b = 2.99999998714269, loss = 1.3320513638471652\n",
      "Iteration 423: w = [1.99999999 2.99999998], b = 2.9999999871433873, loss = 0.12147998652081327\n",
      "Iteration 424: w = [1.99999999 2.99999998], b = 2.999999987136489, loss = 11.89786973914479\n",
      "Iteration 425: w = [1.99999999 2.99999998], b = 2.9999999871328855, loss = 3.245699564426567\n",
      "Iteration 426: w = [1.99999999 2.99999998], b = 2.99999998711719, loss = 61.58766629958058\n",
      "Iteration 427: w = [1.99999999 2.99999998], b = 2.999999987103522, loss = 46.70604315538597\n",
      "Iteration 428: w = [1.99999999 2.99999998], b = 2.9999999870973717, loss = 9.456275728730908\n",
      "Iteration 429: w = [1.99999999 2.99999998], b = 2.9999999870990237, loss = 0.682433178818649\n",
      "Iteration 430: w = [1.99999999 2.99999998], b = 2.999999987095199, loss = 3.6563770864661107\n",
      "Iteration 431: w = [1.99999999 2.99999998], b = 2.9999999870889584, loss = 9.73691485459842\n",
      "Iteration 432: w = [1.99999999 2.99999998], b = 2.9999999870841916, loss = 5.680441020590862\n",
      "Iteration 433: w = [1.99999999 2.99999998], b = 2.999999987064833, loss = 93.68872333073568\n",
      "Iteration 434: w = [1.99999999 2.99999998], b = 2.999999987068423, loss = 3.2220849078830756\n",
      "Iteration 435: w = [1.99999999 2.99999998], b = 2.9999999870733585, loss = 6.090311539956148\n",
      "Iteration 436: w = [1.99999999 2.99999998], b = 2.999999987070076, loss = 2.693286917048143\n",
      "Iteration 437: w = [1.99999999 2.99999998], b = 2.9999999870630103, loss = 12.482369837733199\n",
      "Iteration 438: w = [1.99999999 2.99999998], b = 2.99999998706835, loss = 7.128108037990734\n",
      "Iteration 439: w = [1.99999999 2.99999998], b = 2.9999999870584824, loss = 24.34250721614985\n",
      "Iteration 440: w = [1.99999999 2.99999998], b = 2.999999987063806, loss = 7.0851484534220095\n",
      "Iteration 441: w = [1.99999999 2.99999998], b = 2.999999987065274, loss = 0.5384573217029021\n",
      "Iteration 442: w = [1.99999999 2.99999998], b = 2.999999987056601, loss = 18.803286147538216\n",
      "Iteration 443: w = [1.99999999 2.99999998], b = 2.9999999870526057, loss = 3.9913358478374756\n",
      "Iteration 444: w = [1.99999999 2.99999998], b = 2.99999998704917, loss = 2.9511171149603226\n",
      "Iteration 445: w = [1.99999999 2.99999998], b = 2.9999999870452183, loss = 3.9032680410273533\n",
      "Iteration 446: w = [1.99999999 2.99999998], b = 2.9999999870424476, loss = 1.9193499362973856\n",
      "Iteration 447: w = [1.99999999 2.99999998], b = 2.9999999870375356, loss = 6.031853875198825\n",
      "Iteration 448: w = [1.99999999 2.99999998], b = 2.999999987039148, loss = 0.6501268815353439\n",
      "Iteration 449: w = [1.99999999 2.99999998], b = 2.9999999870390086, loss = 0.004853476482493715\n",
      "Iteration 450: w = [1.99999999 2.99999998], b = 2.9999999870311256, loss = 15.535509701160892\n",
      "Iteration 451: w = [1.99999999 2.99999998], b = 2.999999987021598, loss = 22.693478804344785\n",
      "Iteration 452: w = [1.99999999 2.99999998], b = 2.999999987018638, loss = 2.191077852365072\n",
      "Iteration 453: w = [1.99999999 2.99999998], b = 2.9999999870047396, loss = 48.291150600309784\n",
      "Iteration 454: w = [1.99999999 2.99999998], b = 2.9999999870087306, loss = 3.98167107408276\n",
      "Iteration 455: w = [1.99999999 2.99999998], b = 2.999999987014674, loss = 8.83052127912963\n",
      "Iteration 456: w = [1.99999999 2.99999998], b = 2.9999999870129703, loss = 0.7253502687391812\n",
      "Iteration 457: w = [1.99999999 2.99999998], b = 2.9999999869962393, loss = 69.98239672500902\n",
      "Iteration 458: w = [1.99999999 2.99999998], b = 2.9999999869929517, loss = 2.702321869441383\n",
      "Iteration 459: w = [1.99999999 2.99999998], b = 2.9999999869732443, loss = 97.0966309446235\n",
      "Iteration 460: w = [1.99999999 2.99999998], b = 2.999999986962495, loss = 28.88667668389381\n",
      "Iteration 461: w = [1.99999999 2.99999998], b = 2.999999986948638, loss = 48.00811532350653\n",
      "Iteration 462: w = [1.99999999 2.99999998], b = 2.9999999869292524, loss = 93.94725721360633\n",
      "Iteration 463: w = [1.99999999 2.99999998], b = 2.999999986921472, loss = 15.134646617249539\n",
      "Iteration 464: w = [1.99999999 2.99999998], b = 2.9999999869079885, loss = 45.450268007044144\n",
      "Iteration 465: w = [1.99999999 2.99999998], b = 2.999999986893926, loss = 49.43977467213655\n",
      "Iteration 466: w = [1.99999999 2.99999998], b = 2.9999999868952862, loss = 0.4624342603887395\n",
      "Iteration 467: w = [1.99999999 2.99999998], b = 2.999999986893921, loss = 0.4658127139266594\n",
      "Iteration 468: w = [1.99999999 2.99999998], b = 2.999999986889188, loss = 5.60011768643418\n",
      "Iteration 469: w = [1.99999999 2.99999998], b = 2.9999999868945775, loss = 7.261777856826942\n",
      "Iteration 470: w = [1.99999999 2.99999998], b = 2.999999986896372, loss = 0.8052977378186232\n",
      "Iteration 471: w = [1.99999999 2.99999998], b = 2.9999999868875413, loss = 19.494713931526235\n",
      "Iteration 472: w = [1.99999999 2.99999998], b = 2.999999986864087, loss = 137.52558274743345\n",
      "Iteration 473: w = [1.99999999 2.99999998], b = 2.999999986860497, loss = 3.2216587903604372\n",
      "Iteration 474: w = [1.99999999 2.99999998], b = 2.999999986847749, loss = 40.62784497428668\n",
      "Iteration 475: w = [1.99999999 2.99999998], b = 2.9999999868293137, loss = 84.96699547531277\n",
      "Iteration 476: w = [1.99999999 2.99999998], b = 2.9999999868296428, loss = 0.027063851735117116\n",
      "Iteration 477: w = [1.99999999 2.99999998], b = 2.9999999868215435, loss = 16.400476333485408\n",
      "Iteration 478: w = [1.99999999 2.99999998], b = 2.9999999868104283, loss = 30.88518200204189\n",
      "Iteration 479: w = [1.99999999 2.99999998], b = 2.9999999868107547, loss = 0.02665060061968327\n",
      "Iteration 480: w = [1.99999999 2.99999998], b = 2.9999999868171403, loss = 10.194322082092896\n",
      "Iteration 481: w = [1.99999999 2.99999998], b = 2.9999999868154195, loss = 0.7403022030571039\n",
      "Iteration 482: w = [1.99999999 2.99999998], b = 2.9999999868070004, loss = 17.72070993836528\n",
      "Iteration 483: w = [1.99999999 2.99999998], b = 2.9999999868018183, loss = 6.713222521709659\n",
      "Iteration 484: w = [1.99999999 2.99999998], b = 2.9999999868042373, loss = 1.4629582357881876\n",
      "Iteration 485: w = [1.99999999 2.99999998], b = 2.9999999867768277, loss = 187.82294663120507\n",
      "Iteration 486: w = [1.99999999 2.99999998], b = 2.999999986758219, loss = 86.57177359991948\n",
      "Iteration 487: w = [1.99999999 2.99999998], b = 2.9999999867636413, loss = 7.350316743416308\n",
      "Iteration 488: w = [1.99999999 2.99999998], b = 2.9999999867477185, loss = 63.38263494301066\n",
      "Iteration 489: w = [1.99999999 2.99999998], b = 2.999999986730367, loss = 75.26920188598282\n",
      "Iteration 490: w = [1.99999999 2.99999998], b = 2.9999999867138882, loss = 67.88897885887371\n",
      "Iteration 491: w = [1.99999999 2.99999998], b = 2.999999986708684, loss = 6.771026730576751\n",
      "Iteration 492: w = [1.99999999 2.99999998], b = 2.999999986710944, loss = 1.2770454304801069\n",
      "Iteration 493: w = [1.99999999 2.99999998], b = 2.999999986709845, loss = 0.3019067625580863\n",
      "Iteration 494: w = [1.99999999 2.99999998], b = 2.999999986708719, loss = 0.31682126788527404\n",
      "Iteration 495: w = [1.99999999 2.99999998], b = 2.999999986696562, loss = 36.94647149012609\n",
      "Iteration 496: w = [1.99999999 2.99999998], b = 2.9999999866985387, loss = 0.9765614121956196\n",
      "Iteration 497: w = [1.99999999 2.99999998], b = 2.999999986703065, loss = 5.121787618550957\n",
      "Iteration 498: w = [1.99999999 2.99999998], b = 2.999999986700174, loss = 2.089634691411085\n",
      "Iteration 499: w = [1.99999999 2.99999998], b = 2.999999986701872, loss = 0.7208491952730495\n",
      "Iteration 500: w = [1.99999999 2.99999998], b = 2.9999999866884504, loss = 45.036208303967804\n",
      "Iteration 501: w = [1.99999999 2.99999998], b = 2.9999999866697675, loss = 87.26027210784551\n",
      "Iteration 502: w = [1.99999999 2.99999998], b = 2.9999999866625657, loss = 12.967098261206427\n",
      "Iteration 503: w = [1.99999999 2.99999998], b = 2.9999999866593714, loss = 2.5508996087194875\n",
      "Iteration 504: w = [1.99999999 2.99999998], b = 2.999999986646543, loss = 41.14170743315696\n",
      "Iteration 505: w = [1.99999999 2.99999998], b = 2.9999999866374667, loss = 20.593958324493748\n",
      "Iteration 506: w = [1.99999999 2.99999998], b = 2.999999986612087, loss = 161.03419723400225\n",
      "Iteration 507: w = [1.99999999 2.99999998], b = 2.9999999866110088, loss = 0.2905890111307434\n",
      "Iteration 508: w = [1.99999999 2.99999998], b = 2.999999986593258, loss = 78.77157243105302\n",
      "Iteration 509: w = [1.99999999 2.99999998], b = 2.9999999865896174, loss = 3.3135866678747896\n",
      "Iteration 510: w = [1.99999999 2.99999998], b = 2.999999986578638, loss = 30.13552290936962\n",
      "Iteration 511: w = [1.99999999 2.99999998], b = 2.999999986581487, loss = 2.028793661614324\n",
      "Iteration 512: w = [1.99999999 2.99999998], b = 2.9999999865787976, loss = 1.808072305850609\n",
      "Iteration 513: w = [1.99999999 2.99999998], b = 2.9999999865740454, loss = 5.645756297092374\n",
      "Iteration 514: w = [1.99999999 2.99999998], b = 2.999999986571597, loss = 1.4984347624246328\n",
      "Iteration 515: w = [1.99999999 2.99999998], b = 2.999999986568875, loss = 1.8524105712588113\n",
      "Iteration 516: w = [1.99999999 2.99999998], b = 2.9999999865674893, loss = 0.47999229357153467\n",
      "Iteration 517: w = [1.99999999 2.99999998], b = 2.9999999865550846, loss = 38.468151234801454\n",
      "Iteration 518: w = [1.99999999 2.99999998], b = 2.99999998655691, loss = 0.8326715366334763\n",
      "Iteration 519: w = [1.99999999 2.99999998], b = 2.9999999865516225, loss = 6.988889667675758\n",
      "Iteration 520: w = [1.99999999 2.99999998], b = 2.9999999865421736, loss = 22.319313886953648\n",
      "Iteration 521: w = [1.99999999 2.99999998], b = 2.9999999865446525, loss = 1.5360943662033886\n",
      "Iteration 522: w = [1.99999999 2.99999998], b = 2.9999999865318068, loss = 41.25203010637773\n",
      "Iteration 523: w = [1.99999999 2.99999998], b = 2.9999999865149785, loss = 70.79647766683819\n",
      "Iteration 524: w = [1.99999999 2.99999998], b = 2.9999999865108626, loss = 4.235286740809369\n",
      "Iteration 525: w = [1.99999999 2.99999998], b = 2.999999986510711, loss = 0.005739538861946599\n",
      "Iteration 526: w = [1.99999999 2.99999998], b = 2.9999999865138824, loss = 2.5142810993209697\n",
      "Iteration 527: w = [1.99999999 2.99999998], b = 2.9999999864948053, loss = 90.98611946799514\n",
      "Iteration 528: w = [1.99999999 2.99999998], b = 2.9999999864961806, loss = 0.4729604444767615\n",
      "Iteration 529: w = [1.99999999 2.99999998], b = 2.9999999865013103, loss = 6.578084790774043\n",
      "Iteration 530: w = [1.99999999 2.99999998], b = 2.999999986493332, loss = 15.912376932814986\n",
      "Iteration 531: w = [1.99999999 2.99999998], b = 2.999999986492454, loss = 0.1929063316581536\n",
      "Iteration 532: w = [1.99999999 2.99999998], b = 2.9999999864730214, loss = 94.40447727717577\n",
      "Iteration 533: w = [1.99999999 2.99999998], b = 2.999999986473773, loss = 0.14135330716969108\n",
      "Iteration 534: w = [1.99999999 2.99999998], b = 2.999999986471941, loss = 0.8392254471197814\n",
      "Iteration 535: w = [1.99999999 2.99999998], b = 2.9999999864571274, loss = 54.85911569060689\n",
      "Iteration 536: w = [1.99999999 2.99999998], b = 2.9999999864502143, loss = 11.948470971795672\n",
      "Iteration 537: w = [1.99999999 2.99999998], b = 2.9999999864521096, loss = 0.8979413512288997\n",
      "Iteration 538: w = [1.99999999 2.99999998], b = 2.9999999864538993, loss = 0.8006554099460341\n",
      "Iteration 539: w = [1.99999999 2.99999998], b = 2.999999986458977, loss = 6.445887548783779\n",
      "Iteration 540: w = [1.99999999 2.99999998], b = 2.9999999864633735, loss = 4.831969486755625\n",
      "Iteration 541: w = [1.99999999 2.99999998], b = 2.9999999864653186, loss = 0.9456516785669339\n",
      "Iteration 542: w = [1.99999999 2.99999998], b = 2.9999999864685583, loss = 2.6237456988148025\n",
      "Iteration 543: w = [1.99999999 2.99999998], b = 2.9999999864709364, loss = 1.414041831003491\n",
      "Iteration 544: w = [1.99999999 2.99999998], b = 2.999999986471204, loss = 0.017906445351253516\n",
      "Iteration 545: w = [1.99999999 2.99999998], b = 2.999999986466394, loss = 5.7841159824471395\n",
      "Iteration 546: w = [1.99999999 2.99999998], b = 2.999999986465126, loss = 0.40217743191804195\n",
      "Iteration 547: w = [1.99999999 2.99999998], b = 2.999999986469799, loss = 5.4597185163038775\n",
      "Iteration 548: w = [1.99999999 2.99999998], b = 2.9999999864598115, loss = 24.938471415641548\n",
      "Iteration 549: w = [1.99999999 2.99999998], b = 2.9999999864458937, loss = 48.42631574172065\n",
      "Iteration 550: w = [1.99999999 2.99999998], b = 2.9999999864270923, loss = 88.37311167542194\n",
      "Iteration 551: w = [1.99999999 2.99999998], b = 2.999999986423119, loss = 3.947087373347096\n",
      "Iteration 552: w = [1.99999999 2.99999998], b = 2.999999986414707, loss = 17.690587045010727\n",
      "Iteration 553: w = [1.99999999 2.99999998], b = 2.99999998641846, loss = 3.521150490833675\n",
      "Iteration 554: w = [1.99999999 2.99999998], b = 2.999999986412546, loss = 8.743928872657762\n",
      "Iteration 555: w = [1.99999999 2.99999998], b = 2.9999999863893683, loss = 134.30390753713402\n",
      "Iteration 556: w = [1.99999999 2.99999998], b = 2.9999999863785316, loss = 29.358858631905022\n",
      "Iteration 557: w = [1.99999999 2.99999998], b = 2.999999986362421, loss = 64.88970084769433\n",
      "Iteration 558: w = [1.99999999 2.99999998], b = 2.999999986364449, loss = 1.0285173813303063\n",
      "Iteration 559: w = [1.99999999 2.99999998], b = 2.999999986349488, loss = 55.95646226989037\n",
      "Iteration 560: w = [1.99999999 2.99999998], b = 2.999999986345071, loss = 4.878528159329084\n",
      "Iteration 561: w = [1.99999999 2.99999998], b = 2.99999998633511, loss = 24.80485726501887\n",
      "Iteration 562: w = [1.99999999 2.99999998], b = 2.999999986336568, loss = 0.5314621962689114\n",
      "Iteration 563: w = [1.99999999 2.99999998], b = 2.99999998633551, loss = 0.27984962488100334\n",
      "Iteration 564: w = [1.99999999 2.99999998], b = 2.999999986337294, loss = 0.7954782129489646\n",
      "Iteration 565: w = [1.99999999 2.99999998], b = 2.99999998634024, loss = 2.169856993020171\n",
      "Iteration 566: w = [1.99999999 2.99999998], b = 2.9999999863295757, loss = 28.43187431130715\n",
      "Iteration 567: w = [1.99999999 2.99999998], b = 2.999999986323427, loss = 9.451894828306164\n",
      "Iteration 568: w = [1.99999999 2.99999998], b = 2.999999986327382, loss = 3.9107183802526304\n",
      "Iteration 569: w = [1.99999999 2.99999998], b = 2.999999986326866, loss = 0.06653108176614084\n",
      "Iteration 570: w = [1.99999999 2.99999998], b = 2.9999999863199935, loss = 11.80631706137148\n",
      "Epoch: 5\n",
      "Iteration 1: w = [1.99999999 2.99999998], b = 2.999999986324323, loss = 4.686395588773606\n",
      "Iteration 2: w = [1.99999999 2.99999998], b = 2.9999999863286027, loss = 4.578824138770852\n",
      "Iteration 3: w = [1.99999999 2.99999998], b = 2.9999999863327096, loss = 4.216424244509812\n",
      "Iteration 4: w = [1.99999999 2.99999998], b = 2.9999999863159568, loss = 70.16426657152242\n",
      "Iteration 5: w = [1.99999999 2.99999998], b = 2.999999986293902, loss = 121.60332141925905\n",
      "Iteration 6: w = [1.99999999 2.99999998], b = 2.9999999862804643, loss = 45.14374823507422\n",
      "Iteration 7: w = [1.99999999 2.99999998], b = 2.9999999862602063, loss = 102.59514524963693\n",
      "Iteration 8: w = [1.99999999 2.99999998], b = 2.9999999862565856, loss = 3.277521780700504\n",
      "Iteration 9: w = [1.99999999 2.99999998], b = 2.9999999862501068, loss = 10.494218337039664\n",
      "Iteration 10: w = [1.99999999 2.99999998], b = 2.999999986255717, loss = 7.868641976907958\n",
      "Iteration 11: w = [1.99999999 2.99999998], b = 2.999999986252444, loss = 2.678279671908827\n",
      "Iteration 12: w = [1.99999999 2.99999998], b = 2.9999999862515248, loss = 0.21130430138128933\n",
      "Iteration 13: w = [1.99999999 2.99999998], b = 2.999999986231886, loss = 96.421843201994\n",
      "Iteration 14: w = [1.99999999 2.99999998], b = 2.9999999862174227, loss = 52.29615696444363\n",
      "Iteration 15: w = [1.99999999 2.99999998], b = 2.9999999862142794, loss = 2.469764758025288\n",
      "Iteration 16: w = [1.99999999 2.99999998], b = 2.999999986181943, loss = 261.41094074896614\n",
      "Iteration 17: w = [1.99999999 2.99999998], b = 2.999999986177308, loss = 5.370240378580044\n",
      "Iteration 18: w = [1.99999999 2.99999998], b = 2.9999999861816393, loss = 4.689360157648326\n",
      "Iteration 19: w = [1.99999999 2.99999998], b = 2.9999999861672415, loss = 51.823425130952955\n",
      "Iteration 20: w = [1.99999999 2.99999998], b = 2.999999986154196, loss = 42.546966753860254\n",
      "Iteration 21: w = [1.99999999 2.99999998], b = 2.9999999861521514, loss = 1.0450798784881297\n",
      "Iteration 22: w = [1.99999999 2.99999998], b = 2.9999999861470905, loss = 6.4033487556927655\n",
      "Iteration 23: w = [1.99999999 2.99999998], b = 2.9999999861460327, loss = 0.27978177702063906\n",
      "Iteration 24: w = [1.99999999 2.99999998], b = 2.999999986134691, loss = 32.15682135865538\n",
      "Iteration 25: w = [1.99999999 2.99999998], b = 2.999999986135945, loss = 0.39328084966692606\n",
      "Iteration 26: w = [1.99999999 2.99999998], b = 2.999999986122132, loss = 47.70296431530301\n",
      "Iteration 27: w = [1.99999999 2.99999998], b = 2.9999999861098607, loss = 37.64488158773961\n",
      "Iteration 28: w = [1.99999999 2.99999998], b = 2.9999999861010043, loss = 19.6098895773788\n",
      "Iteration 29: w = [1.99999999 2.99999998], b = 2.9999999860942292, loss = 11.475741578605748\n",
      "Iteration 30: w = [1.99999999 2.99999998], b = 2.9999999860869453, loss = 13.263269359909671\n",
      "Iteration 31: w = [1.99999999 2.99999998], b = 2.9999999860811486, loss = 8.400171274881659\n",
      "Iteration 32: w = [1.99999999 2.99999998], b = 2.9999999860717663, loss = 22.00579362462584\n",
      "Iteration 33: w = [1.99999999 2.99999998], b = 2.999999986073188, loss = 0.5050763832385544\n",
      "Iteration 34: w = [1.99999999 2.99999998], b = 2.999999986065515, loss = 14.719079720841732\n",
      "Iteration 35: w = [1.99999999 2.99999998], b = 2.99999998605557, loss = 24.726365172642787\n",
      "Iteration 36: w = [1.99999999 2.99999998], b = 2.9999999860514635, loss = 4.2160774767853715\n",
      "Iteration 37: w = [1.99999999 2.99999998], b = 2.999999986056485, loss = 6.304221223719559\n",
      "Iteration 38: w = [1.99999999 2.99999998], b = 2.999999986051979, loss = 5.075939044817292\n",
      "Iteration 39: w = [1.99999999 2.99999998], b = 2.9999999860542204, loss = 1.255677166193406\n",
      "Iteration 40: w = [1.99999999 2.99999998], b = 2.999999986048549, loss = 8.041466479018574\n",
      "Iteration 41: w = [1.99999999 2.99999998], b = 2.999999986038335, loss = 26.08105648502806\n",
      "Iteration 42: w = [1.99999999 2.99999998], b = 2.9999999860345286, loss = 3.621618992246763\n",
      "Iteration 43: w = [1.99999999 2.99999998], b = 2.9999999860264466, loss = 16.328926748771693\n",
      "Iteration 44: w = [1.99999999 2.99999998], b = 2.99999998602955, loss = 2.407795114954312\n",
      "Iteration 45: w = [1.99999999 2.99999998], b = 2.9999999860174387, loss = 36.669143399431796\n",
      "Iteration 46: w = [1.99999999 2.99999998], b = 2.999999986004437, loss = 42.261789493962674\n",
      "Iteration 47: w = [1.99999999 2.99999998], b = 2.9999999859907467, loss = 46.855877804769136\n",
      "Iteration 48: w = [1.99999999 2.99999998], b = 2.99999998599341, loss = 1.7732224717096565\n",
      "Iteration 49: w = [1.99999999 2.99999998], b = 2.9999999859759674, loss = 76.05822082469096\n",
      "Iteration 50: w = [1.99999999 2.99999998], b = 2.9999999859605646, loss = 59.31180322734915\n",
      "Iteration 51: w = [1.99999999 2.99999998], b = 2.999999985951171, loss = 22.060649335294556\n",
      "Iteration 52: w = [1.99999999 2.99999998], b = 2.9999999859366406, loss = 52.77982526825887\n",
      "Iteration 53: w = [1.99999999 2.99999998], b = 2.9999999859264532, loss = 25.946931786435574\n",
      "Iteration 54: w = [1.99999999 2.99999998], b = 2.99999998592609, loss = 0.03296408042291783\n",
      "Iteration 55: w = [1.99999999 2.99999998], b = 2.9999999859215305, loss = 5.197414206338588\n",
      "Iteration 56: w = [1.99999999 2.99999998], b = 2.9999999859229907, loss = 0.5328615993287004\n",
      "Iteration 57: w = [1.99999999 2.99999998], b = 2.9999999859240107, loss = 0.26003316773799573\n",
      "Iteration 58: w = [1.99999999 2.99999998], b = 2.9999999859107755, loss = 43.79130890126367\n",
      "Iteration 59: w = [1.99999999 2.99999998], b = 2.999999985905452, loss = 7.08523832105416\n",
      "Iteration 60: w = [1.99999999 2.99999998], b = 2.9999999858946356, loss = 29.247440307004688\n",
      "Iteration 61: w = [1.99999999 2.99999998], b = 2.9999999858814044, loss = 43.765842899287776\n",
      "Iteration 62: w = [1.99999999 2.99999998], b = 2.999999985880893, loss = 0.06544188703906122\n",
      "Iteration 63: w = [1.99999999 2.99999998], b = 2.9999999858803847, loss = 0.06447846421604408\n",
      "Iteration 64: w = [1.99999999 2.99999998], b = 2.9999999858652857, loss = 56.99578985716366\n",
      "Iteration 65: w = [1.99999999 2.99999998], b = 2.9999999858543567, loss = 29.86212263698919\n",
      "Iteration 66: w = [1.99999999 2.99999998], b = 2.9999999858580155, loss = 3.346407334076068\n",
      "Iteration 67: w = [1.99999999 2.99999998], b = 2.999999985855571, loss = 1.4939344020553513\n",
      "Iteration 68: w = [1.99999999 2.99999998], b = 2.9999999858580533, loss = 1.5403938996540991\n",
      "Iteration 69: w = [1.99999999 2.99999998], b = 2.9999999858613307, loss = 2.6849505471166917\n",
      "Iteration 70: w = [1.99999999 2.99999998], b = 2.9999999858539623, loss = 13.573706226723885\n",
      "Iteration 71: w = [1.99999999 2.99999998], b = 2.9999999858514665, loss = 1.5570005053791967\n",
      "Iteration 72: w = [1.99999999 2.99999998], b = 2.9999999858550512, loss = 3.212821810720594\n",
      "Iteration 73: w = [1.99999999 2.99999998], b = 2.999999985847209, loss = 15.375023769889504\n",
      "Iteration 74: w = [1.99999999 2.99999998], b = 2.999999985839318, loss = 15.566792011590746\n",
      "Iteration 75: w = [1.99999999 2.99999998], b = 2.999999985840791, loss = 0.542586094298249\n",
      "Iteration 76: w = [1.99999999 2.99999998], b = 2.9999999858327464, loss = 16.18003599416206\n",
      "Iteration 77: w = [1.99999999 2.99999998], b = 2.9999999858245716, loss = 16.705937154319393\n",
      "Iteration 78: w = [1.99999999 2.99999998], b = 2.999999985815126, loss = 22.305517203056294\n",
      "Iteration 79: w = [1.99999999 2.99999998], b = 2.9999999858034636, loss = 34.00077309681246\n",
      "Iteration 80: w = [1.99999999 2.99999998], b = 2.99999998580865, loss = 6.724941738634564\n",
      "Iteration 81: w = [1.99999999 2.99999998], b = 2.9999999858110553, loss = 1.4464040315933093\n",
      "Iteration 82: w = [1.99999999 2.99999998], b = 2.9999999858158644, loss = 5.781928019244405\n",
      "Iteration 83: w = [1.99999999 2.99999998], b = 2.999999985813896, loss = 0.9682716889459164\n",
      "Iteration 84: w = [1.99999999 2.99999998], b = 2.999999985801221, loss = 40.16532247529083\n",
      "Iteration 85: w = [1.99999999 2.99999998], b = 2.9999999858056734, loss = 4.956500628643733\n",
      "Iteration 86: w = [1.99999999 2.99999998], b = 2.999999985796699, loss = 20.135063254235924\n",
      "Iteration 87: w = [1.99999999 2.99999998], b = 2.999999985792423, loss = 4.570684600707573\n",
      "Iteration 88: w = [1.99999999 2.99999998], b = 2.999999985791008, loss = 0.5007456124234609\n",
      "Iteration 89: w = [1.99999999 2.99999998], b = 2.9999999857911126, loss = 0.0027522825795826505\n",
      "Iteration 90: w = [1.99999999 2.99999998], b = 2.9999999857961384, loss = 6.314283706710788\n",
      "Iteration 91: w = [1.99999999 2.99999998], b = 2.999999985775118, loss = 110.46574556087289\n",
      "Iteration 92: w = [1.99999999 2.99999998], b = 2.9999999857637905, loss = 32.077778319126644\n",
      "Iteration 93: w = [1.99999999 2.99999998], b = 2.9999999857543775, loss = 22.15021443282859\n",
      "Iteration 94: w = [1.99999999 2.99999998], b = 2.9999999857465443, loss = 15.339389047683161\n",
      "Iteration 95: w = [1.99999999 2.99999998], b = 2.99999998574342, loss = 2.4402397921196735\n",
      "Iteration 96: w = [1.99999999 2.99999998], b = 2.9999999857403123, loss = 2.4143515130630666\n",
      "Iteration 97: w = [1.99999999 2.99999998], b = 2.9999999857269577, loss = 44.58701820877039\n",
      "Iteration 98: w = [1.99999999 2.99999998], b = 2.999999985726725, loss = 0.01355020772776662\n",
      "Iteration 99: w = [1.99999999 2.99999998], b = 2.9999999857216073, loss = 6.547856417652926\n",
      "Iteration 100: w = [1.99999999 2.99999998], b = 2.999999985723028, loss = 0.5044356712266518\n",
      "Iteration 101: w = [1.99999999 2.99999998], b = 2.9999999857223036, loss = 0.13111192268602062\n",
      "Iteration 102: w = [1.99999999 2.99999998], b = 2.99999998572368, loss = 0.47356217934462286\n",
      "Iteration 103: w = [1.99999999 2.99999998], b = 2.999999985722532, loss = 0.32947056862599167\n",
      "Iteration 104: w = [1.99999999 2.99999998], b = 2.999999985721372, loss = 0.33639762327806966\n",
      "Iteration 105: w = [1.99999999 2.99999998], b = 2.9999999857032074, loss = 82.4865617509525\n",
      "Iteration 106: w = [1.99999999 2.99999998], b = 2.9999999857064017, loss = 2.5508787054344104\n",
      "Iteration 107: w = [1.99999999 2.99999998], b = 2.999999985695286, loss = 30.88917483565696\n",
      "Iteration 108: w = [1.99999999 2.99999998], b = 2.999999985690369, loss = 6.044471117374042\n",
      "Iteration 109: w = [1.99999999 2.99999998], b = 2.9999999856879476, loss = 1.466123739315745\n",
      "Iteration 110: w = [1.99999999 2.99999998], b = 2.999999985692568, loss = 5.33635452421651\n",
      "Iteration 111: w = [1.99999999 2.99999998], b = 2.9999999856937265, loss = 0.3356042217299913\n",
      "Iteration 112: w = [1.99999999 2.99999998], b = 2.999999985684164, loss = 22.861507907989125\n",
      "Iteration 113: w = [1.99999999 2.99999998], b = 2.9999999856840898, loss = 0.0013828850671989187\n",
      "Iteration 114: w = [1.99999999 2.99999998], b = 2.999999985671105, loss = 42.150176352405765\n",
      "Iteration 115: w = [1.99999999 2.99999998], b = 2.999999985661557, loss = 22.790883712829416\n",
      "Iteration 116: w = [1.99999999 2.99999998], b = 2.9999999856491657, loss = 38.386209289068205\n",
      "Iteration 117: w = [1.99999999 2.99999998], b = 2.999999985652208, loss = 2.3141507879589565\n",
      "Iteration 118: w = [1.99999999 2.99999998], b = 2.9999999856439636, loss = 16.992872896589265\n",
      "Iteration 119: w = [1.99999999 2.99999998], b = 2.9999999856213853, loss = 127.44483674798147\n",
      "Iteration 120: w = [1.99999999 2.99999998], b = 2.999999985624101, loss = 1.8435114289573231\n",
      "Iteration 121: w = [1.99999999 2.99999998], b = 2.999999985617226, loss = 11.816059756362485\n",
      "Iteration 122: w = [1.99999999 2.99999998], b = 2.999999985623357, loss = 9.397161633277008\n",
      "Iteration 123: w = [1.99999999 2.99999998], b = 2.9999999856047666, loss = 86.4023191299459\n",
      "Iteration 124: w = [1.99999999 2.99999998], b = 2.9999999856026225, loss = 1.1491412165470385\n",
      "Iteration 125: w = [1.99999999 2.99999998], b = 2.9999999855955854, loss = 12.38046637879019\n",
      "Iteration 126: w = [1.99999999 2.99999998], b = 2.999999985599093, loss = 3.075281479912502\n",
      "Iteration 127: w = [1.99999999 2.99999998], b = 2.9999999855915687, loss = 14.153890472738471\n",
      "Iteration 128: w = [1.99999999 2.99999998], b = 2.99999998557951, loss = 36.35272552113534\n",
      "Iteration 129: w = [1.99999999 2.99999998], b = 2.9999999855717387, loss = 15.096850325441613\n",
      "Iteration 130: w = [1.99999999 2.99999998], b = 2.9999999855699593, loss = 0.7915851308563553\n",
      "Iteration 131: w = [1.99999999 2.99999998], b = 2.999999985574731, loss = 5.691845859071571\n",
      "Iteration 132: w = [1.99999999 2.99999998], b = 2.9999999855794686, loss = 5.61098368838629\n",
      "Iteration 133: w = [1.99999999 2.99999998], b = 2.9999999855716792, loss = 15.168855780281733\n",
      "Iteration 134: w = [1.99999999 2.99999998], b = 2.9999999855681216, loss = 3.164084494611423\n",
      "Iteration 135: w = [1.99999999 2.99999998], b = 2.999999985568829, loss = 0.12512302526328972\n",
      "Iteration 136: w = [1.99999999 2.99999998], b = 2.9999999855709136, loss = 1.0864188696946153\n",
      "Iteration 137: w = [1.99999999 2.99999998], b = 2.999999985562524, loss = 17.597155055033234\n",
      "Iteration 138: w = [1.99999999 2.99999998], b = 2.9999999855434027, loss = 91.40637447312774\n",
      "Iteration 139: w = [1.99999999 2.99999998], b = 2.99999998553228, loss = 30.92961564269766\n",
      "Iteration 140: w = [1.99999999 2.99999998], b = 2.9999999855326087, loss = 0.02701649725713499\n",
      "Iteration 141: w = [1.99999999 2.99999998], b = 2.9999999855237154, loss = 19.771912203290636\n",
      "Iteration 142: w = [1.99999999 2.99999998], b = 2.999999985527738, loss = 4.044858815210826\n",
      "Iteration 143: w = [1.99999999 2.99999998], b = 2.9999999855213737, loss = 10.1260406998259\n",
      "Iteration 144: w = [1.99999999 2.99999998], b = 2.999999985521172, loss = 0.010152442611660397\n",
      "Iteration 145: w = [1.99999999 2.99999998], b = 2.9999999855048882, loss = 66.29181759191869\n",
      "Iteration 146: w = [1.99999999 2.99999998], b = 2.999999985486208, loss = 87.23551644024053\n",
      "Iteration 147: w = [1.99999999 2.99999998], b = 2.99999998549236, loss = 9.461169606797927\n",
      "Iteration 148: w = [1.99999999 2.99999998], b = 2.999999985495881, loss = 3.099542874393454\n",
      "Iteration 149: w = [1.99999999 2.99999998], b = 2.999999985490822, loss = 6.39866990241574\n",
      "Iteration 150: w = [1.99999999 2.99999998], b = 2.9999999854854518, loss = 7.21066891461318\n",
      "Iteration 151: w = [1.99999999 2.99999998], b = 2.9999999854887074, loss = 2.6498659589013367\n",
      "Iteration 152: w = [1.99999999 2.99999998], b = 2.999999985471832, loss = 71.19318605650504\n",
      "Iteration 153: w = [1.99999999 2.99999998], b = 2.9999999854749433, loss = 2.4198220596364095\n",
      "Iteration 154: w = [1.99999999 2.99999998], b = 2.9999999854748265, loss = 0.0034110763971951954\n",
      "Iteration 155: w = [1.99999999 2.99999998], b = 2.999999985457433, loss = 75.63619778912368\n",
      "Iteration 156: w = [1.99999999 2.99999998], b = 2.999999985446376, loss = 30.5651547178038\n",
      "Iteration 157: w = [1.99999999 2.99999998], b = 2.999999985434627, loss = 34.50778890895007\n",
      "Iteration 158: w = [1.99999999 2.99999998], b = 2.9999999854307657, loss = 3.7275956731973605\n",
      "Iteration 159: w = [1.99999999 2.99999998], b = 2.99999998541859, loss = 37.06067803241289\n",
      "Iteration 160: w = [1.99999999 2.99999998], b = 2.9999999854047266, loss = 48.04843428759006\n",
      "Iteration 161: w = [1.99999999 2.99999998], b = 2.999999985397886, loss = 11.699309843968901\n",
      "Iteration 162: w = [1.99999999 2.99999998], b = 2.9999999853909745, loss = 11.941012353891\n",
      "Iteration 163: w = [1.99999999 2.99999998], b = 2.999999985383941, loss = 12.368069811819069\n",
      "Iteration 164: w = [1.99999999 2.99999998], b = 2.999999985370909, loss = 42.45604140808211\n",
      "Iteration 165: w = [1.99999999 2.99999998], b = 2.9999999853606636, loss = 26.242223498913113\n",
      "Iteration 166: w = [1.99999999 2.99999998], b = 2.99999998535703, loss = 3.3004749997361693\n",
      "Iteration 167: w = [1.99999999 2.99999998], b = 2.9999999853561454, loss = 0.1957095376301821\n",
      "Iteration 168: w = [1.99999999 2.99999998], b = 2.9999999853497563, loss = 10.20573451859796\n",
      "Iteration 169: w = [1.99999999 2.99999998], b = 2.9999999853505894, loss = 0.17348951320356418\n",
      "Iteration 170: w = [1.99999999 2.99999998], b = 2.9999999853532833, loss = 1.8142790456643498\n",
      "Iteration 171: w = [1.99999999 2.99999998], b = 2.999999985342384, loss = 29.697871091915182\n",
      "Iteration 172: w = [1.99999999 2.99999998], b = 2.9999999853436234, loss = 0.38407543386047605\n",
      "Iteration 173: w = [1.99999999 2.99999998], b = 2.9999999853262698, loss = 75.28616689968541\n",
      "Iteration 174: w = [1.99999999 2.99999998], b = 2.999999985326927, loss = 0.10803601902630083\n",
      "Iteration 175: w = [1.99999999 2.99999998], b = 2.9999999853261077, loss = 0.16790240250612712\n",
      "Iteration 176: w = [1.99999999 2.99999998], b = 2.9999999853133112, loss = 40.93590571536604\n",
      "Iteration 177: w = [1.99999999 2.99999998], b = 2.999999985307293, loss = 9.055236125170325\n",
      "Iteration 178: w = [1.99999999 2.99999998], b = 2.9999999853065673, loss = 0.13165650171532245\n",
      "Iteration 179: w = [1.99999999 2.99999998], b = 2.999999985287613, loss = 89.81680054602565\n",
      "Iteration 180: w = [1.99999999 2.99999998], b = 2.9999999852834716, loss = 4.288156169143218\n",
      "Iteration 181: w = [1.99999999 2.99999998], b = 2.999999985289028, loss = 7.718523534273227\n",
      "Iteration 182: w = [1.99999999 2.99999998], b = 2.999999985293436, loss = 4.857615027696251\n",
      "Iteration 183: w = [1.99999999 2.99999998], b = 2.999999985274651, loss = 88.22034364166171\n",
      "Iteration 184: w = [1.99999999 2.99999998], b = 2.9999999852708537, loss = 3.604690064695952\n",
      "Iteration 185: w = [1.99999999 2.99999998], b = 2.9999999852574812, loss = 44.70671655714514\n",
      "Iteration 186: w = [1.99999999 2.99999998], b = 2.9999999852440684, loss = 44.97570606892907\n",
      "Iteration 187: w = [1.99999999 2.99999998], b = 2.9999999852445676, loss = 0.06223693598289446\n",
      "Iteration 188: w = [1.99999999 2.99999998], b = 2.9999999852228862, loss = 117.51825194985251\n",
      "Iteration 189: w = [1.99999999 2.99999998], b = 2.9999999851937873, loss = 211.68531390659533\n",
      "Iteration 190: w = [1.99999999 2.99999998], b = 2.9999999851743047, loss = 94.89236113397247\n",
      "Iteration 191: w = [1.99999999 2.99999998], b = 2.999999985179551, loss = 6.881471569857935\n",
      "Iteration 192: w = [1.99999999 2.99999998], b = 2.9999999851733223, loss = 9.698999074399746\n",
      "Iteration 193: w = [1.99999999 2.99999998], b = 2.9999999851689423, loss = 4.796184037245372\n",
      "Iteration 194: w = [1.99999999 2.99999998], b = 2.9999999851648393, loss = 4.20836314322067\n",
      "Iteration 195: w = [1.99999999 2.99999998], b = 2.999999985163779, loss = 0.28108408174448385\n",
      "Iteration 196: w = [1.99999999 2.99999998], b = 2.999999985168065, loss = 4.593336136314891\n",
      "Iteration 197: w = [1.99999999 2.99999998], b = 2.9999999851668857, loss = 0.34773436800545626\n",
      "Iteration 198: w = [1.99999999 2.99999998], b = 2.999999985138548, loss = 200.75657470391\n",
      "Iteration 199: w = [1.99999999 2.99999998], b = 2.999999985136012, loss = 1.6077553438570729\n",
      "Iteration 200: w = [1.99999999 2.99999998], b = 2.9999999851356254, loss = 0.03739565702368975\n",
      "Iteration 201: w = [1.99999999 2.99999998], b = 2.999999985121964, loss = 46.65801993408243\n",
      "Iteration 202: w = [1.99999999 2.99999998], b = 2.9999999851137638, loss = 16.810046394924562\n",
      "Iteration 203: w = [1.99999999 2.99999998], b = 2.999999985105593, loss = 16.690000942605348\n",
      "Iteration 204: w = [1.99999999 2.99999998], b = 2.9999999850935546, loss = 36.23186428060433\n",
      "Iteration 205: w = [1.99999999 2.99999998], b = 2.999999985096684, loss = 2.448491268048473\n",
      "Iteration 206: w = [1.99999999 2.99999998], b = 2.999999985097026, loss = 0.0292252669138294\n",
      "Iteration 207: w = [1.99999999 2.99999998], b = 2.999999985087301, loss = 23.6433657256576\n",
      "Iteration 208: w = [1.99999999 2.99999998], b = 2.999999985082747, loss = 5.18462480270031\n",
      "Iteration 209: w = [1.99999999 2.99999998], b = 2.9999999850674013, loss = 58.8711407082525\n",
      "Iteration 210: w = [1.99999999 2.99999998], b = 2.9999999850635417, loss = 3.723963791589332\n",
      "Iteration 211: w = [1.99999999 2.99999998], b = 2.9999999850590346, loss = 5.0779191419986\n",
      "Iteration 212: w = [1.99999999 2.99999998], b = 2.9999999850481163, loss = 29.803247901931133\n",
      "Iteration 213: w = [1.99999999 2.99999998], b = 2.999999985051179, loss = 2.3454404558541375\n",
      "Iteration 214: w = [1.99999999 2.99999998], b = 2.999999985045785, loss = 7.2752012399682995\n",
      "Iteration 215: w = [1.99999999 2.99999998], b = 2.999999985033538, loss = 37.495691395681085\n",
      "Iteration 216: w = [1.99999999 2.99999998], b = 2.9999999850163515, loss = 73.84430110933954\n",
      "Iteration 217: w = [1.99999999 2.99999998], b = 2.99999998500396, loss = 38.38681442907066\n",
      "Iteration 218: w = [1.99999999 2.99999998], b = 2.9999999850066046, loss = 1.7481246515919668\n",
      "Iteration 219: w = [1.99999999 2.99999998], b = 2.999999985006194, loss = 0.04220183140252309\n",
      "Iteration 220: w = [1.99999999 2.99999998], b = 2.9999999850003154, loss = 8.638564464764121\n",
      "Iteration 221: w = [1.99999999 2.99999998], b = 2.9999999849888073, loss = 33.1100259573265\n",
      "Iteration 222: w = [1.99999999 2.99999998], b = 2.9999999849808368, loss = 15.881711805063862\n",
      "Iteration 223: w = [1.99999999 2.99999998], b = 2.9999999849638237, loss = 72.36128412997809\n",
      "Iteration 224: w = [1.99999999 2.99999998], b = 2.9999999849611707, loss = 1.7598813262748092\n",
      "Iteration 225: w = [1.99999999 2.99999998], b = 2.999999984938255, loss = 131.2825620558798\n",
      "Iteration 226: w = [1.99999999 2.99999998], b = 2.999999984933112, loss = 6.612334154401461\n",
      "Iteration 227: w = [1.99999999 2.99999998], b = 2.9999999849226624, loss = 27.29873227538006\n",
      "Iteration 228: w = [1.99999999 2.99999998], b = 2.9999999849212244, loss = 0.5168773538781892\n",
      "Iteration 229: w = [1.99999999 2.99999998], b = 2.999999984912754, loss = 17.937665904454278\n",
      "Iteration 230: w = [1.99999999 2.99999998], b = 2.9999999849107692, loss = 0.9845378373053103\n",
      "Iteration 231: w = [1.99999999 2.99999998], b = 2.9999999849094756, loss = 0.4184603579452137\n",
      "Iteration 232: w = [1.99999999 2.99999998], b = 2.999999984896084, loss = 44.832034973062854\n",
      "Iteration 233: w = [1.99999999 2.99999998], b = 2.999999984883167, loss = 41.71251530256009\n",
      "Iteration 234: w = [1.99999999 2.99999998], b = 2.999999984868826, loss = 51.41536853252024\n",
      "Iteration 235: w = [1.99999999 2.99999998], b = 2.999999984866254, loss = 1.653523925962559\n",
      "Iteration 236: w = [1.99999999 2.99999998], b = 2.9999999848518937, loss = 51.55484198717552\n",
      "Iteration 237: w = [1.99999999 2.99999998], b = 2.999999984852443, loss = 0.07538719667365856\n",
      "Iteration 238: w = [1.99999999 2.99999998], b = 2.9999999848600196, loss = 14.350584790038834\n",
      "Iteration 239: w = [1.99999999 2.99999998], b = 2.999999984861493, loss = 0.542936820570712\n",
      "Iteration 240: w = [1.99999999 2.99999998], b = 2.999999984859716, loss = 0.7897310609334675\n",
      "Iteration 241: w = [1.99999999 2.99999998], b = 2.999999984864926, loss = 6.786534532258795\n",
      "Iteration 242: w = [1.99999999 2.99999998], b = 2.999999984866989, loss = 1.0641832424764757\n",
      "Iteration 243: w = [1.99999999 2.99999998], b = 2.9999999848699312, loss = 2.1636592657721185\n",
      "Iteration 244: w = [1.99999999 2.99999998], b = 2.9999999848541923, loss = 61.927815361698734\n",
      "Iteration 245: w = [1.99999999 2.99999998], b = 2.999999984844811, loss = 22.002792565676756\n",
      "Iteration 246: w = [1.99999999 2.99999998], b = 2.9999999848248504, loss = 99.60638571371656\n",
      "Iteration 247: w = [1.99999999 2.99999998], b = 2.9999999848261516, loss = 0.42339566796506634\n",
      "Iteration 248: w = [1.99999999 2.99999998], b = 2.999999984821391, loss = 5.6659855176334775\n",
      "Iteration 249: w = [1.99999999 2.99999998], b = 2.9999999848136536, loss = 14.966608898448593\n",
      "Iteration 250: w = [1.99999999 2.99999998], b = 2.999999984812911, loss = 0.1377842809305201\n",
      "Iteration 251: w = [1.99999999 2.99999998], b = 2.9999999848081127, loss = 5.7560123373095085\n",
      "Iteration 252: w = [1.99999999 2.99999998], b = 2.9999999848109176, loss = 1.9669155642677105\n",
      "Iteration 253: w = [1.99999999 2.99999998], b = 2.9999999848112426, loss = 0.026434392053282317\n",
      "Iteration 254: w = [1.99999999 2.99999998], b = 2.9999999848034515, loss = 15.174509936539861\n",
      "Iteration 255: w = [1.99999999 2.99999998], b = 2.9999999848048855, loss = 0.5141233362949538\n",
      "Iteration 256: w = [1.99999999 2.99999998], b = 2.9999999847948207, loss = 25.32542601177776\n",
      "Iteration 257: w = [1.99999999 2.99999998], b = 2.9999999847935084, loss = 0.4303946461211664\n",
      "Iteration 258: w = [1.99999999 2.99999998], b = 2.9999999847961214, loss = 1.7070603364436112\n",
      "Iteration 259: w = [1.99999999 2.99999998], b = 2.9999999847954126, loss = 0.1255225268349877\n",
      "Iteration 260: w = [1.99999999 2.99999998], b = 2.9999999847984, loss = 2.2312805123352693\n",
      "Iteration 261: w = [1.99999999 2.99999998], b = 2.999999984792188, loss = 9.646997793557572\n",
      "Iteration 262: w = [1.99999999 2.99999998], b = 2.999999984780917, loss = 31.759425750858348\n",
      "Iteration 263: w = [1.99999999 2.99999998], b = 2.9999999847626624, loss = 83.3069728439208\n",
      "Iteration 264: w = [1.99999999 2.99999998], b = 2.999999984755462, loss = 12.962265987460666\n",
      "Iteration 265: w = [1.99999999 2.99999998], b = 2.9999999847536576, loss = 0.8137821746251646\n",
      "Iteration 266: w = [1.99999999 2.99999998], b = 2.9999999847589054, loss = 6.884720713900644\n",
      "Iteration 267: w = [1.99999999 2.99999998], b = 2.999999984761493, loss = 1.6740552398796902\n",
      "Iteration 268: w = [1.99999999 2.99999998], b = 2.9999999847580203, loss = 3.01540189065622\n",
      "Iteration 269: w = [1.99999999 2.99999998], b = 2.999999984762715, loss = 5.509705775817422\n",
      "Iteration 270: w = [1.99999999 2.99999998], b = 2.9999999847481558, loss = 52.99202817093673\n",
      "Iteration 271: w = [1.99999999 2.99999998], b = 2.999999984740732, loss = 13.778711197213642\n",
      "Iteration 272: w = [1.99999998 2.99999998], b = 2.9999999847259358, loss = 54.730044501240414\n",
      "Iteration 273: w = [1.99999998 2.99999998], b = 2.999999984721019, loss = 6.0436484353976585\n",
      "Iteration 274: w = [1.99999998 2.99999998], b = 2.9999999847141043, loss = 11.952183366387766\n",
      "Iteration 275: w = [1.99999998 2.99999998], b = 2.999999984713224, loss = 0.1936727789142592\n",
      "Iteration 276: w = [1.99999998 2.99999998], b = 2.9999999847049588, loss = 17.07888488373203\n",
      "Iteration 277: w = [1.99999998 2.99999998], b = 2.9999999847069345, loss = 0.9760231916342526\n",
      "Iteration 278: w = [1.99999998 2.99999998], b = 2.9999999847064394, loss = 0.06124301775257072\n",
      "Iteration 279: w = [1.99999998 2.99999998], b = 2.9999999846998535, loss = 10.8438230121043\n",
      "Iteration 280: w = [1.99999998 2.99999998], b = 2.9999999846762764, loss = 138.9712125286907\n",
      "Iteration 281: w = [1.99999998 2.99999998], b = 2.9999999846754193, loss = 0.18358560147010677\n",
      "Iteration 282: w = [1.99999998 2.99999998], b = 2.9999999846589516, loss = 67.79527031046338\n",
      "Iteration 283: w = [1.99999998 2.99999998], b = 2.9999999846595014, loss = 0.07561921595801009\n",
      "Iteration 284: w = [1.99999998 2.99999998], b = 2.999999984658601, loss = 0.20252859097118978\n",
      "Iteration 285: w = [1.99999998 2.99999998], b = 2.999999984650522, loss = 16.318034845096978\n",
      "Iteration 286: w = [1.99999998 2.99999998], b = 2.999999984651977, loss = 0.5294554023062823\n",
      "Iteration 287: w = [1.99999998 2.99999998], b = 2.999999984642646, loss = 21.768284729061495\n",
      "Iteration 288: w = [1.99999998 2.99999998], b = 2.9999999846253704, loss = 74.612048392472\n",
      "Iteration 289: w = [1.99999998 2.99999998], b = 2.999999984620422, loss = 6.121607040519424\n",
      "Iteration 290: w = [1.99999998 2.99999998], b = 2.9999999846207466, loss = 0.026374696749834562\n",
      "Iteration 291: w = [1.99999998 2.99999998], b = 2.9999999846189187, loss = 0.8352196991152168\n",
      "Iteration 292: w = [1.99999998 2.99999998], b = 2.9999999846165566, loss = 1.3946838325824995\n",
      "Iteration 293: w = [1.99999998 2.99999998], b = 2.999999984603831, loss = 40.48267199706038\n",
      "Iteration 294: w = [1.99999998 2.99999998], b = 2.999999984599937, loss = 3.7909397823373507\n",
      "Iteration 295: w = [1.99999998 2.99999998], b = 2.9999999845932286, loss = 11.250344550745448\n",
      "Iteration 296: w = [1.99999998 2.99999998], b = 2.9999999845907346, loss = 1.5548569898681417\n",
      "Iteration 297: w = [1.99999998 2.99999998], b = 2.9999999845925394, loss = 0.8143229595004556\n",
      "Iteration 298: w = [1.99999998 2.99999998], b = 2.999999984563627, loss = 208.9818100706353\n",
      "Iteration 299: w = [1.99999998 2.99999998], b = 2.9999999845448015, loss = 88.5994232913797\n",
      "Iteration 300: w = [1.99999998 2.99999998], b = 2.999999984543235, loss = 0.6137987877839909\n",
      "Iteration 301: w = [1.99999998 2.99999998], b = 2.9999999845396634, loss = 3.1889214414387475\n",
      "Iteration 302: w = [1.99999998 2.99999998], b = 2.9999999845434733, loss = 3.6288060316482604\n",
      "Iteration 303: w = [1.99999998 2.99999998], b = 2.9999999845363967, loss = 12.519417616545805\n",
      "Iteration 304: w = [1.99999998 2.99999998], b = 2.999999984518957, loss = 76.03585279069159\n",
      "Iteration 305: w = [1.99999998 2.99999998], b = 2.9999999845122742, loss = 11.164966920496193\n",
      "Iteration 306: w = [1.99999998 2.99999998], b = 2.9999999845037797, loss = 18.03996023571345\n",
      "Iteration 307: w = [1.99999998 2.99999998], b = 2.999999984501114, loss = 1.7766135351122712\n",
      "Iteration 308: w = [1.99999998 2.99999998], b = 2.999999984493885, loss = 13.064793911041678\n",
      "Iteration 309: w = [1.99999998 2.99999998], b = 2.999999984502182, loss = 17.209574972686276\n",
      "Iteration 310: w = [1.99999998 2.99999998], b = 2.9999999845003558, loss = 0.8334989459032841\n",
      "Iteration 311: w = [1.99999998 2.99999998], b = 2.9999999844867755, loss = 46.105640493730014\n",
      "Iteration 312: w = [1.99999998 2.99999998], b = 2.999999984479529, loss = 13.1290503153438\n",
      "Iteration 313: w = [1.99999998 2.99999998], b = 2.999999984470243, loss = 21.55760053539745\n",
      "Iteration 314: w = [1.99999998 2.99999998], b = 2.9999999844590897, loss = 31.10008831923514\n",
      "Iteration 315: w = [1.99999998 2.99999998], b = 2.999999984450298, loss = 19.323443488151728\n",
      "Iteration 316: w = [1.99999998 2.99999998], b = 2.999999984429766, loss = 105.38910132954638\n",
      "Iteration 317: w = [1.99999998 2.99999998], b = 2.999999984419427, loss = 26.723150112112506\n",
      "Iteration 318: w = [1.99999998 2.99999998], b = 2.9999999844183445, loss = 0.29309970465804813\n",
      "Iteration 319: w = [1.99999998 2.99999998], b = 2.999999984415244, loss = 2.4033612951257544\n",
      "Iteration 320: w = [1.99999998 2.99999998], b = 2.9999999843863026, loss = 209.3990029502536\n",
      "Iteration 321: w = [1.99999998 2.99999998], b = 2.999999984387342, loss = 0.2701052159314416\n",
      "Iteration 322: w = [1.99999998 2.99999998], b = 2.999999984379949, loss = 13.664508801965278\n",
      "Iteration 323: w = [1.99999998 2.99999998], b = 2.999999984377569, loss = 1.4158311691523446\n",
      "Iteration 324: w = [1.99999998 2.99999998], b = 2.9999999843689182, loss = 18.70918752225907\n",
      "Iteration 325: w = [1.99999998 2.99999998], b = 2.9999999843605614, loss = 17.459514373765654\n",
      "Iteration 326: w = [1.99999998 2.99999998], b = 2.999999984351963, loss = 18.483217544057414\n",
      "Iteration 327: w = [1.99999998 2.99999998], b = 2.9999999843476566, loss = 4.636375299676606\n",
      "Iteration 328: w = [1.99999998 2.99999998], b = 2.999999984334942, loss = 40.41629021385001\n",
      "Iteration 329: w = [1.99999998 2.99999998], b = 2.9999999843374243, loss = 1.5405622043059113\n",
      "Iteration 330: w = [1.99999998 2.99999998], b = 2.999999984334822, loss = 1.6933238714530194\n",
      "Iteration 331: w = [1.99999998 2.99999998], b = 2.9999999843378604, loss = 2.3081366678605275\n",
      "Iteration 332: w = [1.99999998 2.99999998], b = 2.999999984324844, loss = 42.35669618836641\n",
      "Iteration 333: w = [1.99999998 2.99999998], b = 2.9999999843238783, loss = 0.23329215046985396\n",
      "Iteration 334: w = [1.99999998 2.99999998], b = 2.9999999843211205, loss = 1.901128509962679\n",
      "Iteration 335: w = [1.99999998 2.99999998], b = 2.999999984327197, loss = 9.231125757886517\n",
      "Iteration 336: w = [1.99999998 2.99999998], b = 2.999999984329096, loss = 0.9013802585047096\n",
      "Iteration 337: w = [1.99999998 2.99999998], b = 2.999999984320673, loss = 17.737630419631028\n",
      "Iteration 338: w = [1.99999998 2.99999998], b = 2.9999999843005627, loss = 101.10641446732303\n",
      "Iteration 339: w = [1.99999998 2.99999998], b = 2.9999999842878338, loss = 40.50703907764253\n",
      "Iteration 340: w = [1.99999998 2.99999998], b = 2.999999984269434, loss = 84.63790859391669\n",
      "Iteration 341: w = [1.99999998 2.99999998], b = 2.9999999842694427, loss = 1.9042195987612426e-05\n",
      "Iteration 342: w = [1.99999998 2.99999998], b = 2.9999999842497433, loss = 97.01758821748065\n",
      "Iteration 343: w = [1.99999998 2.99999998], b = 2.9999999842363168, loss = 45.06886315022477\n",
      "Iteration 344: w = [1.99999998 2.99999998], b = 2.9999999842217226, loss = 53.24846789216334\n",
      "Iteration 345: w = [1.99999998 2.99999998], b = 2.999999984212485, loss = 21.33286122242441\n",
      "Iteration 346: w = [1.99999998 2.99999998], b = 2.999999984216499, loss = 4.027295243320705\n",
      "Iteration 347: w = [1.99999998 2.99999998], b = 2.9999999842166316, loss = 0.004409872867687593\n",
      "Iteration 348: w = [1.99999998 2.99999998], b = 2.9999999842225273, loss = 8.68982219416907\n",
      "Iteration 349: w = [1.99999998 2.99999998], b = 2.9999999842119096, loss = 28.185191161477935\n",
      "Iteration 350: w = [1.99999998 2.99999998], b = 2.9999999842015814, loss = 26.668402202045748\n",
      "Iteration 351: w = [1.99999998 2.99999998], b = 2.9999999841875633, loss = 49.12631872499626\n",
      "Iteration 352: w = [1.99999998 2.99999998], b = 2.9999999841836917, loss = 3.7471345952546\n",
      "Iteration 353: w = [1.99999998 2.99999998], b = 2.9999999841711205, loss = 39.50883137829178\n",
      "Iteration 354: w = [1.99999998 2.99999998], b = 2.999999984162735, loss = 17.577953082415757\n",
      "Iteration 355: w = [1.99999998 2.99999998], b = 2.9999999841487144, loss = 49.14419114229191\n",
      "Iteration 356: w = [1.99999998 2.99999998], b = 2.9999999841496092, loss = 0.2001138713808932\n",
      "Iteration 357: w = [1.99999998 2.99999998], b = 2.999999984134763, loss = 55.10387574887476\n",
      "Iteration 358: w = [1.99999998 2.99999998], b = 2.999999984137647, loss = 2.0794968032380887\n",
      "Iteration 359: w = [1.99999998 2.99999998], b = 2.9999999841292295, loss = 17.712748431206585\n",
      "Iteration 360: w = [1.99999998 2.99999998], b = 2.9999999841214344, loss = 15.191030534208025\n",
      "Iteration 361: w = [1.99999998 2.99999998], b = 2.9999999841093743, loss = 36.36203129420149\n",
      "Iteration 362: w = [1.99999998 2.99999998], b = 2.9999999841161, loss = 11.30854495050048\n",
      "Iteration 363: w = [1.99999998 2.99999998], b = 2.999999984108876, loss = 13.047184822646726\n",
      "Iteration 364: w = [1.99999998 2.99999998], b = 2.9999999840997504, loss = 20.818129854273945\n",
      "Iteration 365: w = [1.99999998 2.99999998], b = 2.9999999841022436, loss = 1.554119210009598\n",
      "Iteration 366: w = [1.99999998 2.99999998], b = 2.999999984099592, loss = 1.7579704988044056\n",
      "Iteration 367: w = [1.99999998 2.99999998], b = 2.999999984096083, loss = 3.0774723420164087\n",
      "Iteration 368: w = [1.99999998 2.99999998], b = 2.9999999840959406, loss = 0.005087900515733925\n",
      "Iteration 369: w = [1.99999998 2.99999998], b = 2.9999999840982636, loss = 1.3489675119516489\n",
      "Iteration 370: w = [1.99999998 2.99999998], b = 2.9999999840768723, loss = 114.3961148829768\n",
      "Iteration 371: w = [1.99999998 2.99999998], b = 2.9999999840762106, loss = 0.10946326033970015\n",
      "Iteration 372: w = [1.99999998 2.99999998], b = 2.999999984067311, loss = 19.800018731841053\n",
      "Iteration 373: w = [1.99999998 2.99999998], b = 2.9999999840532916, loss = 49.13684506512188\n",
      "Iteration 374: w = [1.99999998 2.99999998], b = 2.9999999840526166, loss = 0.1138668154459457\n",
      "Iteration 375: w = [1.99999998 2.99999998], b = 2.999999984039685, loss = 41.80605727219395\n",
      "Iteration 376: w = [1.99999998 2.99999998], b = 2.999999984037975, loss = 0.7311174170048003\n",
      "Iteration 377: w = [1.99999998 2.99999998], b = 2.999999984037299, loss = 0.11423942463964011\n",
      "Iteration 378: w = [1.99999998 2.99999998], b = 2.999999984033858, loss = 2.9604151773068557\n",
      "Iteration 379: w = [1.99999998 2.99999998], b = 2.999999984024009, loss = 24.251463548236465\n",
      "Iteration 380: w = [1.99999998 2.99999998], b = 2.999999984021359, loss = 1.7556722870129442\n",
      "Iteration 381: w = [1.99999998 2.99999998], b = 2.9999999840128515, loss = 18.09484744352532\n",
      "Iteration 382: w = [1.99999998 2.99999998], b = 2.9999999840141744, loss = 0.43754009876364947\n",
      "Iteration 383: w = [1.99999998 2.99999998], b = 2.9999999840000506, loss = 49.87162892517683\n",
      "Iteration 384: w = [1.99999998 2.99999998], b = 2.999999983998978, loss = 0.2876142558360061\n",
      "Iteration 385: w = [1.99999998 2.99999998], b = 2.9999999839916303, loss = 13.498706432807571\n",
      "Iteration 386: w = [1.99999998 2.99999998], b = 2.9999999839656213, loss = 169.11576374458926\n",
      "Iteration 387: w = [1.99999998 2.99999998], b = 2.999999983956464, loss = 20.963816026274042\n",
      "Iteration 388: w = [1.99999998 2.99999998], b = 2.999999983939826, loss = 69.20706825205957\n",
      "Iteration 389: w = [1.99999998 2.99999998], b = 2.9999999839434697, loss = 3.3191222119796238\n",
      "Iteration 390: w = [1.99999998 2.99999998], b = 2.9999999839396714, loss = 3.6066918939156842\n",
      "Iteration 391: w = [1.99999998 2.99999998], b = 2.999999983941616, loss = 0.9454238499274824\n",
      "Iteration 392: w = [1.99999998 2.99999998], b = 2.999999983941618, loss = 7.808218056206117e-07\n",
      "Iteration 393: w = [1.99999998 2.99999998], b = 2.9999999839326215, loss = 20.2330411795587\n",
      "Iteration 394: w = [1.99999998 2.99999998], b = 2.999999983916616, loss = 64.04311982813967\n",
      "Iteration 395: w = [1.99999998 2.99999998], b = 2.999999983902844, loss = 47.418488687642046\n",
      "Iteration 396: w = [1.99999998 2.99999998], b = 2.9999999838886473, loss = 50.38544515842723\n",
      "Iteration 397: w = [1.99999998 2.99999998], b = 2.9999999838803606, loss = 17.16668447262182\n",
      "Iteration 398: w = [1.99999998 2.99999998], b = 2.9999999838641087, loss = 66.0320914515796\n",
      "Iteration 399: w = [1.99999998 2.99999998], b = 2.9999999838575477, loss = 10.76171888547343\n",
      "Iteration 400: w = [1.99999998 2.99999998], b = 2.999999983857354, loss = 0.009367264589988725\n",
      "Iteration 401: w = [1.99999998 2.99999998], b = 2.9999999838457665, loss = 33.567986297671915\n",
      "Iteration 402: w = [1.99999998 2.99999998], b = 2.9999999838400644, loss = 8.128506416090328\n",
      "Iteration 403: w = [1.99999998 2.99999998], b = 2.9999999838331006, loss = 12.123064092781009\n",
      "Iteration 404: w = [1.99999998 2.99999998], b = 2.9999999838275295, loss = 7.759141916197462\n",
      "Iteration 405: w = [1.99999998 2.99999998], b = 2.999999983809365, loss = 82.48843656672457\n",
      "Iteration 406: w = [1.99999998 2.99999998], b = 2.9999999838105382, loss = 0.34426727195379186\n",
      "Iteration 407: w = [1.99999998 2.99999998], b = 2.9999999838135403, loss = 2.2533140897492867\n",
      "Iteration 408: w = [1.99999998 2.99999998], b = 2.9999999837969584, loss = 68.74062816786622\n",
      "Iteration 409: w = [1.99999998 2.99999998], b = 2.9999999837829594, loss = 48.99176986326552\n",
      "Iteration 410: w = [1.99999998 2.99999998], b = 2.9999999837777094, loss = 6.890996913087491\n",
      "Iteration 411: w = [1.99999998 2.99999998], b = 2.99999998377228, loss = 7.369864053165166\n",
      "Iteration 412: w = [1.99999998 2.99999998], b = 2.9999999837548375, loss = 76.05839167840735\n",
      "Iteration 413: w = [1.99999998 2.99999998], b = 2.9999999837608993, loss = 9.18676651776277\n",
      "Iteration 414: w = [1.99999998 2.99999998], b = 2.9999999837463145, loss = 53.18049236591645\n",
      "Iteration 415: w = [1.99999998 2.99999998], b = 2.999999983739973, loss = 10.05375430246955\n",
      "Iteration 416: w = [1.99999998 2.99999998], b = 2.999999983742365, loss = 1.430691526884338\n",
      "Iteration 417: w = [1.99999998 2.99999998], b = 2.9999999837458375, loss = 3.014268229921443\n",
      "Iteration 418: w = [1.99999998 2.99999998], b = 2.9999999837296656, loss = 65.38149038053146\n",
      "Iteration 419: w = [1.99999998 2.99999998], b = 2.9999999837150333, loss = 53.525893238655\n",
      "Iteration 420: w = [1.99999998 2.99999998], b = 2.999999983720937, loss = 8.71333629748584\n",
      "Iteration 421: w = [1.99999998 2.99999998], b = 2.999999983724997, loss = 4.120804032691862\n",
      "Iteration 422: w = [1.99999998 2.99999998], b = 2.9999999837226885, loss = 1.3320513607716193\n",
      "Iteration 423: w = [1.99999998 2.99999998], b = 2.9999999837233857, loss = 0.1214799838768832\n",
      "Iteration 424: w = [1.99999998 2.99999998], b = 2.9999999837164872, loss = 11.897869713348538\n",
      "Iteration 425: w = [1.99999998 2.99999998], b = 2.999999983712884, loss = 3.2456995641550286\n",
      "Iteration 426: w = [1.99999998 2.99999998], b = 2.9999999836971885, loss = 61.58766614146866\n",
      "Iteration 427: w = [1.99999998 2.99999998], b = 2.9999999836835203, loss = 46.70604300265766\n",
      "Iteration 428: w = [1.99999998 2.99999998], b = 2.99999998367737, loss = 9.456275714306724\n",
      "Iteration 429: w = [1.99999998 2.99999998], b = 2.999999983679022, loss = 0.6824331721427248\n",
      "Iteration 430: w = [1.99999998 2.99999998], b = 2.9999999836751976, loss = 3.656377068164939\n",
      "Iteration 431: w = [1.99999998 2.99999998], b = 2.999999983668957, loss = 9.736914824351205\n",
      "Iteration 432: w = [1.99999998 2.99999998], b = 2.99999998366419, loss = 5.680441006787214\n",
      "Iteration 433: w = [1.99999998 2.99999998], b = 2.9999999836448312, loss = 93.68872305322174\n",
      "Iteration 434: w = [1.99999998 2.99999998], b = 2.9999999836484212, loss = 3.2220848905491803\n",
      "Iteration 435: w = [1.99999998 2.99999998], b = 2.999999983653357, loss = 6.0903115066320765\n",
      "Iteration 436: w = [1.99999998 2.99999998], b = 2.9999999836500746, loss = 2.6932869078371473\n",
      "Iteration 437: w = [1.99999998 2.99999998], b = 2.9999999836430087, loss = 12.482369795820443\n",
      "Iteration 438: w = [1.99999998 2.99999998], b = 2.9999999836483484, loss = 7.128108003892429\n",
      "Iteration 439: w = [1.99999998 2.99999998], b = 2.9999999836384807, loss = 24.34250714744957\n",
      "Iteration 440: w = [1.99999998 2.99999998], b = 2.9999999836438045, loss = 7.085148412538875\n",
      "Iteration 441: w = [1.99999998 2.99999998], b = 2.999999983645272, loss = 0.5384573193917139\n",
      "Iteration 442: w = [1.99999998 2.99999998], b = 2.9999999836365996, loss = 18.803286076976597\n",
      "Iteration 443: w = [1.99999998 2.99999998], b = 2.999999983632604, loss = 3.99133583461707\n",
      "Iteration 444: w = [1.99999998 2.99999998], b = 2.999999983629168, loss = 2.951117109110976\n",
      "Iteration 445: w = [1.99999998 2.99999998], b = 2.9999999836252167, loss = 3.903268034265566\n",
      "Iteration 446: w = [1.99999998 2.99999998], b = 2.999999983622446, loss = 1.9193499300686245\n",
      "Iteration 447: w = [1.99999998 2.99999998], b = 2.999999983617534, loss = 6.031853870485708\n",
      "Iteration 448: w = [1.99999998 2.99999998], b = 2.9999999836191464, loss = 0.6501268788092741\n",
      "Iteration 449: w = [1.99999998 2.99999998], b = 2.999999983619007, loss = 0.004853476730875561\n",
      "Iteration 450: w = [1.99999998 2.99999998], b = 2.999999983611124, loss = 15.53550964915377\n",
      "Iteration 451: w = [1.99999998 2.99999998], b = 2.9999999836015965, loss = 22.693478718063727\n",
      "Iteration 452: w = [1.99999998 2.99999998], b = 2.999999983598636, loss = 2.1910778540694036\n",
      "Iteration 453: w = [1.99999998 2.99999998], b = 2.999999983584738, loss = 48.29115043275492\n",
      "Iteration 454: w = [1.99999998 2.99999998], b = 2.999999983588729, loss = 3.9816710482540474\n",
      "Iteration 455: w = [1.99999998 2.99999998], b = 2.9999999835946722, loss = 8.830521235028574\n",
      "Iteration 456: w = [1.99999998 2.99999998], b = 2.9999999835929687, loss = 0.7253502713966532\n",
      "Iteration 457: w = [1.99999998 2.99999998], b = 2.9999999835762376, loss = 69.9823964985134\n",
      "Iteration 458: w = [1.99999998 2.99999998], b = 2.99999998357295, loss = 2.7023218595176273\n",
      "Iteration 459: w = [1.99999998 2.99999998], b = 2.9999999835532427, loss = 97.09663065381194\n",
      "Iteration 460: w = [1.99999998 2.99999998], b = 2.9999999835424935, loss = 28.88667660052029\n",
      "Iteration 461: w = [1.99999998 2.99999998], b = 2.999999983528636, loss = 48.0081151724467\n",
      "Iteration 462: w = [1.99999998 2.99999998], b = 2.999999983509251, loss = 93.9472569256577\n",
      "Iteration 463: w = [1.99999998 2.99999998], b = 2.9999999835014703, loss = 15.134646577889896\n",
      "Iteration 464: w = [1.99999998 2.99999998], b = 2.999999983487987, loss = 45.45026785900066\n",
      "Iteration 465: w = [1.99999998 2.99999998], b = 2.9999999834739244, loss = 49.439774539763015\n",
      "Iteration 466: w = [1.99999998 2.99999998], b = 2.9999999834752846, loss = 0.4624342583917151\n",
      "Iteration 467: w = [1.99999998 2.99999998], b = 2.9999999834739195, loss = 0.46581271502455374\n",
      "Iteration 468: w = [1.99999998 2.99999998], b = 2.9999999834691864, loss = 5.60011768351208\n",
      "Iteration 469: w = [1.99999998 2.99999998], b = 2.999999983474576, loss = 7.2617778173611285\n",
      "Iteration 470: w = [1.99999998 2.99999998], b = 2.9999999834763704, loss = 0.8052977301935915\n",
      "Iteration 471: w = [1.99999998 2.99999998], b = 2.9999999834675397, loss = 19.49471388284238\n",
      "Iteration 472: w = [1.99999998 2.99999998], b = 2.9999999834440856, loss = 137.5255823093924\n",
      "Iteration 473: w = [1.99999998 2.99999998], b = 2.9999999834404956, loss = 3.2216587843592523\n",
      "Iteration 474: w = [1.99999998 2.99999998], b = 2.9999999834277475, loss = 40.62784485579207\n",
      "Iteration 475: w = [1.99999998 2.99999998], b = 2.999999983409312, loss = 84.96699520929991\n",
      "Iteration 476: w = [1.99999998 2.99999998], b = 2.999999983409641, loss = 0.027063850386754528\n",
      "Iteration 477: w = [1.99999998 2.99999998], b = 2.999999983401542, loss = 16.400476280916862\n",
      "Iteration 478: w = [1.99999998 2.99999998], b = 2.9999999833904267, loss = 30.885181885089306\n",
      "Iteration 479: w = [1.99999998 2.99999998], b = 2.999999983390753, loss = 0.026650600327915747\n",
      "Iteration 480: w = [1.99999998 2.99999998], b = 2.9999999833971387, loss = 10.194322027108976\n",
      "Iteration 481: w = [1.99999998 2.99999998], b = 2.999999983395418, loss = 0.7403022019570553\n",
      "Iteration 482: w = [1.99999998 2.99999998], b = 2.999999983386999, loss = 17.720709876228863\n",
      "Iteration 483: w = [1.99999998 2.99999998], b = 2.9999999833818167, loss = 6.713222507709227\n",
      "Iteration 484: w = [1.99999998 2.99999998], b = 2.9999999833842357, loss = 1.462958228752821\n",
      "Iteration 485: w = [1.99999998 2.99999998], b = 2.999999983356826, loss = 187.8229460403071\n",
      "Iteration 486: w = [1.99999998 2.99999998], b = 2.9999999833382174, loss = 86.57177332418988\n",
      "Iteration 487: w = [1.99999998 2.99999998], b = 2.9999999833436397, loss = 7.350316706836186\n",
      "Iteration 488: w = [1.99999998 2.99999998], b = 2.999999983327717, loss = 63.38263474072466\n",
      "Iteration 489: w = [1.99999998 2.99999998], b = 2.9999999833103654, loss = 75.26920168339592\n",
      "Iteration 490: w = [1.99999998 2.99999998], b = 2.9999999832938866, loss = 67.88897863111545\n",
      "Iteration 491: w = [1.99999998 2.99999998], b = 2.9999999832886823, loss = 6.771026713235516\n",
      "Iteration 492: w = [1.99999998 2.99999998], b = 2.9999999832909423, loss = 1.2770454213209432\n",
      "Iteration 493: w = [1.99999998 2.99999998], b = 2.999999983289843, loss = 0.3019067654635122\n",
      "Iteration 494: w = [1.99999998 2.99999998], b = 2.9999999832887174, loss = 0.3168212687555885\n",
      "Iteration 495: w = [1.99999998 2.99999998], b = 2.9999999832765605, loss = 36.94647138687141\n",
      "Iteration 496: w = [1.99999998 2.99999998], b = 2.999999983278537, loss = 0.9765614104864152\n",
      "Iteration 497: w = [1.99999998 2.99999998], b = 2.9999999832830633, loss = 5.121787597876248\n",
      "Iteration 498: w = [1.99999998 2.99999998], b = 2.9999999832801723, loss = 2.089634688415983\n",
      "Iteration 499: w = [1.99999998 2.99999998], b = 2.9999999832818705, loss = 0.7208491890053365\n",
      "Iteration 500: w = [1.99999998 2.99999998], b = 2.9999999832684487, loss = 45.03620815392045\n",
      "Iteration 501: w = [1.99999998 2.99999998], b = 2.999999983249766, loss = 87.26027184157914\n",
      "Iteration 502: w = [1.99999998 2.99999998], b = 2.999999983242564, loss = 12.967098223552158\n",
      "Iteration 503: w = [1.99999998 2.99999998], b = 2.99999998323937, loss = 2.5508996027750994\n",
      "Iteration 504: w = [1.99999998 2.99999998], b = 2.9999999832265414, loss = 41.1417073261812\n",
      "Iteration 505: w = [1.99999998 2.99999998], b = 2.999999983217465, loss = 20.593958271586267\n",
      "Iteration 506: w = [1.99999998 2.99999998], b = 2.9999999831920854, loss = 161.03419680503328\n",
      "Iteration 507: w = [1.99999998 2.99999998], b = 2.999999983191007, loss = 0.29058901189706193\n",
      "Iteration 508: w = [1.99999998 2.99999998], b = 2.9999999831732564, loss = 78.77157218173927\n",
      "Iteration 509: w = [1.99999998 2.99999998], b = 2.999999983169616, loss = 3.313586658013608\n",
      "Iteration 510: w = [1.99999998 2.99999998], b = 2.9999999831586366, loss = 30.13552280807149\n",
      "Iteration 511: w = [1.99999998 2.99999998], b = 2.9999999831614854, loss = 2.0287936446320862\n",
      "Iteration 512: w = [1.99999998 2.99999998], b = 2.999999983158796, loss = 1.8080722985918942\n",
      "Iteration 513: w = [1.99999998 2.99999998], b = 2.999999983154044, loss = 5.645756285087654\n",
      "Iteration 514: w = [1.99999998 2.99999998], b = 2.9999999831515956, loss = 1.4984347638066722\n",
      "Iteration 515: w = [1.99999998 2.99999998], b = 2.9999999831488733, loss = 1.8524105654728216\n",
      "Iteration 516: w = [1.99999998 2.99999998], b = 2.9999999831474877, loss = 0.4799922922191954\n",
      "Iteration 517: w = [1.99999998 2.99999998], b = 2.999999983135083, loss = 38.46815112744151\n",
      "Iteration 518: w = [1.99999998 2.99999998], b = 2.999999983136908, loss = 0.8326715300287767\n",
      "Iteration 519: w = [1.99999998 2.99999998], b = 2.999999983131621, loss = 6.988889655740919\n",
      "Iteration 520: w = [1.99999998 2.99999998], b = 2.999999983122172, loss = 22.319313837698587\n",
      "Iteration 521: w = [1.99999998 2.99999998], b = 2.999999983124651, loss = 1.5360943585167433\n",
      "Iteration 522: w = [1.99999998 2.99999998], b = 2.999999983111805, loss = 41.2520299868534\n",
      "Iteration 523: w = [1.99999998 2.99999998], b = 2.999999983094977, loss = 70.79647749600306\n",
      "Iteration 524: w = [1.99999998 2.99999998], b = 2.999999983090861, loss = 4.235286733778124\n",
      "Iteration 525: w = [1.99999998 2.99999998], b = 2.9999999830907096, loss = 0.005739539435990267\n",
      "Iteration 526: w = [1.99999998 2.99999998], b = 2.999999983093881, loss = 2.5142810873444894\n",
      "Iteration 527: w = [1.99999998 2.99999998], b = 2.9999999830748036, loss = 90.98611918348546\n",
      "Iteration 528: w = [1.99999998 2.99999998], b = 2.999999983076179, loss = 0.4729604373444691\n",
      "Iteration 529: w = [1.99999998 2.99999998], b = 2.9999999830813087, loss = 6.578084756887769\n",
      "Iteration 530: w = [1.99999998 2.99999998], b = 2.9999999830733306, loss = 15.912376869509414\n",
      "Iteration 531: w = [1.99999998 2.99999998], b = 2.999999983072452, loss = 0.19290633059931106\n",
      "Iteration 532: w = [1.99999998 2.99999998], b = 2.9999999830530197, loss = 94.40447699006418\n",
      "Iteration 533: w = [1.99999998 2.99999998], b = 2.9999999830537716, loss = 0.1413533048080638\n",
      "Iteration 534: w = [1.99999998 2.99999998], b = 2.9999999830519393, loss = 0.8392254526042777\n",
      "Iteration 535: w = [1.99999998 2.99999998], b = 2.999999983037126, loss = 54.859115514425895\n",
      "Iteration 536: w = [1.99999998 2.99999998], b = 2.9999999830302126, loss = 11.9484709545954\n",
      "Iteration 537: w = [1.99999998 2.99999998], b = 2.999999983032108, loss = 0.8979413435151822\n",
      "Iteration 538: w = [1.99999998 2.99999998], b = 2.9999999830338977, loss = 0.8006554033174782\n",
      "Iteration 539: w = [1.99999998 2.99999998], b = 2.9999999830389754, loss = 6.445887511497876\n",
      "Iteration 540: w = [1.99999998 2.99999998], b = 2.999999983043372, loss = 4.83196945577794\n",
      "Iteration 541: w = [1.99999998 2.99999998], b = 2.999999983045317, loss = 0.9456516740031912\n",
      "Iteration 542: w = [1.99999998 2.99999998], b = 2.9999999830485566, loss = 2.623745682605664\n",
      "Iteration 543: w = [1.99999998 2.99999998], b = 2.9999999830509347, loss = 1.4140418217852444\n",
      "Iteration 544: w = [1.99999998 2.99999998], b = 2.9999999830512025, loss = 0.01790644510256672\n",
      "Iteration 545: w = [1.99999998 2.99999998], b = 2.9999999830463926, loss = 5.784115965956383\n",
      "Iteration 546: w = [1.99999998 2.99999998], b = 2.9999999830451243, loss = 0.4021774290860218\n",
      "Iteration 547: w = [1.99999998 2.99999998], b = 2.9999999830497974, loss = 5.459718490248444\n",
      "Iteration 548: w = [1.99999998 2.99999998], b = 2.99999998303981, loss = 24.93847134983233\n",
      "Iteration 549: w = [1.99999998 2.99999998], b = 2.999999983025892, loss = 48.426315615689084\n",
      "Iteration 550: w = [1.99999998 2.99999998], b = 2.9999999830070907, loss = 88.37311140187845\n",
      "Iteration 551: w = [1.99999998 2.99999998], b = 2.9999999830031174, loss = 3.9470873677704104\n",
      "Iteration 552: w = [1.99999998 2.99999998], b = 2.9999999829947055, loss = 17.6905869899607\n",
      "Iteration 553: w = [1.99999998 2.99999998], b = 2.9999999829984585, loss = 3.521150469854848\n",
      "Iteration 554: w = [1.99999998 2.99999998], b = 2.9999999829925446, loss = 8.743928864221866\n",
      "Iteration 555: w = [1.99999998 2.99999998], b = 2.9999999829693667, loss = 134.30390709980875\n",
      "Iteration 556: w = [1.99999998 2.99999998], b = 2.99999998295853, loss = 29.35885856745031\n",
      "Iteration 557: w = [1.99999998 2.99999998], b = 2.9999999829424193, loss = 64.88970065074454\n",
      "Iteration 558: w = [1.99999998 2.99999998], b = 2.9999999829444475, loss = 1.0285173722845968\n",
      "Iteration 559: w = [1.99999998 2.99999998], b = 2.9999999829294866, loss = 55.95646209560017\n",
      "Iteration 560: w = [1.99999998 2.99999998], b = 2.999999982925069, loss = 4.878528142570743\n",
      "Iteration 561: w = [1.99999998 2.99999998], b = 2.9999999829151083, loss = 24.80485718598084\n",
      "Iteration 562: w = [1.99999998 2.99999998], b = 2.999999982916566, loss = 0.5314621892005488\n",
      "Iteration 563: w = [1.99999998 2.99999998], b = 2.9999999829155084, loss = 0.2798496271795498\n",
      "Iteration 564: w = [1.99999998 2.99999998], b = 2.9999999829172923, loss = 0.7954782037758201\n",
      "Iteration 565: w = [1.99999998 2.99999998], b = 2.9999999829202384, loss = 2.1698569755979333\n",
      "Iteration 566: w = [1.99999998 2.99999998], b = 2.999999982909574, loss = 28.431874215779256\n",
      "Iteration 567: w = [1.99999998 2.99999998], b = 2.999999982903425, loss = 9.451894801109416\n",
      "Iteration 568: w = [1.99999998 2.99999998], b = 2.9999999829073802, loss = 3.9107183565822075\n",
      "Iteration 569: w = [1.99999998 2.99999998], b = 2.999999982906864, loss = 0.06653108277754291\n",
      "Iteration 570: w = [1.99999998 2.99999998], b = 2.999999982899992, loss = 11.806317034779484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x168d2ad20>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize w, b, epochs, alpha\n",
    "w = np.array([2, 3])\n",
    "b = 3\n",
    "epochs = 5\n",
    "alpha = 0.000000000001\n",
    "losses = []\n",
    "for i in range(epochs):\n",
    "    print(f'Epoch: {i+1}')\n",
    "    for item in range(len(df)):\n",
    "        # 1. Pick samples\n",
    "        x = np.array([df['Radio'][item], df['Social Media'][item]])\n",
    "        y = df['Sales'][item]\n",
    "        \n",
    "        # 2. Calculate y_hat\n",
    "        y_hat = linear_function(w, x, b)\n",
    "\n",
    "        # 3. Calculate loss\n",
    "        loss = loss_function(y, y_hat)\n",
    "        losses.append(loss)\n",
    "\n",
    "        # 4. Calculate gradients/derivatives\n",
    "        dLdw = 2*(y_hat - y)*x\n",
    "        dLdb = 2*(y_hat - y)\n",
    "\n",
    "        # 5. Update weights\n",
    "        w, b = gradient_descent(y, y_hat, x, w, b, alpha, dLdw, dLdb)\n",
    "        \n",
    "        print(f'Iteration {item+1}: w = {w}, b = {b}, loss = {loss}')\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini-batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[['Radio', 'Social Media']]\n",
    "y = data['Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142.5"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n",
    "570/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Iteration 0: w = [5. 7.], b = 4.000000000018195, loss = 83.85403593719326\n",
      "Iteration 3: w = [5. 7.], b = 3.9999999999792477, loss = 402.5466247211566\n",
      "Iteration 6: w = [5. 7.], b = 3.9999999999593707, loss = 197.34352453935915\n",
      "Iteration 9: w = [5. 7.], b = 3.999999999970128, loss = 52.91649850662\n",
      "Iteration 12: w = [5. 7.], b = 3.999999999946014, loss = 230.17109218401333\n",
      "Iteration 15: w = [5. 7.], b = 3.9999999999255724, loss = 486.7960635324032\n",
      "Iteration 18: w = [5. 7.], b = 3.9999999999069438, loss = 138.70151021337747\n",
      "Iteration 21: w = [5. 7.], b = 3.999999999899657, loss = 38.52175095278819\n",
      "Iteration 24: w = [5. 7.], b = 3.999999999886302, loss = 121.96657142134707\n",
      "Iteration 27: w = [5. 7.], b = 3.999999999872412, loss = 49.59024410409731\n",
      "Iteration 30: w = [5. 7.], b = 3.9999999998656817, loss = 27.056816408919516\n",
      "Iteration 33: w = [5. 7.], b = 3.999999999856851, loss = 32.40399614323293\n",
      "Iteration 36: w = [5. 7.], b = 3.999999999865689, loss = 63.20386445720583\n",
      "Iteration 39: w = [5. 7.], b = 3.9999999998555076, loss = 42.22048074340811\n",
      "Iteration 42: w = [5. 7.], b = 3.999999999847118, loss = 91.58721295780298\n",
      "Iteration 45: w = [5. 7.], b = 3.9999999998344995, loss = 154.88755214614847\n",
      "Iteration 48: w = [5. 7.], b = 3.9999999998056097, loss = 235.81646905647162\n",
      "Iteration 51: w = [5. 7.], b = 3.9999999997915245, loss = 113.52436823553239\n",
      "Iteration 54: w = [5. 7.], b = 3.999999999798678, loss = 33.216291413704035\n",
      "Iteration 57: w = [5. 7.], b = 3.9999999997814784, loss = 88.78680056410302\n",
      "Iteration 60: w = [5. 7.], b = 3.999999999778706, loss = 65.34406371581531\n",
      "Iteration 63: w = [5. 7.], b = 3.999999999769801, loss = 125.93335321358649\n",
      "Iteration 66: w = [5. 7.], b = 3.9999999997820592, loss = 44.823812335130434\n",
      "Iteration 69: w = [5. 7.], b = 3.9999999997807576, loss = 28.201607943334853\n",
      "Iteration 72: w = [5. 7.], b = 3.999999999774557, loss = 59.01557313148521\n",
      "Iteration 75: w = [5. 7.], b = 3.999999999757249, loss = 75.22740453352192\n",
      "Iteration 78: w = [5. 7.], b = 3.9999999997612212, loss = 108.7582162231224\n",
      "Iteration 81: w = [5. 7.], b = 3.999999999761145, loss = 106.07549888687107\n",
      "Iteration 84: w = [5. 7.], b = 3.999999999762031, loss = 48.36443605282474\n",
      "Iteration 87: w = [5. 7.], b = 3.9999999997743183, loss = 53.283900917836654\n",
      "Iteration 90: w = [5. 7.], b = 3.9999999997448494, loss = 253.8120163141053\n",
      "Iteration 93: w = [5. 7.], b = 3.999999999742069, loss = 11.11149178735161\n",
      "Iteration 96: w = [5. 7.], b = 3.9999999997319704, loss = 84.05473227771485\n",
      "Iteration 99: w = [5. 7.], b = 3.9999999997417452, loss = 25.637787805828932\n",
      "Iteration 102: w = [5. 7.], b = 3.9999999997303375, loss = 115.94617604509956\n",
      "Iteration 105: w = [5. 7.], b = 3.999999999725366, loss = 76.85727376283175\n",
      "Iteration 108: w = [5. 7.], b = 3.999999999734468, loss = 24.30492415262489\n",
      "Iteration 111: w = [5. 7.], b = 3.9999999997237365, loss = 72.63961598630205\n",
      "Iteration 114: w = [5. 7.], b = 3.999999999715379, loss = 93.05408032915032\n",
      "Iteration 117: w = [5. 7.], b = 3.999999999697649, loss = 258.41164555429543\n",
      "Iteration 120: w = [5. 7.], b = 3.9999999996909654, loss = 189.42962580529448\n",
      "Iteration 123: w = [5. 7.], b = 3.9999999996951456, loss = 35.26902602661971\n",
      "Iteration 126: w = [5. 7.], b = 3.999999999676956, loss = 94.54465206187409\n",
      "Iteration 129: w = [5. 7.], b = 3.999999999688134, loss = 48.83027290339973\n",
      "Iteration 132: w = [5. 7.], b = 3.9999999996854902, loss = 23.89803528182443\n",
      "Iteration 135: w = [5. 7.], b = 3.9999999996722186, loss = 174.4250320155687\n",
      "Iteration 138: w = [5. 7.], b = 3.999999999664427, loss = 59.81784072354805\n",
      "Iteration 141: w = [5. 7.], b = 3.9999999996684803, loss = 25.927287217352895\n",
      "Iteration 144: w = [5. 7.], b = 3.999999999651027, loss = 300.66651703954534\n",
      "Iteration 147: w = [5. 7.], b = 3.9999999996515845, loss = 38.09977995046745\n",
      "Iteration 150: w = [5. 7.], b = 3.999999999651517, loss = 152.32540082342473\n",
      "Iteration 153: w = [5. 7.], b = 3.999999999634981, loss = 158.9548301924468\n",
      "Iteration 156: w = [5. 7.], b = 3.9999999996193814, loss = 82.80929846351493\n",
      "Iteration 159: w = [5. 7.], b = 3.9999999996020974, loss = 90.81209515662643\n",
      "Iteration 162: w = [5. 7.], b = 3.999999999581046, loss = 119.0931464732417\n",
      "Iteration 165: w = [5. 7.], b = 3.999999999576721, loss = 14.400319651871149\n",
      "Iteration 168: w = [5. 7.], b = 3.9999999995779483, loss = 32.87757557849404\n",
      "Iteration 171: w = [5. 7.], b = 3.999999999570922, loss = 113.33655486430403\n",
      "Iteration 174: w = [5. 7.], b = 3.9999999995608286, loss = 59.5949849954636\n",
      "Iteration 177: w = [5. 7.], b = 3.9999999995493516, loss = 131.99510041362785\n",
      "Iteration 180: w = [5. 7.], b = 3.9999999995494213, loss = 203.90349466169118\n",
      "Iteration 183: w = [5. 7.], b = 3.999999999531118, loss = 133.72723546367664\n",
      "Iteration 186: w = [5. 7.], b = 3.9999999994958495, loss = 595.2019656537592\n",
      "Iteration 189: w = [5. 7.], b = 3.9999999994867905, loss = 197.06042500454214\n",
      "Iteration 192: w = [5. 7.], b = 3.999999999488248, loss = 4.136901898776775\n",
      "Iteration 195: w = [5. 7.], b = 3.999999999476594, loss = 362.3972783453246\n",
      "Iteration 198: w = [5. 7.], b = 3.9999999994697464, loss = 67.06673149071291\n",
      "Iteration 201: w = [5. 7.], b = 3.999999999454519, loss = 58.73339978740311\n",
      "Iteration 204: w = [5. 7.], b = 3.9999999994568505, loss = 55.523080344994774\n",
      "Iteration 207: w = [5. 7.], b = 3.9999999994429363, loss = 107.77729822453482\n",
      "Iteration 210: w = [5. 7.], b = 3.9999999994366804, loss = 52.651348889209125\n",
      "Iteration 213: w = [5. 7.], b = 3.9999999994120206, loss = 182.6055489117995\n",
      "Iteration 216: w = [5. 7.], b = 3.999999999412047, loss = 64.83561057780216\n",
      "Iteration 219: w = [5. 7.], b = 3.9999999993986255, loss = 49.22351647101141\n",
      "Iteration 222: w = [5. 7.], b = 3.999999999367947, loss = 359.73587609235915\n",
      "Iteration 225: w = [5. 7.], b = 3.9999999993619606, loss = 23.27867290728958\n",
      "Iteration 228: w = [5. 7.], b = 3.9999999993583386, loss = 20.295628874358933\n",
      "Iteration 231: w = [5. 7.], b = 3.9999999993289923, loss = 216.45509040381168\n",
      "Iteration 234: w = [5. 7.], b = 3.99999999932446, loss = 79.6128556021378\n",
      "Iteration 237: w = [5. 7.], b = 3.9999999993394595, loss = 77.61336103913204\n",
      "Iteration 240: w = [5. 7.], b = 3.9999999993571533, loss = 79.96515253425436\n",
      "Iteration 243: w = [5. 7.], b = 3.999999999326497, loss = 272.8547206128548\n",
      "Iteration 246: w = [5. 7.], b = 3.9999999993262314, loss = 13.550812331099435\n",
      "Iteration 249: w = [5. 7.], b = 3.999999999331188, loss = 32.705271902594795\n",
      "Iteration 252: w = [5. 7.], b = 3.999999999334332, loss = 41.23861926933344\n",
      "Iteration 255: w = [5. 7.], b = 3.9999999993351203, loss = 32.53803551665727\n",
      "Iteration 258: w = [5. 7.], b = 3.9999999993401785, loss = 25.024190972795452\n",
      "Iteration 261: w = [5. 7.], b = 3.9999999993159783, loss = 188.4376975490542\n",
      "Iteration 264: w = [5. 7.], b = 3.9999999993298356, loss = 65.7741060884125\n",
      "Iteration 267: w = [5. 7.], b = 3.999999999325643, loss = 110.37671210168914\n",
      "Iteration 270: w = [5. 7.], b = 3.9999999993090163, loss = 103.02660027236396\n",
      "Iteration 273: w = [5. 7.], b = 3.999999999300806, loss = 48.23756151996445\n",
      "Iteration 276: w = [5. 7.], b = 3.999999999305781, loss = 21.88486081187415\n",
      "Iteration 279: w = [5. 7.], b = 3.999999999277834, loss = 333.2698283103361\n",
      "Iteration 282: w = [5. 7.], b = 3.9999999992785145, loss = 33.202000138883925\n",
      "Iteration 285: w = [5. 7.], b = 3.9999999992649897, loss = 166.0600981387572\n",
      "Iteration 288: w = [5. 7.], b = 3.9999999992678883, loss = 15.458911045851444\n",
      "Iteration 291: w = [5. 7.], b = 3.9999999992569926, loss = 70.60845083838892\n",
      "Iteration 294: w = [5. 7.], b = 3.9999999992590825, loss = 20.209451671509328\n",
      "Iteration 297: w = [5. 7.], b = 3.999999999225483, loss = 501.88810726275284\n",
      "Iteration 300: w = [5. 7.], b = 3.9999999992267736, loss = 25.545807335358376\n",
      "Iteration 303: w = [5. 7.], b = 3.999999999205128, loss = 160.1863641974266\n",
      "Iteration 306: w = [5. 7.], b = 3.9999999992098707, loss = 81.08442949152727\n",
      "Iteration 309: w = [5. 7.], b = 3.9999999991956474, loss = 96.87673051155076\n",
      "Iteration 312: w = [5. 7.], b = 3.999999999178308, loss = 75.18555774259309\n",
      "Iteration 315: w = [5. 7.], b = 3.9999999991579416, loss = 206.7927004564816\n",
      "Iteration 318: w = [5. 7.], b = 3.9999999991370605, loss = 399.946450197366\n",
      "Iteration 321: w = [5. 7.], b = 3.999999999128361, loss = 26.721781195329516\n",
      "Iteration 324: w = [5. 7.], b = 3.9999999991157233, loss = 47.49878683544756\n",
      "Iteration 327: w = [5. 7.], b = 3.999999999111544, loss = 50.861167988052124\n",
      "Iteration 330: w = [5. 7.], b = 3.9999999991104516, loss = 94.49531761106527\n",
      "Iteration 333: w = [5. 7.], b = 3.999999999123726, loss = 55.35605236643695\n",
      "Iteration 336: w = [5. 7.], b = 3.9999999990946353, loss = 250.84965703544069\n",
      "Iteration 339: w = [5. 7.], b = 3.9999999990690767, loss = 308.09210160660564\n",
      "Iteration 342: w = [5. 7.], b = 3.999999999045279, loss = 144.43353472015755\n",
      "Iteration 345: w = [5. 7.], b = 3.9999999990644786, loss = 99.16964167630881\n",
      "Iteration 348: w = [5. 7.], b = 3.9999999990427986, loss = 125.86384722680435\n",
      "Iteration 351: w = [5. 7.], b = 3.9999999990301736, loss = 68.1414387650039\n",
      "Iteration 354: w = [5. 7.], b = 3.9999999990115924, loss = 190.41387195550263\n",
      "Iteration 357: w = [5. 7.], b = 3.9999999990099595, loss = 39.89863274445842\n",
      "Iteration 360: w = [5. 7.], b = 3.999999999006545, loss = 119.55929589664096\n",
      "Iteration 363: w = [5. 7.], b = 3.9999999990086827, loss = 34.4684928828732\n",
      "Iteration 366: w = [5. 7.], b = 3.999999999016781, loss = 21.784263247166166\n",
      "Iteration 369: w = [5. 7.], b = 3.9999999989980726, loss = 194.2470621614685\n",
      "Iteration 372: w = [5. 7.], b = 3.9999999989820516, loss = 113.50803097230589\n",
      "Iteration 375: w = [5. 7.], b = 3.999999998985176, loss = 7.699684765355941\n",
      "Iteration 378: w = [5. 7.], b = 3.9999999989728323, loss = 54.49112983661602\n",
      "Iteration 381: w = [5. 7.], b = 3.9999999989688666, loss = 79.43237243688036\n",
      "Iteration 384: w = [5. 7.], b = 3.9999999989377595, loss = 348.8485250693641\n",
      "Iteration 387: w = [5. 7.], b = 3.9999999989317963, loss = 133.91902184166437\n",
      "Iteration 390: w = [5. 7.], b = 3.9999999989360893, loss = 44.037430013513664\n",
      "Iteration 393: w = [5. 7.], b = 3.999999998903707, loss = 264.770380146322\n",
      "Iteration 396: w = [5. 7.], b = 3.9999999988828963, loss = 139.14292567974977\n",
      "Iteration 399: w = [5. 7.], b = 3.999999998876645, loss = 58.468287097773704\n",
      "Iteration 402: w = [5. 7.], b = 3.9999999988563646, loss = 163.87649884700213\n",
      "Iteration 405: w = [5. 7.], b = 3.999999998852532, loss = 138.50077921594914\n",
      "Iteration 408: w = [5. 7.], b = 3.9999999988383617, loss = 84.14489032612802\n",
      "Iteration 411: w = [5. 7.], b = 3.9999999988242463, loss = 237.7973301241875\n",
      "Iteration 414: w = [5. 7.], b = 3.9999999988298756, loss = 54.64281454108783\n",
      "Iteration 417: w = [5. 7.], b = 3.999999998817581, loss = 214.95688668467446\n",
      "Iteration 420: w = [5. 7.], b = 3.9999999988275854, loss = 40.21493922265929\n",
      "Iteration 423: w = [5. 7.], b = 3.9999999988144372, loss = 83.58319389632636\n",
      "Iteration 426: w = [5. 7.], b = 3.9999999988068056, loss = 93.59086888129632\n",
      "Iteration 429: w = [5. 7.], b = 3.9999999987986623, loss = 17.605571211590902\n",
      "Iteration 432: w = [5. 7.], b = 3.9999999987976236, loss = 209.0452876913588\n",
      "Iteration 435: w = [5. 7.], b = 3.9999999987989043, loss = 54.82669065850962\n",
      "Iteration 438: w = [5. 7.], b = 3.9999999988035793, loss = 81.99060497978836\n",
      "Iteration 441: w = [5. 7.], b = 3.9999999987944825, loss = 34.15324185317389\n",
      "Iteration 444: w = [5. 7.], b = 3.9999999987919113, loss = 1.6633865503597587\n",
      "Iteration 447: w = [5. 7.], b = 3.999999998792354, loss = 31.688839776301155\n",
      "Iteration 450: w = [5. 7.], b = 3.999999998775339, loss = 123.30199210972758\n",
      "Iteration 453: w = [5. 7.], b = 3.9999999987919206, loss = 84.62997503126977\n",
      "Iteration 456: w = [5. 7.], b = 3.9999999987636516, loss = 272.7354732247705\n",
      "Iteration 459: w = [5. 7.], b = 3.9999999987326773, loss = 259.62067056594987\n",
      "Iteration 462: w = [5. 7.], b = 3.9999999987095283, loss = 147.86379120549356\n",
      "Iteration 465: w = [5. 7.], b = 3.999999998713304, loss = 8.189288939621532\n",
      "Iteration 468: w = [5. 7.], b = 3.9999999987207584, loss = 78.90993457718234\n",
      "Iteration 471: w = [5. 7.], b = 3.999999998693527, loss = 297.7577929950882\n",
      "Iteration 474: w = [5. 7.], b = 3.9999999986786907, loss = 172.03861594073848\n",
      "Iteration 477: w = [5. 7.], b = 3.9999999986810413, loss = 117.12171582422711\n",
      "Iteration 480: w = [5. 7.], b = 3.9999999986737413, loss = 29.120195816779766\n",
      "Iteration 483: w = [5. 7.], b = 3.9999999986429984, loss = 488.09921615751404\n",
      "Iteration 486: w = [5. 7.], b = 3.9999999986270995, loss = 245.07804085813677\n",
      "Iteration 489: w = [5. 7.], b = 3.999999998616945, loss = 138.30743038163692\n",
      "Iteration 492: w = [5. 7.], b = 3.999999998613325, loss = 53.62525144928244\n",
      "Iteration 495: w = [5. 7.], b = 3.9999999986216905, loss = 30.710168392213806\n",
      "Iteration 498: w = [5. 7.], b = 3.9999999986024117, loss = 226.24425113875319\n",
      "Iteration 501: w = [5. 7.], b = 3.999999998589562, loss = 60.94220113231122\n",
      "Iteration 504: w = [5. 7.], b = 3.999999998568434, loss = 252.38254103162862\n",
      "Iteration 507: w = [5. 7.], b = 3.9999999985461074, loss = 177.29607575156083\n",
      "Iteration 510: w = [5. 7.], b = 3.999999998549391, loss = 30.25255779647642\n",
      "Iteration 513: w = [5. 7.], b = 3.999999998550026, loss = 1.1522088894589593\n",
      "Iteration 516: w = [5. 7.], b = 3.9999999985445385, loss = 63.84175489538328\n",
      "Iteration 519: w = [5. 7.], b = 3.9999999985351575, loss = 86.77353727128688\n",
      "Iteration 522: w = [5. 7.], b = 3.999999998527204, loss = 94.78937919664283\n",
      "Iteration 525: w = [5. 7.], b = 3.999999998522959, loss = 185.3904051182068\n",
      "Iteration 528: w = [5. 7.], b = 3.999999998524632, loss = 65.61512785467154\n",
      "Iteration 531: w = [5. 7.], b = 3.999999998516665, loss = 166.3897633996924\n",
      "Iteration 534: w = [5. 7.], b = 3.9999999985080654, loss = 107.78321556926407\n",
      "Iteration 537: w = [5. 7.], b = 3.999999998527158, loss = 96.23615794662537\n",
      "Iteration 540: w = [5. 7.], b = 3.99999999854097, loss = 49.16501087726623\n",
      "Iteration 543: w = [5. 7.], b = 3.999999998540749, loss = 7.068746320600265\n",
      "Iteration 546: w = [5. 7.], b = 3.999999998532354, loss = 115.32837500223643\n",
      "Iteration 549: w = [5. 7.], b = 3.9999999985124464, loss = 164.92505465714905\n",
      "Iteration 552: w = [5. 7.], b = 3.9999999984990384, loss = 269.5514831461957\n",
      "Iteration 555: w = [5. 7.], b = 3.9999999984865395, loss = 142.78659215526324\n",
      "Iteration 558: w = [5. 7.], b = 3.999999998466739, loss = 124.89297987605299\n",
      "Iteration 561: w = [5. 7.], b = 3.9999999984791765, loss = 42.032325486121\n",
      "Iteration 564: w = [5. 7.], b = 3.999999998474298, loss = 78.17713368076119\n",
      "Iteration 567: w = [5. 7.], b = 3.999999998479999, loss = 43.8589195546796\n",
      "Epoch: 2\n",
      "Iteration 0: w = [5. 7.], b = 3.999999998498194, loss = 83.85403582207397\n",
      "Iteration 3: w = [5. 7.], b = 3.9999999984592467, loss = 402.5466242672258\n",
      "Iteration 6: w = [5. 7.], b = 3.9999999984393697, loss = 197.34352430923784\n",
      "Iteration 9: w = [5. 7.], b = 3.999999998450127, loss = 52.91649842870037\n",
      "Iteration 12: w = [5. 7.], b = 3.999999998426013, loss = 230.17109193450855\n",
      "Iteration 15: w = [5. 7.], b = 3.9999999984055714, loss = 486.7960629925176\n",
      "Iteration 18: w = [5. 7.], b = 3.9999999983869428, loss = 138.70151006276802\n",
      "Iteration 21: w = [5. 7.], b = 3.999999998379656, loss = 38.521750914768866\n",
      "Iteration 24: w = [5. 7.], b = 3.999999998366301, loss = 121.9665712856919\n",
      "Iteration 27: w = [5. 7.], b = 3.999999998352411, loss = 49.59024404891397\n",
      "Iteration 30: w = [5. 7.], b = 3.9999999983456807, loss = 27.056816386365067\n",
      "Iteration 33: w = [5. 7.], b = 3.99999999833685, loss = 32.40399611437498\n",
      "Iteration 36: w = [5. 7.], b = 3.999999998345688, loss = 63.2038643676606\n",
      "Iteration 39: w = [5. 7.], b = 3.9999999983355066, loss = 42.220480698259856\n",
      "Iteration 42: w = [5. 7.], b = 3.999999998327117, loss = 91.58721285100148\n",
      "Iteration 45: w = [5. 7.], b = 3.9999999983144985, loss = 154.88755196532637\n",
      "Iteration 48: w = [5. 7.], b = 3.9999999982856087, loss = 235.81646879966286\n",
      "Iteration 51: w = [5. 7.], b = 3.9999999982715235, loss = 113.52436810814713\n",
      "Iteration 54: w = [5. 7.], b = 3.999999998278677, loss = 33.21629136222423\n",
      "Iteration 57: w = [5. 7.], b = 3.9999999982614773, loss = 88.78680047670154\n",
      "Iteration 60: w = [5. 7.], b = 3.999999998258705, loss = 65.3440636405759\n",
      "Iteration 63: w = [5. 7.], b = 3.9999999982498, loss = 125.93335307573422\n",
      "Iteration 66: w = [5. 7.], b = 3.999999998262058, loss = 44.82381226844669\n",
      "Iteration 69: w = [5. 7.], b = 3.9999999982607566, loss = 28.201607912861775\n",
      "Iteration 72: w = [5. 7.], b = 3.9999999982545558, loss = 59.01557305285544\n",
      "Iteration 75: w = [5. 7.], b = 3.999999998237248, loss = 75.22740444696784\n",
      "Iteration 78: w = [5. 7.], b = 3.99999999824122, loss = 108.7582160830734\n",
      "Iteration 81: w = [5. 7.], b = 3.999999998241144, loss = 106.07549875128488\n",
      "Iteration 84: w = [5. 7.], b = 3.99999999824203, loss = 48.3644359964218\n",
      "Iteration 87: w = [5. 7.], b = 3.9999999982543173, loss = 53.28390083756947\n",
      "Iteration 90: w = [5. 7.], b = 3.9999999982248484, loss = 253.81201603538298\n",
      "Iteration 93: w = [5. 7.], b = 3.999999998222068, loss = 11.111491777144025\n",
      "Iteration 96: w = [5. 7.], b = 3.9999999982119694, loss = 84.05473217359021\n",
      "Iteration 99: w = [5. 7.], b = 3.9999999982217442, loss = 25.63778776533611\n",
      "Iteration 102: w = [5. 7.], b = 3.9999999982103365, loss = 115.94617592768277\n",
      "Iteration 105: w = [5. 7.], b = 3.999999998205365, loss = 76.85727366720538\n",
      "Iteration 108: w = [5. 7.], b = 3.999999998214467, loss = 24.30492412147365\n",
      "Iteration 111: w = [5. 7.], b = 3.9999999982037355, loss = 72.63961591473254\n",
      "Iteration 114: w = [5. 7.], b = 3.9999999981953778, loss = 93.05408022624123\n",
      "Iteration 117: w = [5. 7.], b = 3.999999998177648, loss = 258.4116452624122\n",
      "Iteration 120: w = [5. 7.], b = 3.9999999981709644, loss = 189.4296255889714\n",
      "Iteration 123: w = [5. 7.], b = 3.9999999981751446, loss = 35.2690259809869\n",
      "Iteration 126: w = [5. 7.], b = 3.999999998156955, loss = 94.54465195247877\n",
      "Iteration 129: w = [5. 7.], b = 3.999999998168133, loss = 48.830272843166306\n",
      "Iteration 132: w = [5. 7.], b = 3.9999999981654892, loss = 23.898035254192767\n",
      "Iteration 135: w = [5. 7.], b = 3.9999999981522176, loss = 174.42503181689813\n",
      "Iteration 138: w = [5. 7.], b = 3.999999998144426, loss = 59.817840658629166\n",
      "Iteration 141: w = [5. 7.], b = 3.9999999981484793, loss = 25.927287189447345\n",
      "Iteration 144: w = [5. 7.], b = 3.999999998131026, loss = 300.6665166875867\n",
      "Iteration 147: w = [5. 7.], b = 3.9999999981315835, loss = 38.09977990032983\n",
      "Iteration 150: w = [5. 7.], b = 3.999999998131516, loss = 152.3254006427573\n",
      "Iteration 153: w = [5. 7.], b = 3.99999999811498, loss = 158.95483001723417\n",
      "Iteration 156: w = [5. 7.], b = 3.9999999980993803, loss = 82.80929838378545\n",
      "Iteration 159: w = [5. 7.], b = 3.9999999980820964, loss = 90.81209505853333\n",
      "Iteration 162: w = [5. 7.], b = 3.999999998061045, loss = 119.09314633624761\n",
      "Iteration 165: w = [5. 7.], b = 3.99999999805672, loss = 14.400319636365742\n",
      "Iteration 168: w = [5. 7.], b = 3.9999999980579473, loss = 32.877575550342584\n",
      "Iteration 171: w = [5. 7.], b = 3.999999998050921, loss = 113.33655474877689\n",
      "Iteration 174: w = [5. 7.], b = 3.9999999980408276, loss = 59.594984936205485\n",
      "Iteration 177: w = [5. 7.], b = 3.9999999980293506, loss = 131.9951002778167\n",
      "Iteration 180: w = [5. 7.], b = 3.9999999980294203, loss = 203.90349442089575\n",
      "Iteration 183: w = [5. 7.], b = 3.999999998011117, loss = 133.72723531592794\n",
      "Iteration 186: w = [5. 7.], b = 3.9999999979758485, loss = 595.2019649742056\n",
      "Iteration 189: w = [5. 7.], b = 3.9999999979667895, loss = 197.06042477504025\n",
      "Iteration 192: w = [5. 7.], b = 3.999999997968247, loss = 4.136901891516244\n",
      "Iteration 195: w = [5. 7.], b = 3.999999997956593, loss = 362.3972779451257\n",
      "Iteration 198: w = [5. 7.], b = 3.9999999979497454, loss = 67.06673141632072\n",
      "Iteration 201: w = [5. 7.], b = 3.999999997934518, loss = 58.73339973870717\n",
      "Iteration 204: w = [5. 7.], b = 3.9999999979368495, loss = 55.52308027526187\n",
      "Iteration 207: w = [5. 7.], b = 3.9999999979229353, loss = 107.77729809531532\n",
      "Iteration 210: w = [5. 7.], b = 3.9999999979166794, loss = 52.651348831491134\n",
      "Iteration 213: w = [5. 7.], b = 3.9999999978920195, loss = 182.60554870687827\n",
      "Iteration 216: w = [5.         6.99999999], b = 3.999999997892046, loss = 64.83561050496299\n",
      "Iteration 219: w = [5.         6.99999999], b = 3.9999999978786245, loss = 49.22351642760623\n",
      "Iteration 222: w = [5.         6.99999999], b = 3.999999997847946, loss = 359.73587568012766\n",
      "Iteration 225: w = [5.         6.99999999], b = 3.9999999978419596, loss = 23.278672890203435\n",
      "Iteration 228: w = [5.         6.99999999], b = 3.9999999978383376, loss = 20.2956288517634\n",
      "Iteration 231: w = [5.         6.99999999], b = 3.9999999978089913, loss = 216.45509016240283\n",
      "Iteration 234: w = [5.         6.99999999], b = 3.999999997804459, loss = 79.61285551263708\n",
      "Iteration 237: w = [5.         6.99999999], b = 3.9999999978194585, loss = 77.61336092937161\n",
      "Iteration 240: w = [5.         6.99999999], b = 3.9999999978371523, loss = 79.96515242005964\n",
      "Iteration 243: w = [5.         6.99999999], b = 3.999999997806496, loss = 272.854720318332\n",
      "Iteration 246: w = [5.         6.99999999], b = 3.9999999978062304, loss = 13.55081232077193\n",
      "Iteration 249: w = [5.         6.99999999], b = 3.999999997811187, loss = 32.70527185324003\n",
      "Iteration 252: w = [5.         6.99999999], b = 3.999999997814331, loss = 41.23861920991144\n",
      "Iteration 255: w = [5.         6.99999999], b = 3.9999999978151193, loss = 32.538035482580305\n",
      "Iteration 258: w = [5.         6.99999999], b = 3.9999999978201775, loss = 25.024190938206377\n",
      "Iteration 261: w = [5.         6.99999999], b = 3.9999999977959773, loss = 188.43769734314856\n",
      "Iteration 264: w = [5.         6.99999999], b = 3.9999999978098346, loss = 65.77410599373023\n",
      "Iteration 267: w = [5.         6.99999999], b = 3.999999997805642, loss = 110.37671197150371\n",
      "Iteration 270: w = [5.         6.99999999], b = 3.9999999977890153, loss = 103.02660015980972\n",
      "Iteration 273: w = [5.         6.99999999], b = 3.999999997780805, loss = 48.2375614566574\n",
      "Iteration 276: w = [5.         6.99999999], b = 3.99999999778578, loss = 21.884860781116643\n",
      "Iteration 279: w = [5.         6.99999999], b = 3.999999997757833, loss = 333.26982794503874\n",
      "Iteration 282: w = [5.         6.99999999], b = 3.9999999977585134, loss = 33.20200009397994\n",
      "Iteration 285: w = [5.         6.99999999], b = 3.9999999977449887, loss = 166.06009794438637\n",
      "Iteration 288: w = [5.         6.99999999], b = 3.9999999977478873, loss = 15.458911021884033\n",
      "Iteration 291: w = [5.         6.99999999], b = 3.9999999977369916, loss = 70.60845075314808\n",
      "Iteration 294: w = [5.         6.99999999], b = 3.9999999977390814, loss = 20.20945164463371\n",
      "Iteration 297: w = [5.         6.99999999], b = 3.999999997705482, loss = 501.88810670822426\n",
      "Iteration 300: w = [5.         6.99999999], b = 3.9999999977067726, loss = 25.545807307157805\n",
      "Iteration 303: w = [5.         6.99999999], b = 3.999999997685127, loss = 160.18636401511617\n",
      "Iteration 306: w = [5.         6.99999999], b = 3.9999999976898697, loss = 81.08442938851024\n",
      "Iteration 309: w = [5.         6.99999999], b = 3.9999999976756464, loss = 96.87673039311755\n",
      "Iteration 312: w = [5.         6.99999999], b = 3.999999997658307, loss = 75.18555767025605\n",
      "Iteration 315: w = [5.         6.99999999], b = 3.9999999976379406, loss = 206.7927002263035\n",
      "Iteration 318: w = [5.         6.99999999], b = 3.9999999976170595, loss = 399.94644972697625\n",
      "Iteration 321: w = [5.         6.99999999], b = 3.99999999760836, loss = 26.721781171582492\n",
      "Iteration 324: w = [5.         6.99999999], b = 3.9999999975957223, loss = 47.49878678239964\n",
      "Iteration 327: w = [5.         6.99999999], b = 3.999999997591543, loss = 50.86116793701772\n",
      "Iteration 330: w = [4.99999999 6.99999999], b = 3.9999999975904506, loss = 94.49531749014733\n",
      "Iteration 333: w = [4.99999999 6.99999999], b = 3.999999997603725, loss = 55.35605228864993\n",
      "Iteration 336: w = [4.99999999 6.99999999], b = 3.9999999975746343, loss = 250.84965675649286\n",
      "Iteration 339: w = [4.99999999 6.99999999], b = 3.9999999975490756, loss = 308.0921012540432\n",
      "Iteration 342: w = [4.99999999 6.99999999], b = 3.999999997525278, loss = 144.43353457935726\n",
      "Iteration 345: w = [4.99999999 6.99999999], b = 3.9999999975444775, loss = 99.16964152866025\n",
      "Iteration 348: w = [4.99999999 6.99999999], b = 3.9999999975227976, loss = 125.86384710013822\n",
      "Iteration 351: w = [4.99999999 6.99999999], b = 3.9999999975101725, loss = 68.1414386946414\n",
      "Iteration 354: w = [4.99999999 6.99999999], b = 3.9999999974915914, loss = 190.4138717276602\n",
      "Iteration 357: w = [4.99999999 6.99999999], b = 3.9999999974899585, loss = 39.89863270279286\n",
      "Iteration 360: w = [4.99999999 6.99999999], b = 3.999999997486544, loss = 119.55929574647763\n",
      "Iteration 363: w = [4.99999999 6.99999999], b = 3.9999999974886817, loss = 34.468492840844384\n",
      "Iteration 366: w = [4.99999999 6.99999999], b = 3.99999999749678, loss = 21.784263213730863\n",
      "Iteration 369: w = [4.99999999 6.99999999], b = 3.9999999974780716, loss = 194.2470619570622\n",
      "Iteration 372: w = [4.99999999 6.99999999], b = 3.9999999974620506, loss = 113.50803085763904\n",
      "Iteration 375: w = [4.99999999 6.99999999], b = 3.999999997465175, loss = 7.699684750148623\n",
      "Iteration 378: w = [4.99999999 6.99999999], b = 3.9999999974528313, loss = 54.49112977730953\n",
      "Iteration 381: w = [4.99999999 6.99999999], b = 3.9999999974488656, loss = 79.43237234981954\n",
      "Iteration 384: w = [4.99999999 6.99999999], b = 3.9999999974177585, loss = 348.8485246739158\n",
      "Iteration 387: w = [4.99999999 6.99999999], b = 3.9999999974117952, loss = 133.91902168587885\n",
      "Iteration 390: w = [4.99999999 6.99999999], b = 3.9999999974160882, loss = 44.03742995458585\n",
      "Iteration 393: w = [4.99999999 6.99999999], b = 3.999999997383706, loss = 264.77037984268844\n",
      "Iteration 396: w = [4.99999999 6.99999999], b = 3.9999999973628952, loss = 139.14292552800214\n",
      "Iteration 399: w = [4.99999999 6.99999999], b = 3.999999997356644, loss = 58.46828702831615\n",
      "Iteration 402: w = [4.99999999 6.99999999], b = 3.9999999973363636, loss = 163.87649865937027\n",
      "Iteration 405: w = [4.99999999 6.99999999], b = 3.999999997332531, loss = 138.50077905178918\n",
      "Iteration 408: w = [4.99999999 6.99999999], b = 3.9999999973183606, loss = 84.14489023306645\n",
      "Iteration 411: w = [4.99999999 6.99999999], b = 3.9999999973042453, loss = 237.79732985641115\n",
      "Iteration 414: w = [4.99999999 6.99999999], b = 3.9999999973098745, loss = 54.64281446585253\n",
      "Iteration 417: w = [4.99999999 6.99999999], b = 3.99999999729758, loss = 214.95688644810954\n",
      "Iteration 420: w = [4.99999999 6.99999999], b = 3.9999999973075844, loss = 40.21493916314677\n",
      "Iteration 423: w = [4.99999999 6.99999999], b = 3.9999999972944362, loss = 83.58319381658673\n",
      "Iteration 426: w = [4.99999999 6.99999999], b = 3.9999999972868046, loss = 93.59086877263046\n",
      "Iteration 429: w = [4.99999999 6.99999999], b = 3.9999999972786613, loss = 17.605571191221852\n",
      "Iteration 432: w = [4.99999999 6.99999999], b = 3.9999999972776226, loss = 209.0452874487245\n",
      "Iteration 435: w = [4.99999999 6.99999999], b = 3.9999999972789033, loss = 54.82669058743149\n",
      "Iteration 438: w = [4.99999999 6.99999999], b = 3.9999999972835782, loss = 81.99060487550801\n",
      "Iteration 441: w = [4.99999999 6.99999999], b = 3.9999999972744815, loss = 34.15324181301322\n",
      "Iteration 444: w = [4.99999999 6.99999999], b = 3.9999999972719102, loss = 1.6633865506619845\n",
      "Iteration 447: w = [4.99999999 6.99999999], b = 3.999999997272353, loss = 31.68883973587802\n",
      "Iteration 450: w = [4.99999999 6.99999999], b = 3.999999997255338, loss = 123.30199196027094\n",
      "Iteration 453: w = [4.99999999 6.99999999], b = 3.9999999972719196, loss = 84.62997491066801\n",
      "Iteration 456: w = [4.99999999 6.99999999], b = 3.9999999972436506, loss = 272.73547292694576\n",
      "Iteration 459: w = [4.99999999 6.99999999], b = 3.9999999972126763, loss = 259.6206702800699\n",
      "Iteration 462: w = [4.99999999 6.99999999], b = 3.9999999971895273, loss = 147.8637910493405\n",
      "Iteration 465: w = [4.99999999 6.99999999], b = 3.999999997193303, loss = 8.189288929237911\n",
      "Iteration 468: w = [4.99999999 6.99999999], b = 3.9999999972007574, loss = 78.909934472558\n",
      "Iteration 471: w = [4.99999999 6.99999999], b = 3.999999997173526, loss = 297.7577926676484\n",
      "Iteration 474: w = [4.99999999 6.99999999], b = 3.9999999971586897, loss = 172.03861574245533\n",
      "Iteration 477: w = [4.99999999 6.99999999], b = 3.9999999971610403, loss = 117.12171566622841\n",
      "Iteration 480: w = [4.99999999 6.99999999], b = 3.9999999971537403, loss = 29.12019578450789\n",
      "Iteration 483: w = [4.99999999 6.99999999], b = 3.9999999971229974, loss = 488.0992156056179\n",
      "Iteration 486: w = [4.99999999 6.99999999], b = 3.9999999971070985, loss = 245.07804058523934\n",
      "Iteration 489: w = [4.99999999 6.99999999], b = 3.999999997096944, loss = 138.3074302170679\n",
      "Iteration 492: w = [4.99999999 6.99999999], b = 3.9999999970933238, loss = 53.62525138787649\n",
      "Iteration 495: w = [4.99999999 6.99999999], b = 3.9999999971016895, loss = 30.71016835418402\n",
      "Iteration 498: w = [4.99999999 6.99999999], b = 3.9999999970824107, loss = 226.24425088217427\n",
      "Iteration 501: w = [4.99999999 6.99999999], b = 3.999999997069561, loss = 60.942201071547316\n",
      "Iteration 504: w = [4.99999999 6.99999999], b = 3.999999997048433, loss = 252.38254077430972\n",
      "Iteration 507: w = [4.99999999 6.99999999], b = 3.9999999970261064, loss = 177.2960755518252\n",
      "Iteration 510: w = [4.99999999 6.99999999], b = 3.99999999702939, loss = 30.252557752256667\n",
      "Iteration 513: w = [4.99999999 6.99999999], b = 3.999999997030025, loss = 1.1522088868768903\n",
      "Iteration 516: w = [4.99999999 6.99999999], b = 3.9999999970245375, loss = 63.841754823933684\n",
      "Iteration 519: w = [4.99999999 6.99999999], b = 3.9999999970151565, loss = 86.77353717995645\n",
      "Iteration 522: w = [4.99999999 6.99999999], b = 3.999999997007203, loss = 94.78937910174915\n",
      "Iteration 525: w = [4.99999999 6.99999999], b = 3.999999997002958, loss = 185.39040489570232\n",
      "Iteration 528: w = [4.99999999 6.99999999], b = 3.999999997004631, loss = 65.6151277654384\n",
      "Iteration 531: w = [4.99999999 6.99999999], b = 3.999999996996664, loss = 166.38976320837304\n",
      "Iteration 534: w = [4.99999999 6.99999999], b = 3.9999999969880644, loss = 107.78321544503974\n",
      "Iteration 537: w = [4.99999999 6.99999999], b = 3.999999997007157, loss = 96.23615780572804\n",
      "Iteration 540: w = [4.99999999 6.99999999], b = 3.999999997020969, loss = 49.16501080757987\n",
      "Iteration 543: w = [4.99999999 6.99999999], b = 3.999999997020748, loss = 7.0687463117728875\n",
      "Iteration 546: w = [4.99999999 6.99999999], b = 3.999999997012353, loss = 115.32837487660272\n",
      "Iteration 549: w = [4.99999999 6.99999999], b = 3.9999999969924454, loss = 164.9250544755636\n",
      "Iteration 552: w = [4.99999999 6.99999999], b = 3.9999999969790374, loss = 269.55148283094877\n",
      "Iteration 555: w = [4.99999999 6.99999999], b = 3.9999999969665385, loss = 142.7865919991929\n",
      "Iteration 558: w = [4.99999999 6.99999999], b = 3.999999996946738, loss = 124.89297973864001\n",
      "Iteration 561: w = [4.99999999 6.99999999], b = 3.9999999969591755, loss = 42.0323254162441\n",
      "Iteration 564: w = [4.99999999 6.99999999], b = 3.999999996954297, loss = 78.17713358064566\n",
      "Iteration 567: w = [4.99999999 6.99999999], b = 3.999999996959998, loss = 43.85891949592278\n",
      "Epoch: 3\n",
      "Iteration 0: w = [4.99999999 6.99999999], b = 3.999999996978193, loss = 83.85403570695469\n",
      "Iteration 3: w = [4.99999999 6.99999999], b = 3.9999999969392457, loss = 402.5466238132951\n",
      "Iteration 6: w = [4.99999999 6.99999999], b = 3.9999999969193687, loss = 197.34352407911646\n",
      "Iteration 9: w = [4.99999999 6.99999999], b = 3.999999996930126, loss = 52.91649835078068\n",
      "Iteration 12: w = [4.99999999 6.99999999], b = 3.999999996906012, loss = 230.17109168500374\n",
      "Iteration 15: w = [4.99999999 6.99999999], b = 3.9999999968855704, loss = 486.796062452632\n",
      "Iteration 18: w = [4.99999999 6.99999999], b = 3.9999999968669417, loss = 138.7015099121586\n",
      "Iteration 21: w = [4.99999999 6.99999999], b = 3.999999996859655, loss = 38.521750876749536\n",
      "Iteration 24: w = [4.99999999 6.99999999], b = 3.9999999968463, loss = 121.96657115003676\n",
      "Iteration 27: w = [4.99999999 6.99999999], b = 3.99999999683241, loss = 49.590243993730645\n",
      "Iteration 30: w = [4.99999999 6.99999999], b = 3.9999999968256796, loss = 27.056816363810622\n",
      "Iteration 33: w = [4.99999999 6.99999999], b = 3.999999996816849, loss = 32.40399608551703\n",
      "Iteration 36: w = [4.99999999 6.99999999], b = 3.999999996825687, loss = 63.20386427811542\n",
      "Iteration 39: w = [4.99999999 6.99999999], b = 3.9999999968155056, loss = 42.22048065311158\n",
      "Iteration 42: w = [4.99999999 6.99999999], b = 3.999999996807116, loss = 91.58721274420002\n",
      "Iteration 45: w = [4.99999999 6.99999999], b = 3.9999999967944975, loss = 154.88755178450424\n",
      "Iteration 48: w = [4.99999999 6.99999999], b = 3.9999999967656077, loss = 235.81646854285415\n",
      "Iteration 51: w = [4.99999999 6.99999999], b = 3.9999999967515225, loss = 113.5243679807619\n",
      "Iteration 54: w = [4.99999999 6.99999999], b = 3.999999996758676, loss = 33.2162913107444\n",
      "Iteration 57: w = [4.99999999 6.99999999], b = 3.9999999967414763, loss = 88.78680038930007\n",
      "Iteration 60: w = [4.99999999 6.99999999], b = 3.999999996738704, loss = 65.34406356533648\n",
      "Iteration 63: w = [4.99999999 6.99999999], b = 3.999999996729799, loss = 125.9333529378819\n",
      "Iteration 66: w = [4.99999999 6.99999999], b = 3.999999996742057, loss = 44.823812201762955\n",
      "Iteration 69: w = [4.99999999 6.99999999], b = 3.9999999967407556, loss = 28.201607882388714\n",
      "Iteration 72: w = [4.99999999 6.99999999], b = 3.9999999967345548, loss = 59.01557297422564\n",
      "Iteration 75: w = [4.99999999 6.99999999], b = 3.999999996717247, loss = 75.22740436041374\n",
      "Iteration 78: w = [4.99999999 6.99999999], b = 3.999999996721219, loss = 108.75821594302441\n",
      "Iteration 81: w = [4.99999999 6.99999999], b = 3.999999996721143, loss = 106.07549861569866\n",
      "Iteration 84: w = [4.99999999 6.99999999], b = 3.9999999967220288, loss = 48.36443594001881\n",
      "Iteration 87: w = [4.99999999 6.99999999], b = 3.9999999967343163, loss = 53.28390075730233\n",
      "Iteration 90: w = [4.99999999 6.99999999], b = 3.9999999967048474, loss = 253.8120157566606\n",
      "Iteration 93: w = [4.99999999 6.99999999], b = 3.999999996702067, loss = 11.111491766936432\n",
      "Iteration 96: w = [4.99999999 6.99999999], b = 3.9999999966919684, loss = 84.05473206946556\n",
      "Iteration 99: w = [4.99999999 6.99999999], b = 3.999999996701743, loss = 25.637787724843292\n",
      "Iteration 102: w = [4.99999999 6.99999999], b = 3.9999999966903355, loss = 115.94617581026596\n",
      "Iteration 105: w = [4.99999999 6.99999999], b = 3.999999996685364, loss = 76.85727357157903\n",
      "Iteration 108: w = [4.99999999 6.99999999], b = 3.999999996694466, loss = 24.304924090322412\n",
      "Iteration 111: w = [4.99999999 6.99999999], b = 3.9999999966837345, loss = 72.63961584316307\n",
      "Iteration 114: w = [4.99999999 6.99999999], b = 3.9999999966753768, loss = 93.0540801233321\n",
      "Iteration 117: w = [4.99999999 6.99999999], b = 3.999999996657647, loss = 258.4116449705289\n",
      "Iteration 120: w = [4.99999999 6.99999999], b = 3.9999999966509634, loss = 189.42962537264836\n",
      "Iteration 123: w = [4.99999999 6.99999999], b = 3.9999999966551436, loss = 35.269025935354115\n",
      "Iteration 126: w = [4.99999999 6.99999999], b = 3.999999996636954, loss = 94.54465184308343\n",
      "Iteration 129: w = [4.99999999 6.99999999], b = 3.999999996648132, loss = 48.830272782932866\n",
      "Iteration 132: w = [4.99999999 6.99999999], b = 3.999999996645488, loss = 23.898035226561106\n",
      "Iteration 135: w = [4.99999999 6.99999999], b = 3.9999999966322166, loss = 174.42503161822762\n",
      "Iteration 138: w = [4.99999999 6.99999999], b = 3.999999996624425, loss = 59.81784059371026\n",
      "Iteration 141: w = [4.99999999 6.99999999], b = 3.9999999966284783, loss = 25.927287161541802\n",
      "Iteration 144: w = [4.99999999 6.99999999], b = 3.999999996611025, loss = 300.6665163356283\n",
      "Iteration 147: w = [4.99999999 6.99999999], b = 3.9999999966115825, loss = 38.09977985019221\n",
      "Iteration 150: w = [4.99999999 6.99999999], b = 3.999999996611515, loss = 152.32540046208987\n",
      "Iteration 153: w = [4.99999999 6.99999999], b = 3.999999996594979, loss = 158.95482984202155\n",
      "Iteration 156: w = [4.99999999 6.99999999], b = 3.9999999965793793, loss = 82.80929830405599\n",
      "Iteration 159: w = [4.99999999 6.99999999], b = 3.9999999965620954, loss = 90.81209496044026\n",
      "Iteration 162: w = [4.99999999 6.99999999], b = 3.9999999965410438, loss = 119.09314619925355\n",
      "Iteration 165: w = [4.99999999 6.99999999], b = 3.999999996536719, loss = 14.40031962086033\n",
      "Iteration 168: w = [4.99999999 6.99999999], b = 3.9999999965379462, loss = 32.87757552219113\n",
      "Iteration 171: w = [4.99999999 6.99999999], b = 3.99999999653092, loss = 113.3365546332498\n",
      "Iteration 174: w = [4.99999999 6.99999999], b = 3.9999999965208266, loss = 59.5949848769474\n",
      "Iteration 177: w = [4.99999999 6.99999999], b = 3.9999999965093496, loss = 131.99510014200558\n",
      "Iteration 180: w = [4.99999999 6.99999999], b = 3.9999999965094193, loss = 203.9034941801004\n",
      "Iteration 183: w = [4.99999999 6.99999999], b = 3.999999996491116, loss = 133.72723516817925\n",
      "Iteration 186: w = [4.99999999 6.99999999], b = 3.9999999964558475, loss = 595.2019642946517\n",
      "Iteration 189: w = [4.99999999 6.99999999], b = 3.9999999964467885, loss = 197.06042454553827\n",
      "Iteration 192: w = [4.99999999 6.99999999], b = 3.999999996448246, loss = 4.1369018842557095\n",
      "Iteration 195: w = [4.99999999 6.99999999], b = 3.9999999964365918, loss = 362.397277544927\n",
      "Iteration 198: w = [4.99999999 6.99999999], b = 3.9999999964297444, loss = 67.06673134192853\n",
      "Iteration 201: w = [4.99999999 6.99999999], b = 3.999999996414517, loss = 58.733399690011254\n",
      "Iteration 204: w = [4.99999999 6.99999999], b = 3.9999999964168484, loss = 55.52308020552898\n",
      "Iteration 207: w = [4.99999999 6.99999999], b = 3.9999999964029342, loss = 107.77729796609576\n",
      "Iteration 210: w = [4.99999999 6.99999999], b = 3.9999999963966784, loss = 52.651348773773144\n",
      "Iteration 213: w = [4.99999999 6.99999999], b = 3.9999999963720185, loss = 182.60554850195695\n",
      "Iteration 216: w = [4.99999999 6.99999999], b = 3.999999996372045, loss = 64.8356104321238\n",
      "Iteration 219: w = [4.99999999 6.99999999], b = 3.9999999963586235, loss = 49.22351638420104\n",
      "Iteration 222: w = [4.99999999 6.99999999], b = 3.999999996327945, loss = 359.73587526789606\n",
      "Iteration 225: w = [4.99999999 6.99999999], b = 3.9999999963219586, loss = 23.27867287311729\n",
      "Iteration 228: w = [4.99999999 6.99999999], b = 3.9999999963183366, loss = 20.295628829167864\n",
      "Iteration 231: w = [4.99999999 6.99999999], b = 3.9999999962889903, loss = 216.45508992099397\n",
      "Iteration 234: w = [4.99999999 6.99999999], b = 3.999999996284458, loss = 79.61285542313637\n",
      "Iteration 237: w = [4.99999999 6.99999999], b = 3.9999999962994575, loss = 77.61336081961117\n",
      "Iteration 240: w = [4.99999999 6.99999999], b = 3.9999999963171513, loss = 79.96515230586492\n",
      "Iteration 243: w = [4.99999999 6.99999999], b = 3.999999996286495, loss = 272.8547200238092\n",
      "Iteration 246: w = [4.99999999 6.99999999], b = 3.9999999962862294, loss = 13.550812310444428\n",
      "Iteration 249: w = [4.99999999 6.99999999], b = 3.999999996291186, loss = 32.70527180388527\n",
      "Iteration 252: w = [4.99999999 6.99999999], b = 3.99999999629433, loss = 41.23861915048947\n",
      "Iteration 255: w = [4.99999999 6.99999999], b = 3.9999999962951183, loss = 32.53803544850336\n",
      "Iteration 258: w = [4.99999999 6.99999999], b = 3.9999999963001764, loss = 25.024190903617306\n",
      "Iteration 261: w = [4.99999999 6.99999999], b = 3.9999999962759762, loss = 188.43769713724282\n",
      "Iteration 264: w = [4.99999999 6.99999999], b = 3.9999999962898336, loss = 65.77410589904797\n",
      "Iteration 267: w = [4.99999999 6.99999999], b = 3.999999996285641, loss = 110.37671184131828\n",
      "Iteration 270: w = [4.99999999 6.99999999], b = 3.9999999962690143, loss = 103.02660004725549\n",
      "Iteration 273: w = [4.99999999 6.99999999], b = 3.999999996260804, loss = 48.237561393350326\n",
      "Iteration 276: w = [4.99999999 6.99999999], b = 3.999999996265779, loss = 21.884860750359127\n",
      "Iteration 279: w = [4.99999999 6.99999999], b = 3.999999996237832, loss = 333.2698275797414\n",
      "Iteration 282: w = [4.99999999 6.99999999], b = 3.9999999962385124, loss = 33.202000049075956\n",
      "Iteration 285: w = [4.99999999 6.99999999], b = 3.9999999962249877, loss = 166.06009775001556\n",
      "Iteration 288: w = [4.99999999 6.99999999], b = 3.9999999962278863, loss = 15.458910997916634\n",
      "Iteration 291: w = [4.99999999 6.99999999], b = 3.9999999962169905, loss = 70.60845066790728\n",
      "Iteration 294: w = [4.99999999 6.99999999], b = 3.9999999962190804, loss = 20.209451617758102\n",
      "Iteration 297: w = [4.99999999 6.99999999], b = 3.999999996185481, loss = 501.8881061536954\n",
      "Iteration 300: w = [4.99999999 6.99999999], b = 3.9999999961867716, loss = 25.545807278957227\n",
      "Iteration 303: w = [4.99999999 6.99999999], b = 3.999999996165126, loss = 160.1863638328057\n",
      "Iteration 306: w = [4.99999999 6.99999999], b = 3.9999999961698687, loss = 81.08442928549319\n",
      "Iteration 309: w = [4.99999999 6.99999999], b = 3.9999999961556454, loss = 96.87673027468439\n",
      "Iteration 312: w = [4.99999999 6.99999999], b = 3.999999996138306, loss = 75.18555759791901\n",
      "Iteration 315: w = [4.99999999 6.99999999], b = 3.9999999961179395, loss = 206.79269999612544\n",
      "Iteration 318: w = [4.99999999 6.99999999], b = 3.9999999960970585, loss = 399.9464492565865\n",
      "Iteration 321: w = [4.99999999 6.99999999], b = 3.9999999960883588, loss = 26.72178114783547\n",
      "Iteration 324: w = [4.99999999 6.99999999], b = 3.9999999960757213, loss = 47.498786729351714\n",
      "Iteration 327: w = [4.99999999 6.99999999], b = 3.999999996071542, loss = 50.86116788598332\n",
      "Iteration 330: w = [4.99999999 6.99999999], b = 3.9999999960704495, loss = 94.49531736922934\n",
      "Iteration 333: w = [4.99999999 6.99999999], b = 3.999999996083724, loss = 55.356052210862934\n",
      "Iteration 336: w = [4.99999999 6.99999999], b = 3.9999999960546333, loss = 250.8496564775451\n",
      "Iteration 339: w = [4.99999999 6.99999999], b = 3.9999999960290746, loss = 308.0921009014807\n",
      "Iteration 342: w = [4.99999999 6.99999999], b = 3.999999996005277, loss = 144.43353443855702\n",
      "Iteration 345: w = [4.99999999 6.99999999], b = 3.9999999960244765, loss = 99.16964138101163\n",
      "Iteration 348: w = [4.99999999 6.99999999], b = 3.9999999960027965, loss = 125.8638469734721\n",
      "Iteration 351: w = [4.99999999 6.99999999], b = 3.9999999959901715, loss = 68.14143862427888\n",
      "Iteration 354: w = [4.99999999 6.99999999], b = 3.9999999959715904, loss = 190.4138714998178\n",
      "Iteration 357: w = [4.99999999 6.99999999], b = 3.9999999959699575, loss = 39.898632661127316\n",
      "Iteration 360: w = [4.99999999 6.99999999], b = 3.999999995966543, loss = 119.55929559631427\n",
      "Iteration 363: w = [4.99999999 6.99999999], b = 3.9999999959686807, loss = 34.46849279881558\n",
      "Iteration 366: w = [4.99999999 6.99999999], b = 3.999999995976779, loss = 21.784263180295582\n",
      "Iteration 369: w = [4.99999999 6.99999999], b = 3.9999999959580705, loss = 194.2470617526558\n",
      "Iteration 372: w = [4.99999999 6.99999999], b = 3.9999999959420496, loss = 113.50803074297221\n",
      "Iteration 375: w = [4.99999999 6.99999999], b = 3.999999995945174, loss = 7.6996847349413\n",
      "Iteration 378: w = [4.99999999 6.99999999], b = 3.9999999959328303, loss = 54.491129718003045\n",
      "Iteration 381: w = [4.99999999 6.99999999], b = 3.9999999959288646, loss = 79.43237226275873\n",
      "Iteration 384: w = [4.99999999 6.99999999], b = 3.9999999958977575, loss = 348.84852427846755\n",
      "Iteration 387: w = [4.99999999 6.99999999], b = 3.9999999958917942, loss = 133.9190215300932\n",
      "Iteration 390: w = [4.99999999 6.99999999], b = 3.9999999958960872, loss = 44.03742989565804\n",
      "Iteration 393: w = [4.99999999 6.99999999], b = 3.999999995863705, loss = 264.7703795390549\n",
      "Iteration 396: w = [4.99999999 6.99999999], b = 3.9999999958428942, loss = 139.14292537625448\n",
      "Iteration 399: w = [4.99999999 6.99999999], b = 3.999999995836643, loss = 58.46828695885855\n",
      "Iteration 402: w = [4.99999999 6.99999999], b = 3.9999999958163626, loss = 163.87649847173859\n",
      "Iteration 405: w = [4.99999999 6.99999999], b = 3.99999999581253, loss = 138.50077888762917\n",
      "Iteration 408: w = [4.99999999 6.99999999], b = 3.9999999957983596, loss = 84.14489014000488\n",
      "Iteration 411: w = [4.99999999 6.99999999], b = 3.9999999957842443, loss = 237.79732958863485\n",
      "Iteration 414: w = [4.99999999 6.99999999], b = 3.9999999957898735, loss = 54.64281439061724\n",
      "Iteration 417: w = [4.99999999 6.99999999], b = 3.999999995777579, loss = 214.95688621154463\n",
      "Iteration 420: w = [4.99999999 6.99999999], b = 3.9999999957875834, loss = 40.214939103634265\n",
      "Iteration 423: w = [4.99999999 6.99999999], b = 3.999999995774435, loss = 83.58319373684706\n",
      "Iteration 426: w = [4.99999999 6.99999999], b = 3.9999999957668035, loss = 93.59086866396451\n",
      "Iteration 429: w = [4.99999999 6.99999999], b = 3.9999999957586603, loss = 17.605571170852798\n",
      "Iteration 432: w = [4.99999999 6.99999999], b = 3.9999999957576216, loss = 209.04528720609028\n",
      "Iteration 435: w = [4.99999999 6.99999999], b = 3.9999999957589023, loss = 54.82669051635338\n",
      "Iteration 438: w = [4.99999999 6.99999999], b = 3.9999999957635772, loss = 81.9906047712277\n",
      "Iteration 441: w = [4.99999999 6.99999999], b = 3.9999999957544805, loss = 34.15324177285253\n",
      "Iteration 444: w = [4.99999999 6.99999999], b = 3.9999999957519092, loss = 1.6633865509642112\n",
      "Iteration 447: w = [4.99999999 6.99999999], b = 3.999999995752352, loss = 31.688839695454885\n",
      "Iteration 450: w = [4.99999999 6.99999999], b = 3.999999995735337, loss = 123.30199181081429\n",
      "Iteration 453: w = [4.99999999 6.99999999], b = 3.9999999957519186, loss = 84.62997479006634\n",
      "Iteration 456: w = [4.99999999 6.99999999], b = 3.9999999957236496, loss = 272.7354726291209\n",
      "Iteration 459: w = [4.99999999 6.99999999], b = 3.9999999956926753, loss = 259.62066999419\n",
      "Iteration 462: w = [4.99999999 6.99999999], b = 3.9999999956695262, loss = 147.8637908931875\n",
      "Iteration 465: w = [4.99999999 6.99999999], b = 3.999999995673302, loss = 8.18928891885429\n",
      "Iteration 468: w = [4.99999999 6.99999999], b = 3.9999999956807564, loss = 78.90993436793364\n",
      "Iteration 471: w = [4.99999999 6.99999999], b = 3.999999995653525, loss = 297.75779234020865\n",
      "Iteration 474: w = [4.99999999 6.99999999], b = 3.9999999956386887, loss = 172.0386155441722\n",
      "Iteration 477: w = [4.99999999 6.99999999], b = 3.9999999956410393, loss = 117.12171550822974\n",
      "Iteration 480: w = [4.99999999 6.99999999], b = 3.9999999956337393, loss = 29.12019575223601\n",
      "Iteration 483: w = [4.99999999 6.99999999], b = 3.9999999956029963, loss = 488.09921505372205\n",
      "Iteration 486: w = [4.99999999 6.99999999], b = 3.9999999955870975, loss = 245.07804031234187\n",
      "Iteration 489: w = [4.99999999 6.99999999], b = 3.999999995576943, loss = 138.30743005249877\n",
      "Iteration 492: w = [4.99999999 6.99999999], b = 3.9999999955733228, loss = 53.62525132647053\n",
      "Iteration 495: w = [4.99999999 6.99999999], b = 3.9999999955816885, loss = 30.710168316154242\n",
      "Iteration 498: w = [4.99999999 6.99999999], b = 3.9999999955624097, loss = 226.2442506255954\n",
      "Iteration 501: w = [4.99999999 6.99999999], b = 3.99999999554956, loss = 60.94220101078344\n",
      "Iteration 504: w = [4.99999999 6.99999999], b = 3.999999995528432, loss = 252.38254051699082\n",
      "Iteration 507: w = [4.99999999 6.99999999], b = 3.9999999955061054, loss = 177.29607535208956\n",
      "Iteration 510: w = [4.99999999 6.99999999], b = 3.999999995509389, loss = 30.252557708036928\n",
      "Iteration 513: w = [4.99999999 6.99999999], b = 3.999999995510024, loss = 1.1522088842948224\n",
      "Iteration 516: w = [4.99999999 6.99999999], b = 3.9999999955045364, loss = 63.84175475248407\n",
      "Iteration 519: w = [4.99999999 6.99999999], b = 3.9999999954951555, loss = 86.77353708862601\n",
      "Iteration 522: w = [4.99999999 6.99999999], b = 3.999999995487202, loss = 94.78937900685548\n",
      "Iteration 525: w = [4.99999999 6.99999999], b = 3.999999995482957, loss = 185.39040467319788\n",
      "Iteration 528: w = [4.99999999 6.99999999], b = 3.99999999548463, loss = 65.61512767620529\n",
      "Iteration 531: w = [4.99999999 6.99999999], b = 3.999999995476663, loss = 166.3897630170536\n",
      "Iteration 534: w = [4.99999999 6.99999999], b = 3.9999999954680634, loss = 107.78321532081539\n",
      "Iteration 537: w = [4.99999999 6.99999999], b = 3.999999995487156, loss = 96.23615766483073\n",
      "Iteration 540: w = [4.99999999 6.99999999], b = 3.999999995500968, loss = 49.165010737893475\n",
      "Iteration 543: w = [4.99999999 6.99999999], b = 3.999999995500747, loss = 7.0687463029455095\n",
      "Iteration 546: w = [4.99999999 6.99999999], b = 3.999999995492352, loss = 115.32837475096908\n",
      "Iteration 549: w = [4.99999999 6.99999999], b = 3.9999999954724443, loss = 164.9250542939782\n",
      "Iteration 552: w = [4.99999999 6.99999999], b = 3.9999999954590364, loss = 269.5514825157017\n",
      "Iteration 555: w = [4.99999999 6.99999999], b = 3.9999999954465375, loss = 142.78659184312264\n",
      "Iteration 558: w = [4.99999999 6.99999999], b = 3.999999995426737, loss = 124.89297960122705\n",
      "Iteration 561: w = [4.99999999 6.99999999], b = 3.9999999954391745, loss = 42.03232534636715\n",
      "Iteration 564: w = [4.99999999 6.99999999], b = 3.999999995434296, loss = 78.17713348053014\n",
      "Iteration 567: w = [4.99999999 6.99999999], b = 3.999999995439997, loss = 43.85891943716596\n",
      "Epoch: 4\n",
      "Iteration 0: w = [4.99999999 6.99999999], b = 3.999999995458192, loss = 83.85403559183543\n",
      "Iteration 3: w = [4.99999999 6.99999999], b = 3.9999999954192447, loss = 402.54662335936445\n",
      "Iteration 6: w = [4.99999999 6.99999999], b = 3.9999999953993677, loss = 197.34352384899523\n",
      "Iteration 9: w = [4.99999999 6.99999999], b = 3.999999995410125, loss = 52.91649827286104\n",
      "Iteration 12: w = [4.99999999 6.99999999], b = 3.999999995386011, loss = 230.17109143549888\n",
      "Iteration 15: w = [4.99999999 6.99999999], b = 3.9999999953655694, loss = 486.7960619127464\n",
      "Iteration 18: w = [4.99999999 6.99999999], b = 3.9999999953469407, loss = 138.70150976154918\n",
      "Iteration 21: w = [4.99999999 6.99999999], b = 3.999999995339654, loss = 38.52175083873021\n",
      "Iteration 24: w = [4.99999999 6.99999999], b = 3.999999995326299, loss = 121.96657101438161\n",
      "Iteration 27: w = [4.99999999 6.99999999], b = 3.999999995312409, loss = 49.590243938547324\n",
      "Iteration 30: w = [4.99999999 6.99999999], b = 3.9999999953056786, loss = 27.056816341256177\n",
      "Iteration 33: w = [4.99999999 6.99999999], b = 3.999999995296848, loss = 32.40399605665908\n",
      "Iteration 36: w = [4.99999999 6.99999999], b = 3.999999995305686, loss = 63.20386418857018\n",
      "Iteration 39: w = [4.99999999 6.99999999], b = 3.9999999952955045, loss = 42.22048060796332\n",
      "Iteration 42: w = [4.99999999 6.99999999], b = 3.999999995287115, loss = 91.58721263739854\n",
      "Iteration 45: w = [4.99999999 6.99999999], b = 3.9999999952744965, loss = 154.88755160368217\n",
      "Iteration 48: w = [4.99999999 6.99999999], b = 3.9999999952456067, loss = 235.8164682860454\n",
      "Iteration 51: w = [4.99999999 6.99999999], b = 3.9999999952315215, loss = 113.52436785337663\n",
      "Iteration 54: w = [4.99999999 6.99999999], b = 3.999999995238675, loss = 33.2162912592646\n",
      "Iteration 57: w = [4.99999999 6.99999999], b = 3.9999999952214753, loss = 88.7868003018986\n",
      "Iteration 60: w = [4.99999999 6.99999999], b = 3.999999995218703, loss = 65.34406349009708\n",
      "Iteration 63: w = [4.99999999 6.99999999], b = 3.999999995209798, loss = 125.93335280002962\n",
      "Iteration 66: w = [4.99999999 6.99999999], b = 3.999999995222056, loss = 44.823812135079216\n",
      "Iteration 69: w = [4.99999999 6.99999999], b = 3.9999999952207546, loss = 28.20160785191564\n",
      "Iteration 72: w = [4.99999999 6.99999999], b = 3.9999999952145537, loss = 59.01557289559583\n",
      "Iteration 75: w = [4.99999999 6.99999999], b = 3.999999995197246, loss = 75.22740427385968\n",
      "Iteration 78: w = [4.99999999 6.99999999], b = 3.999999995201218, loss = 108.75821580297543\n",
      "Iteration 81: w = [4.99999999 6.99999999], b = 3.999999995201142, loss = 106.07549848011247\n",
      "Iteration 84: w = [4.99999999 6.99999999], b = 3.9999999952020278, loss = 48.36443588361583\n",
      "Iteration 87: w = [4.99999999 6.99999999], b = 3.9999999952143153, loss = 53.28390067703514\n",
      "Iteration 90: w = [4.99999999 6.99999999], b = 3.9999999951848464, loss = 253.8120154779383\n",
      "Iteration 93: w = [4.99999999 6.99999999], b = 3.999999995182066, loss = 11.11149175672884\n",
      "Iteration 96: w = [4.99999999 6.99999999], b = 3.9999999951719674, loss = 84.05473196534095\n",
      "Iteration 99: w = [4.99999999 6.99999999], b = 3.999999995181742, loss = 25.637787684350467\n",
      "Iteration 102: w = [4.99999999 6.99999999], b = 3.9999999951703344, loss = 115.94617569284917\n",
      "Iteration 105: w = [4.99999999 6.99999999], b = 3.999999995165363, loss = 76.85727347595267\n",
      "Iteration 108: w = [4.99999999 6.99999999], b = 3.999999995174465, loss = 24.304924059171185\n",
      "Iteration 111: w = [4.99999999 6.99999999], b = 3.9999999951637335, loss = 72.6396157715936\n",
      "Iteration 114: w = [4.99999999 6.99999999], b = 3.9999999951553757, loss = 93.05408002042299\n",
      "Iteration 117: w = [4.99999999 6.99999999], b = 3.999999995137646, loss = 258.4116446786456\n",
      "Iteration 120: w = [4.99999999 6.99999999], b = 3.9999999951309624, loss = 189.4296251563253\n",
      "Iteration 123: w = [4.99999999 6.99999999], b = 3.9999999951351426, loss = 35.26902588972129\n",
      "Iteration 126: w = [4.99999999 6.99999999], b = 3.999999995116953, loss = 94.54465173368811\n",
      "Iteration 129: w = [4.99999999 6.99999999], b = 3.999999995128131, loss = 48.8302727226994\n",
      "Iteration 132: w = [4.99999999 6.99999999], b = 3.999999995125487, loss = 23.898035198929453\n",
      "Iteration 135: w = [4.99999999 6.99999999], b = 3.9999999951122156, loss = 174.4250314195571\n",
      "Iteration 138: w = [4.99999999 6.99999999], b = 3.999999995104424, loss = 59.81784052879138\n",
      "Iteration 141: w = [4.99999999 6.99999999], b = 3.9999999951084773, loss = 25.927287133636252\n",
      "Iteration 144: w = [4.99999999 6.99999999], b = 3.999999995091024, loss = 300.66651598366974\n",
      "Iteration 147: w = [4.99999999 6.99999999], b = 3.9999999950915814, loss = 38.09977980005461\n",
      "Iteration 150: w = [4.99999999 6.99999999], b = 3.999999995091514, loss = 152.32540028142245\n",
      "Iteration 153: w = [4.99999999 6.99999999], b = 3.999999995074978, loss = 158.9548296668089\n",
      "Iteration 156: w = [4.99999999 6.99999999], b = 3.9999999950593783, loss = 82.8092982243265\n",
      "Iteration 159: w = [4.99999999 6.99999999], b = 3.9999999950420944, loss = 90.81209486234717\n",
      "Iteration 162: w = [4.99999999 6.99999999], b = 3.9999999950210428, loss = 119.0931460622595\n",
      "Iteration 165: w = [4.99999999 6.99999999], b = 3.9999999950167178, loss = 14.400319605354916\n",
      "Iteration 168: w = [4.99999999 6.99999999], b = 3.9999999950179452, loss = 32.87757549403967\n",
      "Iteration 171: w = [4.99999999 6.99999999], b = 3.999999995010919, loss = 113.33655451772266\n",
      "Iteration 174: w = [4.99999999 6.99999999], b = 3.9999999950008256, loss = 59.594984817689294\n",
      "Iteration 177: w = [4.99999999 6.99999999], b = 3.9999999949893486, loss = 131.9951000061944\n",
      "Iteration 180: w = [4.99999999 6.99999999], b = 3.9999999949894183, loss = 203.903493939305\n",
      "Iteration 183: w = [4.99999999 6.99999999], b = 3.999999994971115, loss = 133.72723502043056\n",
      "Iteration 186: w = [4.99999999 6.99999999], b = 3.9999999949358465, loss = 595.2019636150981\n",
      "Iteration 189: w = [4.99999999 6.99999999], b = 3.9999999949267875, loss = 197.06042431603626\n",
      "Iteration 192: w = [4.99999999 6.99999999], b = 3.999999994928245, loss = 4.1369018769951795\n",
      "Iteration 195: w = [4.99999999 6.99999999], b = 3.9999999949165908, loss = 362.3972771447282\n",
      "Iteration 198: w = [4.99999999 6.99999999], b = 3.9999999949097433, loss = 67.06673126753634\n",
      "Iteration 201: w = [4.99999999 6.99999999], b = 3.999999994894516, loss = 58.73339964131534\n",
      "Iteration 204: w = [4.99999999 6.99999999], b = 3.9999999948968474, loss = 55.5230801357961\n",
      "Iteration 207: w = [4.99999999 6.99999999], b = 3.9999999948829332, loss = 107.77729783687626\n",
      "Iteration 210: w = [4.99999999 6.99999999], b = 3.9999999948766773, loss = 52.65134871605513\n",
      "Iteration 213: w = [4.99999999 6.99999999], b = 3.9999999948520175, loss = 182.6055482970357\n",
      "Iteration 216: w = [4.99999999 6.99999999], b = 3.999999994852044, loss = 64.83561035928462\n",
      "Iteration 219: w = [4.99999999 6.99999999], b = 3.9999999948386225, loss = 49.22351634079586\n",
      "Iteration 222: w = [4.99999999 6.99999999], b = 3.999999994807944, loss = 359.73587485566446\n",
      "Iteration 225: w = [4.99999999 6.99999999], b = 3.9999999948019576, loss = 23.27867285603115\n",
      "Iteration 228: w = [4.99999999 6.99999999], b = 3.9999999947983356, loss = 20.29562880657233\n",
      "Iteration 231: w = [4.99999999 6.99999999], b = 3.9999999947689893, loss = 216.4550896795852\n",
      "Iteration 234: w = [4.99999999 6.99999999], b = 3.999999994764457, loss = 79.61285533363565\n",
      "Iteration 237: w = [4.99999999 6.99999999], b = 3.9999999947794564, loss = 77.61336070985072\n",
      "Iteration 240: w = [4.99999999 6.99999999], b = 3.9999999947971503, loss = 79.96515219167021\n",
      "Iteration 243: w = [4.99999999 6.99999999], b = 3.999999994766494, loss = 272.8547197292864\n",
      "Iteration 246: w = [4.99999999 6.99999999], b = 3.9999999947662284, loss = 13.550812300116926\n",
      "Iteration 249: w = [4.99999999 6.99999999], b = 3.999999994771185, loss = 32.70527175453051\n",
      "Iteration 252: w = [4.99999999 6.99999999], b = 3.999999994774329, loss = 41.238619091067484\n",
      "Iteration 255: w = [4.99999999 6.99999999], b = 3.9999999947751173, loss = 32.538035414426396\n",
      "Iteration 258: w = [4.99999999 6.99999999], b = 3.9999999947801754, loss = 25.024190869028235\n",
      "Iteration 261: w = [4.99999999 6.99999999], b = 3.9999999947559752, loss = 188.4376969313372\n",
      "Iteration 264: w = [4.99999999 6.99999999], b = 3.9999999947698326, loss = 65.77410580436573\n",
      "Iteration 267: w = [4.99999999 6.99999999], b = 3.99999999476564, loss = 110.37671171113284\n",
      "Iteration 270: w = [4.99999999 6.99999999], b = 3.9999999947490132, loss = 103.02659993470125\n",
      "Iteration 273: w = [4.99999999 6.99999999], b = 3.999999994740803, loss = 48.23756133004326\n",
      "Iteration 276: w = [4.99999999 6.99999999], b = 3.999999994745778, loss = 21.88486071960162\n",
      "Iteration 279: w = [4.99999999 6.99999999], b = 3.999999994717831, loss = 333.26982721444415\n",
      "Iteration 282: w = [4.99999999 6.99999999], b = 3.9999999947185114, loss = 33.20200000417197\n",
      "Iteration 285: w = [4.99999999 6.99999999], b = 3.9999999947049867, loss = 166.06009755564472\n",
      "Iteration 288: w = [4.99999999 6.99999999], b = 3.9999999947078853, loss = 15.458910973949228\n",
      "Iteration 291: w = [4.99999999 6.99999999], b = 3.9999999946969895, loss = 70.60845058266644\n",
      "Iteration 294: w = [4.99999999 6.99999999], b = 3.9999999946990794, loss = 20.20945159088249\n",
      "Iteration 297: w = [4.99999999 6.99999999], b = 3.99999999466548, loss = 501.8881055991665\n",
      "Iteration 300: w = [4.99999999 6.99999999], b = 3.9999999946667706, loss = 25.545807250756667\n",
      "Iteration 303: w = [4.99999999 6.99999999], b = 3.999999994645125, loss = 160.18636365049528\n",
      "Iteration 306: w = [4.99999999 6.99999999], b = 3.9999999946498677, loss = 81.08442918247619\n",
      "Iteration 309: w = [4.99999999 6.99999999], b = 3.9999999946356444, loss = 96.87673015625121\n",
      "Iteration 312: w = [4.99999999 6.99999999], b = 3.999999994618305, loss = 75.18555752558198\n",
      "Iteration 315: w = [4.99999999 6.99999999], b = 3.9999999945979385, loss = 206.79269976594733\n",
      "Iteration 318: w = [4.99999999 6.99999999], b = 3.9999999945770575, loss = 399.9464487861967\n",
      "Iteration 321: w = [4.99999999 6.99999999], b = 3.9999999945683578, loss = 26.721781124088448\n",
      "Iteration 324: w = [4.99999999 6.99999999], b = 3.9999999945557203, loss = 47.4987866763038\n",
      "Iteration 327: w = [4.99999999 6.99999999], b = 3.999999994551541, loss = 50.86116783494894\n",
      "Iteration 330: w = [4.99999999 6.99999999], b = 3.9999999945504485, loss = 94.49531724831137\n",
      "Iteration 333: w = [4.99999999 6.99999999], b = 3.999999994563723, loss = 55.35605213307593\n",
      "Iteration 336: w = [4.99999999 6.99999999], b = 3.9999999945346323, loss = 250.84965619859736\n",
      "Iteration 339: w = [4.99999999 6.99999999], b = 3.9999999945090736, loss = 308.09210054891815\n",
      "Iteration 342: w = [4.99999999 6.99999999], b = 3.9999999944852758, loss = 144.43353429775678\n",
      "Iteration 345: w = [4.99999999 6.99999999], b = 3.9999999945044755, loss = 99.16964123336307\n",
      "Iteration 348: w = [4.99999999 6.99999999], b = 3.9999999944827955, loss = 125.86384684680598\n",
      "Iteration 351: w = [4.99999999 6.99999999], b = 3.9999999944701705, loss = 68.14143855391633\n",
      "Iteration 354: w = [4.99999999 6.99999999], b = 3.9999999944515894, loss = 190.41387127197535\n",
      "Iteration 357: w = [4.99999999 6.99999999], b = 3.9999999944499565, loss = 39.89863261946177\n",
      "Iteration 360: w = [4.99999999 6.99999999], b = 3.999999994446542, loss = 119.55929544615098\n",
      "Iteration 363: w = [4.99999999 6.99999999], b = 3.9999999944486797, loss = 34.46849275678676\n",
      "Iteration 366: w = [4.99999999 6.99999999], b = 3.999999994456778, loss = 21.784263146860287\n",
      "Iteration 369: w = [4.99999999 6.99999999], b = 3.9999999944380695, loss = 194.2470615482495\n",
      "Iteration 372: w = [4.99999999 6.99999999], b = 3.9999999944220486, loss = 113.50803062830539\n",
      "Iteration 375: w = [4.99999999 6.99999999], b = 3.999999994425173, loss = 7.699684719733983\n",
      "Iteration 378: w = [4.99999999 6.99999999], b = 3.9999999944128293, loss = 54.49112965869656\n",
      "Iteration 381: w = [4.99999999 6.99999999], b = 3.9999999944088636, loss = 79.43237217569792\n",
      "Iteration 384: w = [4.99999999 6.99999999], b = 3.9999999943777564, loss = 348.84852388301937\n",
      "Iteration 387: w = [4.99999999 6.99999999], b = 3.999999994371793, loss = 133.91902137430768\n",
      "Iteration 390: w = [4.99999999 6.99999999], b = 3.999999994376086, loss = 44.03742983673023\n",
      "Iteration 393: w = [4.99999999 6.99999999], b = 3.999999994343704, loss = 264.77037923542133\n",
      "Iteration 396: w = [4.99999999 6.99999999], b = 3.999999994322893, loss = 139.14292522450688\n",
      "Iteration 399: w = [4.99999999 6.99999999], b = 3.9999999943166418, loss = 58.468286889400986\n",
      "Iteration 402: w = [4.99999999 6.99999999], b = 3.9999999942963615, loss = 163.8764982841068\n",
      "Iteration 405: w = [4.99999999 6.99999999], b = 3.999999994292529, loss = 138.50077872346912\n",
      "Iteration 408: w = [4.99999999 6.99999999], b = 3.9999999942783586, loss = 84.1448900469433\n",
      "Iteration 411: w = [4.99999999 6.99999999], b = 3.9999999942642432, loss = 237.7973293208585\n",
      "Iteration 414: w = [4.99999999 6.99999999], b = 3.9999999942698725, loss = 54.64281431538192\n",
      "Iteration 417: w = [4.99999999 6.99999999], b = 3.999999994257578, loss = 214.95688597497977\n",
      "Iteration 420: w = [4.99999999 6.99999999], b = 3.9999999942675823, loss = 40.214939044121735\n",
      "Iteration 423: w = [4.99999999 6.99999999], b = 3.999999994254434, loss = 83.5831936571074\n",
      "Iteration 426: w = [4.99999999 6.99999999], b = 3.9999999942468025, loss = 93.59086855529864\n",
      "Iteration 429: w = [4.99999999 6.99999999], b = 3.9999999942386593, loss = 17.605571150483737\n",
      "Iteration 432: w = [4.99999999 6.99999999], b = 3.9999999942376205, loss = 209.04528696345608\n",
      "Iteration 435: w = [4.99999999 6.99999999], b = 3.9999999942389013, loss = 54.826690445275254\n",
      "Iteration 438: w = [4.99999999 6.99999999], b = 3.999999994243576, loss = 81.99060466694732\n",
      "Iteration 441: w = [4.99999999 6.99999999], b = 3.9999999942344795, loss = 34.15324173269185\n",
      "Iteration 444: w = [4.99999999 6.99999999], b = 3.999999994231908, loss = 1.663386551266437\n",
      "Iteration 447: w = [4.99999999 6.99999999], b = 3.999999994232351, loss = 31.688839655031767\n",
      "Iteration 450: w = [4.99999999 6.99999999], b = 3.999999994215336, loss = 123.30199166135766\n",
      "Iteration 453: w = [4.99999999 6.99999999], b = 3.9999999942319175, loss = 84.6299746694646\n",
      "Iteration 456: w = [4.99999999 6.99999999], b = 3.9999999942036486, loss = 272.7354723312962\n",
      "Iteration 459: w = [4.99999999 6.99999999], b = 3.9999999941726743, loss = 259.62066970831006\n",
      "Iteration 462: w = [4.99999999 6.99999999], b = 3.9999999941495252, loss = 147.8637907370345\n",
      "Iteration 465: w = [4.99999999 6.99999999], b = 3.999999994153301, loss = 8.189288908470663\n",
      "Iteration 468: w = [4.99999999 6.99999999], b = 3.9999999941607554, loss = 78.9099342633093\n",
      "Iteration 471: w = [4.99999999 6.99999999], b = 3.999999994133524, loss = 297.7577920127689\n",
      "Iteration 474: w = [4.99999999 6.99999999], b = 3.9999999941186877, loss = 172.03861534588907\n",
      "Iteration 477: w = [4.99999999 6.99999999], b = 3.9999999941210382, loss = 117.12171535023106\n",
      "Iteration 480: w = [4.99999999 6.99999999], b = 3.9999999941137383, loss = 29.12019571996413\n",
      "Iteration 483: w = [4.99999999 6.99999999], b = 3.9999999940829953, loss = 488.099214501826\n",
      "Iteration 486: w = [4.99999999 6.99999999], b = 3.9999999940670965, loss = 245.07804003944435\n",
      "Iteration 489: w = [4.99999999 6.99999999], b = 3.999999994056942, loss = 138.30742988792966\n",
      "Iteration 492: w = [4.99999999 6.99999999], b = 3.9999999940533217, loss = 53.625251265064584\n",
      "Iteration 495: w = [4.99999999 6.99999999], b = 3.9999999940616875, loss = 30.710168278124456\n",
      "Iteration 498: w = [4.99999999 6.99999999], b = 3.9999999940424087, loss = 226.24425036901644\n",
      "Iteration 501: w = [4.99999999 6.99999999], b = 3.999999994029559, loss = 60.94220095001956\n",
      "Iteration 504: w = [4.99999999 6.99999999], b = 3.999999994008431, loss = 252.38254025967186\n",
      "Iteration 507: w = [4.99999999 6.99999999], b = 3.9999999939861044, loss = 177.29607515235395\n",
      "Iteration 510: w = [4.99999999 6.99999999], b = 3.999999993989388, loss = 30.25255766381716\n",
      "Iteration 513: w = [4.99999999 6.99999999], b = 3.999999993990023, loss = 1.1522088817127532\n",
      "Iteration 516: w = [4.99999999 6.99999999], b = 3.9999999939845354, loss = 63.84175468103448\n",
      "Iteration 519: w = [4.99999999 6.99999999], b = 3.9999999939751545, loss = 86.7735369972956\n",
      "Iteration 522: w = [4.99999999 6.99999999], b = 3.999999993967201, loss = 94.78937891196178\n",
      "Iteration 525: w = [4.99999999 6.99999999], b = 3.999999993962956, loss = 185.39040445069347\n",
      "Iteration 528: w = [4.99999999 6.99999999], b = 3.999999993964629, loss = 65.61512758697215\n",
      "Iteration 531: w = [4.99999999 6.99999999], b = 3.999999993956662, loss = 166.38976282573418\n",
      "Iteration 534: w = [4.99999999 6.99999999], b = 3.9999999939480624, loss = 107.78321519659109\n",
      "Iteration 537: w = [4.99999999 6.99999999], b = 3.999999993967155, loss = 96.23615752393336\n",
      "Iteration 540: w = [4.99999999 6.99999999], b = 3.999999993980967, loss = 49.165010668207096\n",
      "Iteration 543: w = [4.99999999 6.99999999], b = 3.999999993980746, loss = 7.068746294118132\n",
      "Iteration 546: w = [4.99999999 6.99999999], b = 3.999999993972351, loss = 115.32837462533541\n",
      "Iteration 549: w = [4.99999999 6.99999999], b = 3.9999999939524433, loss = 164.92505411239276\n",
      "Iteration 552: w = [4.99999999 6.99999999], b = 3.9999999939390354, loss = 269.55148220045476\n",
      "Iteration 555: w = [4.99999999 6.99999999], b = 3.9999999939265365, loss = 142.78659168705235\n",
      "Iteration 558: w = [4.99999999 6.99999999], b = 3.999999993906736, loss = 124.89297946381411\n",
      "Iteration 561: w = [4.99999999 6.99999999], b = 3.9999999939191735, loss = 42.03232527649024\n",
      "Iteration 564: w = [4.99999999 6.99999999], b = 3.999999993914295, loss = 78.17713338041465\n",
      "Iteration 567: w = [4.99999999 6.99999999], b = 3.999999993919996, loss = 43.858919378409134\n",
      "Epoch: 5\n",
      "Iteration 0: w = [4.99999999 6.99999999], b = 3.999999993938191, loss = 83.85403547671609\n",
      "Iteration 3: w = [4.99999999 6.99999999], b = 3.9999999938992437, loss = 402.5466229054338\n",
      "Iteration 6: w = [4.99999999 6.99999999], b = 3.9999999938793667, loss = 197.34352361887395\n",
      "Iteration 9: w = [4.99999999 6.99999999], b = 3.999999993890124, loss = 52.9164981949414\n",
      "Iteration 12: w = [4.99999999 6.99999999], b = 3.99999999386601, loss = 230.1710911859942\n",
      "Iteration 15: w = [4.99999999 6.99999999], b = 3.9999999938455684, loss = 486.7960613728608\n",
      "Iteration 18: w = [4.99999999 6.99999999], b = 3.9999999938269397, loss = 138.70150961093972\n",
      "Iteration 21: w = [4.99999999 6.99999999], b = 3.999999993819653, loss = 38.52175080071088\n",
      "Iteration 24: w = [4.99999999 6.99999999], b = 3.999999993806298, loss = 121.96657087872647\n",
      "Iteration 27: w = [4.99999999 6.99999999], b = 3.999999993792408, loss = 49.590243883363996\n",
      "Iteration 30: w = [4.99999999 6.99999999], b = 3.9999999937856776, loss = 27.056816318701724\n",
      "Iteration 33: w = [4.99999999 6.99999999], b = 3.999999993776847, loss = 32.40399602780114\n",
      "Iteration 36: w = [4.99999999 6.99999999], b = 3.999999993785685, loss = 63.20386409902499\n",
      "Iteration 39: w = [4.99999999 6.99999999], b = 3.9999999937755035, loss = 42.22048056281505\n",
      "Iteration 42: w = [4.99999999 6.99999999], b = 3.999999993767114, loss = 91.58721253059709\n",
      "Iteration 45: w = [4.99999999 6.99999999], b = 3.9999999937544954, loss = 154.8875514228601\n",
      "Iteration 48: w = [4.99999999 6.99999999], b = 3.9999999937256057, loss = 235.81646802923672\n",
      "Iteration 51: w = [4.99999999 6.99999999], b = 3.9999999937115205, loss = 113.52436772599135\n",
      "Iteration 54: w = [4.99999999 6.99999999], b = 3.999999993718674, loss = 33.21629120778477\n",
      "Iteration 57: w = [4.99999999 6.99999999], b = 3.9999999937014743, loss = 88.78680021449712\n",
      "Iteration 60: w = [4.99999999 6.99999999], b = 3.999999993698702, loss = 65.34406341485766\n",
      "Iteration 63: w = [4.99999999 6.99999999], b = 3.999999993689797, loss = 125.9333526621773\n",
      "Iteration 66: w = [4.99999999 6.99999999], b = 3.999999993702055, loss = 44.82381206839548\n",
      "Iteration 69: w = [4.99999999 6.99999999], b = 3.9999999937007535, loss = 28.201607821442565\n",
      "Iteration 72: w = [4.99999999 6.99999999], b = 3.9999999936945527, loss = 59.01557281696603\n",
      "Iteration 75: w = [4.99999999 6.99999999], b = 3.999999993677245, loss = 75.22740418730561\n",
      "Iteration 78: w = [4.99999999 6.99999999], b = 3.999999993681217, loss = 108.75821566292643\n",
      "Iteration 81: w = [4.99999999 6.99999998], b = 3.999999993681141, loss = 106.07549834452624\n",
      "Iteration 84: w = [4.99999999 6.99999998], b = 3.9999999936820267, loss = 48.36443582721285\n",
      "Iteration 87: w = [4.99999999 6.99999998], b = 3.9999999936943142, loss = 53.283900596768\n",
      "Iteration 90: w = [4.99999999 6.99999998], b = 3.9999999936648454, loss = 253.812015199216\n",
      "Iteration 93: w = [4.99999999 6.99999998], b = 3.999999993662065, loss = 11.111491746521251\n",
      "Iteration 96: w = [4.99999999 6.99999998], b = 3.9999999936519663, loss = 84.05473186121631\n",
      "Iteration 99: w = [4.99999999 6.99999998], b = 3.999999993661741, loss = 25.63778764385765\n",
      "Iteration 102: w = [4.99999999 6.99999998], b = 3.9999999936503334, loss = 115.94617557543232\n",
      "Iteration 105: w = [4.99999999 6.99999998], b = 3.999999993645362, loss = 76.8572733803263\n",
      "Iteration 108: w = [4.99999999 6.99999998], b = 3.999999993654464, loss = 24.304924028019936\n",
      "Iteration 111: w = [4.99999999 6.99999998], b = 3.9999999936437325, loss = 72.63961570002412\n",
      "Iteration 114: w = [4.99999999 6.99999998], b = 3.9999999936353747, loss = 93.05407991751387\n",
      "Iteration 117: w = [4.99999999 6.99999998], b = 3.999999993617645, loss = 258.4116443867623\n",
      "Iteration 120: w = [4.99999999 6.99999998], b = 3.9999999936109614, loss = 189.42962494000224\n",
      "Iteration 123: w = [4.99999999 6.99999998], b = 3.9999999936151416, loss = 35.2690258440885\n",
      "Iteration 126: w = [4.99999999 6.99999998], b = 3.999999993596952, loss = 94.54465162429278\n",
      "Iteration 129: w = [4.99999999 6.99999998], b = 3.99999999360813, loss = 48.83027266246597\n",
      "Iteration 132: w = [4.99999999 6.99999998], b = 3.999999993605486, loss = 23.898035171297796\n",
      "Iteration 135: w = [4.99999999 6.99999998], b = 3.9999999935922146, loss = 174.4250312208865\n",
      "Iteration 138: w = [4.99999999 6.99999998], b = 3.999999993584423, loss = 59.81784046387248\n",
      "Iteration 141: w = [4.99999999 6.99999998], b = 3.9999999935884762, loss = 25.92728710573071\n",
      "Iteration 144: w = [4.99999999 6.99999998], b = 3.999999993571023, loss = 300.66651563171115\n",
      "Iteration 147: w = [4.99999999 6.99999998], b = 3.9999999935715804, loss = 38.09977974991698\n",
      "Iteration 150: w = [4.99999999 6.99999998], b = 3.999999993571513, loss = 152.32540010075502\n",
      "Iteration 153: w = [4.99999999 6.99999998], b = 3.999999993554977, loss = 158.95482949159629\n",
      "Iteration 156: w = [4.99999999 6.99999998], b = 3.9999999935393773, loss = 82.80929814459701\n",
      "Iteration 159: w = [4.99999999 6.99999998], b = 3.9999999935220933, loss = 90.81209476425408\n",
      "Iteration 162: w = [4.99999999 6.99999998], b = 3.9999999935010417, loss = 119.09314592526545\n",
      "Iteration 165: w = [4.99999999 6.99999998], b = 3.9999999934967168, loss = 14.400319589849511\n",
      "Iteration 168: w = [4.99999999 6.99999998], b = 3.999999993497944, loss = 32.87757546588821\n",
      "Iteration 171: w = [4.99999999 6.99999998], b = 3.999999993490918, loss = 113.33655440219557\n",
      "Iteration 174: w = [4.99999999 6.99999998], b = 3.9999999934808246, loss = 59.59498475843117\n",
      "Iteration 177: w = [4.99999999 6.99999998], b = 3.9999999934693475, loss = 131.9950998703832\n",
      "Iteration 180: w = [4.99999999 6.99999998], b = 3.9999999934694173, loss = 203.9034936985096\n",
      "Iteration 183: w = [4.99999999 6.99999998], b = 3.999999993451114, loss = 133.72723487268183\n",
      "Iteration 186: w = [4.99999999 6.99999998], b = 3.9999999934158454, loss = 595.2019629355443\n",
      "Iteration 189: w = [4.99999999 6.99999998], b = 3.9999999934067865, loss = 197.0604240865343\n",
      "Iteration 192: w = [4.99999999 6.99999998], b = 3.999999993408244, loss = 4.136901869734645\n",
      "Iteration 195: w = [4.99999999 6.99999998], b = 3.9999999933965897, loss = 362.39727674452956\n",
      "Iteration 198: w = [4.99999999 6.99999998], b = 3.9999999933897423, loss = 67.06673119314412\n",
      "Iteration 201: w = [4.99999999 6.99999998], b = 3.999999993374515, loss = 58.7333995926194\n",
      "Iteration 204: w = [4.99999999 6.99999998], b = 3.9999999933768464, loss = 55.52308006606322\n",
      "Iteration 207: w = [4.99999999 6.99999998], b = 3.999999993362932, loss = 107.77729770765673\n",
      "Iteration 210: w = [4.99999999 6.99999998], b = 3.9999999933566763, loss = 52.65134865833712\n",
      "Iteration 213: w = [4.99999999 6.99999998], b = 3.9999999933320165, loss = 182.60554809211442\n",
      "Iteration 216: w = [4.99999999 6.99999998], b = 3.999999993332043, loss = 64.83561028644543\n",
      "Iteration 219: w = [4.99999999 6.99999998], b = 3.9999999933186214, loss = 49.22351629739067\n",
      "Iteration 222: w = [4.99999999 6.99999998], b = 3.999999993287943, loss = 359.735874443433\n",
      "Iteration 225: w = [4.99999999 6.99999998], b = 3.9999999932819565, loss = 23.278672838945\n",
      "Iteration 228: w = [4.99999999 6.99999998], b = 3.9999999932783346, loss = 20.29562878397679\n",
      "Iteration 231: w = [4.99999999 6.99999998], b = 3.9999999932489883, loss = 216.45508943817637\n",
      "Iteration 234: w = [4.99999999 6.99999998], b = 3.999999993244456, loss = 79.6128552441349\n",
      "Iteration 237: w = [4.99999999 6.99999998], b = 3.9999999932594554, loss = 77.61336060009026\n",
      "Iteration 240: w = [4.99999999 6.99999998], b = 3.9999999932771493, loss = 79.96515207747548\n",
      "Iteration 243: w = [4.99999999 6.99999998], b = 3.999999993246493, loss = 272.8547194347635\n",
      "Iteration 246: w = [4.99999999 6.99999998], b = 3.9999999932462273, loss = 13.550812289789427\n",
      "Iteration 249: w = [4.99999999 6.99999998], b = 3.999999993251184, loss = 32.70527170517576\n",
      "Iteration 252: w = [4.99999999 6.99999998], b = 3.999999993254328, loss = 41.238619031645506\n",
      "Iteration 255: w = [4.99999999 6.99999998], b = 3.9999999932551162, loss = 32.538035380349456\n",
      "Iteration 258: w = [4.99999999 6.99999998], b = 3.9999999932601744, loss = 25.02419083443917\n",
      "Iteration 261: w = [4.99999999 6.99999998], b = 3.999999993235974, loss = 188.4376967254315\n",
      "Iteration 264: w = [4.99999999 6.99999998], b = 3.9999999932498316, loss = 65.77410570968348\n",
      "Iteration 267: w = [4.99999999 6.99999998], b = 3.999999993245639, loss = 110.37671158094743\n",
      "Iteration 270: w = [4.99999999 6.99999998], b = 3.9999999932290122, loss = 103.02659982214703\n",
      "Iteration 273: w = [4.99999999 6.99999998], b = 3.999999993220802, loss = 48.2375612667362\n",
      "Iteration 276: w = [4.99999999 6.99999998], b = 3.999999993225777, loss = 21.8848606888441\n",
      "Iteration 279: w = [4.99999999 6.99999998], b = 3.99999999319783, loss = 333.2698268491468\n",
      "Iteration 282: w = [4.99999999 6.99999998], b = 3.9999999931985104, loss = 33.20199995926799\n",
      "Iteration 285: w = [4.99999999 6.99999998], b = 3.9999999931849857, loss = 166.06009736127388\n",
      "Iteration 288: w = [4.99999999 6.99999998], b = 3.9999999931878842, loss = 15.45891094998182\n",
      "Iteration 291: w = [4.99999999 6.99999998], b = 3.9999999931769885, loss = 70.60845049742564\n",
      "Iteration 294: w = [4.99999999 6.99999998], b = 3.9999999931790784, loss = 20.209451564006873\n",
      "Iteration 297: w = [4.99999999 6.99999998], b = 3.999999993145479, loss = 501.8881050446378\n",
      "Iteration 300: w = [4.99999999 6.99999998], b = 3.9999999931467696, loss = 25.545807222556093\n",
      "Iteration 303: w = [4.99999999 6.99999998], b = 3.9999999931251238, loss = 160.18636346818482\n",
      "Iteration 306: w = [4.99999999 6.99999998], b = 3.9999999931298666, loss = 81.08442907945913\n",
      "Iteration 309: w = [4.99999999 6.99999998], b = 3.9999999931156434, loss = 96.87673003781805\n",
      "Iteration 312: w = [4.99999999 6.99999998], b = 3.999999993098304, loss = 75.18555745324494\n",
      "Iteration 315: w = [4.99999999 6.99999998], b = 3.9999999930779375, loss = 206.79269953576917\n",
      "Iteration 318: w = [4.99999999 6.99999998], b = 3.9999999930570564, loss = 399.94644831580695\n",
      "Iteration 321: w = [4.99999999 6.99999998], b = 3.9999999930483567, loss = 26.721781100341417\n",
      "Iteration 324: w = [4.99999999 6.99999998], b = 3.9999999930357193, loss = 47.49878662325588\n",
      "Iteration 327: w = [4.99999999 6.99999998], b = 3.99999999303154, loss = 50.861167783914546\n",
      "Iteration 330: w = [4.99999999 6.99999998], b = 3.9999999930304475, loss = 94.49531712739342\n",
      "Iteration 333: w = [4.99999999 6.99999998], b = 3.999999993043722, loss = 55.356052055288906\n",
      "Iteration 336: w = [4.99999999 6.99999998], b = 3.9999999930146313, loss = 250.84965591964965\n",
      "Iteration 339: w = [4.99999999 6.99999998], b = 3.9999999929890726, loss = 308.0921001963556\n",
      "Iteration 342: w = [4.99999999 6.99999998], b = 3.9999999929652748, loss = 144.4335341569565\n",
      "Iteration 345: w = [4.99999999 6.99999998], b = 3.9999999929844745, loss = 99.16964108571442\n",
      "Iteration 348: w = [4.99999999 6.99999998], b = 3.9999999929627945, loss = 125.86384672013988\n",
      "Iteration 351: w = [4.99999999 6.99999998], b = 3.9999999929501695, loss = 68.14143848355383\n",
      "Iteration 354: w = [4.99999999 6.99999998], b = 3.9999999929315884, loss = 190.41387104413295\n",
      "Iteration 357: w = [4.99999999 6.99999998], b = 3.9999999929299555, loss = 39.89863257779623\n",
      "Iteration 360: w = [4.99999999 6.99999998], b = 3.999999992926541, loss = 119.55929529598762\n",
      "Iteration 363: w = [4.99999999 6.99999998], b = 3.9999999929286787, loss = 34.468492714757964\n",
      "Iteration 366: w = [4.99999999 6.99999998], b = 3.999999992936777, loss = 21.784263113425\n",
      "Iteration 369: w = [4.99999999 6.99999998], b = 3.9999999929180685, loss = 194.24706134384323\n",
      "Iteration 372: w = [4.99999999 6.99999998], b = 3.9999999929020476, loss = 113.50803051363853\n",
      "Iteration 375: w = [4.99999999 6.99999998], b = 3.999999992905172, loss = 7.699684704526668\n",
      "Iteration 378: w = [4.99999999 6.99999998], b = 3.9999999928928283, loss = 54.49112959939007\n",
      "Iteration 381: w = [4.99999999 6.99999998], b = 3.9999999928888625, loss = 79.43237208863708\n",
      "Iteration 384: w = [4.99999999 6.99999998], b = 3.9999999928577554, loss = 348.8485234875711\n",
      "Iteration 387: w = [4.99999999 6.99999998], b = 3.999999992851792, loss = 133.91902121852203\n",
      "Iteration 390: w = [4.99999999 6.99999998], b = 3.999999992856085, loss = 44.0374297778024\n",
      "Iteration 393: w = [4.99999999 6.99999998], b = 3.999999992823703, loss = 264.7703789317879\n",
      "Iteration 396: w = [4.99999998 6.99999998], b = 3.999999992802892, loss = 139.14292507275925\n",
      "Iteration 399: w = [4.99999998 6.99999998], b = 3.9999999927966408, loss = 58.46828681994338\n",
      "Iteration 402: w = [4.99999998 6.99999998], b = 3.9999999927763605, loss = 163.87649809647505\n",
      "Iteration 405: w = [4.99999998 6.99999998], b = 3.999999992772528, loss = 138.50077855930917\n",
      "Iteration 408: w = [4.99999998 6.99999998], b = 3.9999999927583576, loss = 84.14488995388172\n",
      "Iteration 411: w = [4.99999998 6.99999998], b = 3.9999999927442422, loss = 237.79732905308217\n",
      "Iteration 414: w = [4.99999998 6.99999998], b = 3.9999999927498715, loss = 54.642814240146585\n",
      "Iteration 417: w = [4.99999998 6.99999998], b = 3.999999992737577, loss = 214.9568857384148\n",
      "Iteration 420: w = [4.99999998 6.99999998], b = 3.9999999927475813, loss = 40.21493898460924\n",
      "Iteration 423: w = [4.99999998 6.99999998], b = 3.999999992734433, loss = 83.58319357736774\n",
      "Iteration 426: w = [4.99999998 6.99999998], b = 3.9999999927268015, loss = 93.59086844663274\n",
      "Iteration 429: w = [4.99999998 6.99999998], b = 3.9999999927186582, loss = 17.60557113011468\n",
      "Iteration 432: w = [4.99999998 6.99999998], b = 3.9999999927176195, loss = 209.04528672082185\n",
      "Iteration 435: w = [4.99999998 6.99999998], b = 3.9999999927189003, loss = 54.826690374197135\n",
      "Iteration 438: w = [4.99999998 6.99999998], b = 3.999999992723575, loss = 81.990604562667\n",
      "Iteration 441: w = [4.99999998 6.99999998], b = 3.9999999927144785, loss = 34.15324169253117\n",
      "Iteration 444: w = [4.99999998 6.99999998], b = 3.999999992711907, loss = 1.6633865515686637\n",
      "Iteration 447: w = [4.99999998 6.99999998], b = 3.99999999271235, loss = 31.68883961460864\n",
      "Iteration 450: w = [4.99999998 6.99999998], b = 3.999999992695335, loss = 123.30199151190101\n",
      "Iteration 453: w = [4.99999998 6.99999998], b = 3.9999999927119165, loss = 84.62997454886288\n",
      "Iteration 456: w = [4.99999998 6.99999998], b = 3.9999999926836476, loss = 272.7354720334714\n",
      "Iteration 459: w = [4.99999998 6.99999998], b = 3.9999999926526733, loss = 259.6206694224302\n",
      "Iteration 462: w = [4.99999998 6.99999998], b = 3.999999992629524, loss = 147.8637905808815\n",
      "Iteration 465: w = [4.99999998 6.99999998], b = 3.9999999926333, loss = 8.189288898087044\n",
      "Iteration 468: w = [4.99999998 6.99999998], b = 3.9999999926407543, loss = 78.90993415868496\n",
      "Iteration 471: w = [4.99999998 6.99999998], b = 3.999999992613523, loss = 297.7577916853291\n",
      "Iteration 474: w = [4.99999998 6.99999998], b = 3.9999999925986867, loss = 172.0386151476059\n",
      "Iteration 477: w = [4.99999998 6.99999998], b = 3.9999999926010372, loss = 117.12171519223243\n",
      "Iteration 480: w = [4.99999998 6.99999998], b = 3.9999999925937373, loss = 29.120195687692256\n",
      "Iteration 483: w = [4.99999998 6.99999998], b = 3.9999999925629943, loss = 488.09921394993006\n",
      "Iteration 486: w = [4.99999998 6.99999998], b = 3.9999999925470955, loss = 245.07803976654688\n",
      "Iteration 489: w = [4.99999998 6.99999998], b = 3.999999992536941, loss = 138.30742972336063\n",
      "Iteration 492: w = [4.99999998 6.99999998], b = 3.9999999925333207, loss = 53.62525120365863\n",
      "Iteration 495: w = [4.99999998 6.99999998], b = 3.9999999925416865, loss = 30.710168240094674\n",
      "Iteration 498: w = [4.99999998 6.99999998], b = 3.9999999925224077, loss = 226.24425011243753\n",
      "Iteration 501: w = [4.99999998 6.99999998], b = 3.999999992509558, loss = 60.942200889255695\n",
      "Iteration 504: w = [4.99999998 6.99999998], b = 3.99999999248843, loss = 252.382540002353\n",
      "Iteration 507: w = [4.99999998 6.99999998], b = 3.9999999924661034, loss = 177.29607495261828\n",
      "Iteration 510: w = [4.99999998 6.99999998], b = 3.999999992469387, loss = 30.25255761959742\n",
      "Iteration 513: w = [4.99999998 6.99999998], b = 3.999999992470022, loss = 1.1522088791306844\n",
      "Iteration 516: w = [4.99999998 6.99999998], b = 3.9999999924645344, loss = 63.84175460958485\n",
      "Iteration 519: w = [4.99999998 6.99999998], b = 3.9999999924551535, loss = 86.77353690596516\n",
      "Iteration 522: w = [4.99999998 6.99999998], b = 3.9999999924472, loss = 94.78937881706811\n",
      "Iteration 525: w = [4.99999998 6.99999998], b = 3.999999992442955, loss = 185.3904042281891\n",
      "Iteration 528: w = [4.99999998 6.99999998], b = 3.999999992444628, loss = 65.61512749773901\n",
      "Iteration 531: w = [4.99999998 6.99999998], b = 3.999999992436661, loss = 166.38976263441478\n",
      "Iteration 534: w = [4.99999998 6.99999998], b = 3.9999999924280614, loss = 107.7832150723667\n",
      "Iteration 537: w = [4.99999998 6.99999998], b = 3.999999992447154, loss = 96.23615738303606\n",
      "Iteration 540: w = [4.99999998 6.99999998], b = 3.999999992460966, loss = 49.165010598520695\n",
      "Iteration 543: w = [4.99999998 6.99999998], b = 3.999999992460745, loss = 7.068746285290753\n",
      "Iteration 546: w = [4.99999998 6.99999998], b = 3.99999999245235, loss = 115.32837449970178\n",
      "Iteration 549: w = [4.99999998 6.99999998], b = 3.9999999924324423, loss = 164.9250539308073\n",
      "Iteration 552: w = [4.99999998 6.99999998], b = 3.9999999924190344, loss = 269.5514818852078\n",
      "Iteration 555: w = [4.99999998 6.99999998], b = 3.9999999924065355, loss = 142.78659153098207\n",
      "Iteration 558: w = [4.99999998 6.99999998], b = 3.999999992386735, loss = 124.89297932640115\n",
      "Iteration 561: w = [4.99999998 6.99999998], b = 3.9999999923991725, loss = 42.0323252066133\n",
      "Iteration 564: w = [4.99999998 6.99999998], b = 3.999999992394294, loss = 78.17713328029913\n",
      "Iteration 567: w = [4.99999998 6.99999998], b = 3.999999992399995, loss = 43.85891931965233\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x168ff5ac0>]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  initialize w, b, epochs, alpha\n",
    "mini_batch = 3\n",
    "w = np.array([5, 7])\n",
    "b = 4\n",
    "epochs = 5\n",
    "alpha = 0.000000000001\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch: {epoch+1}')\n",
    "\n",
    "    for batch in np.arange(0, len(df), mini_batch):\n",
    "        # 1. Pick samples (batch)\n",
    "        x = np.array([df['Radio'][batch:batch+mini_batch], df['Social Media'][batch:batch+mini_batch]]).T\n",
    "        y = df['Sales'][batch:batch+mini_batch].values\n",
    "\n",
    "        cost = 0\n",
    "        dLdw = 0\n",
    "        dLdb = 0\n",
    "        for i in range(mini_batch):\n",
    "            # 2. Calculate y_hat\n",
    "            y_hat_i = linear_function(w, x[i], b)\n",
    "\n",
    "            # 3. Calculate loss\n",
    "            cost += loss_function(y[i], y_hat_i)\n",
    "\n",
    "            # 4. Calculate gradients/derivatives\n",
    "            dLdw += 2*( y_hat_i - y[i] )*x[i]\n",
    "            dLdb += 2*(y_hat_i - y[i] )\n",
    "        \n",
    "        loss = cost/mini_batch\n",
    "        dLdb = dLdb/mini_batch\n",
    "        dLdw = dLdw/mini_batch\n",
    "        losses.append(loss)\n",
    "        \n",
    "        # 5. Update w, b\n",
    "        w, b = gradient_descent(y, y_hat, x, w, b, alpha, dLdw, dLdb)\n",
    "        \n",
    "        print(f'Iteration {batch}: w = {w}, b = {b}, loss = {loss}')\n",
    "\n",
    "plt.plot(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   2,   4,   6,   8,  10,  12,  14,  16,  18,  20,  22,  24,\n",
       "        26,  28,  30,  32,  34,  36,  38,  40,  42,  44,  46,  48,  50,\n",
       "        52,  54,  56,  58,  60,  62,  64,  66,  68,  70,  72,  74,  76,\n",
       "        78,  80,  82,  84,  86,  88,  90,  92,  94,  96,  98, 100, 102,\n",
       "       104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128,\n",
       "       130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154,\n",
       "       156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180,\n",
       "       182, 184, 186, 188, 190, 192, 194, 196, 198, 200, 202, 204, 206,\n",
       "       208, 210, 212, 214, 216, 218, 220, 222, 224, 226, 228, 230, 232,\n",
       "       234, 236, 238, 240, 242, 244, 246, 248, 250, 252, 254, 256, 258,\n",
       "       260, 262, 264, 266, 268, 270, 272, 274, 276, 278, 280, 282, 284,\n",
       "       286, 288, 290, 292, 294, 296, 298, 300, 302, 304, 306, 308, 310,\n",
       "       312, 314, 316, 318, 320, 322, 324, 326, 328, 330, 332, 334, 336,\n",
       "       338, 340, 342, 344, 346, 348, 350, 352, 354, 356, 358, 360, 362,\n",
       "       364, 366, 368, 370, 372, 374, 376, 378, 380, 382, 384, 386, 388,\n",
       "       390, 392, 394, 396, 398, 400, 402, 404, 406, 408, 410, 412, 414,\n",
       "       416, 418, 420, 422, 424, 426, 428, 430, 432, 434, 436, 438, 440,\n",
       "       442, 444, 446, 448, 450, 452, 454, 456, 458, 460, 462, 464, 466,\n",
       "       468, 470, 472, 474, 476, 478, 480, 482, 484, 486, 488, 490, 492,\n",
       "       494, 496, 498, 500, 502, 504, 506, 508, 510, 512, 514, 516, 518,\n",
       "       520, 522, 524, 526, 528, 530, 532, 534, 536, 538, 540, 542, 544,\n",
       "       546, 548, 550, 552, 554, 556, 558, 560, 562, 564, 566, 568, 570])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0, len(df), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Gradient Descent using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [70.5808891   6.62378817], b = [203.31458818]\n"
     ]
    }
   ],
   "source": [
    "data.head()\n",
    "x = data[['Radio', 'Social Media']].dropna()\n",
    "y = data['Sales'].dropna()\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "\n",
    "# Initialize SGDRegressor\n",
    "sgd_regressor = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1, learning_rate='constant')\n",
    "\n",
    "# Fit the model\n",
    "sgd_regressor.fit(x, y)\n",
    "\n",
    "# Print the coefficients\n",
    "print(f'w = {sgd_regressor.coef_}, b = {sgd_regressor.intercept_}')\n",
    "\n",
    "# Print the loss function value\n",
    "#print(f'Loss function value: {sgd_regressor.loss_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [55.08181457  2.51941673]\n",
      "Intercept: [194.71565111]\n",
      "R-squared score: 0.5929173999893271\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Scale the features (important for gradient descent)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(x)\n",
    "\n",
    "# Initialize SGD regressor\n",
    "sgd_reg = SGDRegressor(max_iter=1000, \n",
    "                      tol=1e-3, \n",
    "                      penalty=None, \n",
    "                      eta0=0.1,\n",
    "                      learning_rate='constant')\n",
    "\n",
    "# Fit the model\n",
    "sgd_reg.fit(X_scaled, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = sgd_reg.predict(X_scaled)\n",
    "\n",
    "# Print coefficients and intercept\n",
    "print(\"Coefficients:\", sgd_reg.coef_)\n",
    "print(\"Intercept:\", sgd_reg.intercept_)\n",
    "\n",
    "# Calculate R-squared score\n",
    "r2_score = sgd_reg.score(X_scaled, y)\n",
    "print(\"R-squared score:\", r2_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training history\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def plot_learning_curve(X, y, n_iter=1000):\n",
    "    sgd_reg = SGDRegressor(max_iter= n_iter, learning_rate='constant', eta0=0.00001)\n",
    "    losses = []\n",
    "    \n",
    "    for _ in range(n_iter):\n",
    "        sgd_reg.partial_fit(X, y)\n",
    "        y_pred = sgd_reg.predict(X)\n",
    "        loss = mean_squared_error(y, y_pred)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.title('SGD Learning Curve')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return sgd_reg\n",
    "\n",
    "# Call the function with scaled data\n",
    "model = plot_learning_curve(X_scaled, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
